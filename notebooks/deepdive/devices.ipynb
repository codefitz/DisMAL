{"cells":[{"cell_type":"markdown","id":"intro-title","metadata":{},"source":["# Devices Report (BMC Discovery)\n","\n","This notebook reproduces the DisMAL `devices` report using the Tideway Python library to run Discovery Data API searches.\n","It reads connection details from `config.yaml`, supports an optional `devices_with_cred` filter,\n","and writes a CSV under the standard `output_<target>` folder.\n","\n","> **NOTE:** Due to API limits, this may take a while to run."]},{"cell_type":"markdown","id":"requirements","metadata":{},"source":["## Requirements\n","\n","We use `tideway` (local package in this repo or PyPI), `pandas`, and `PyYAML`.\n","Uncomment the following to install in your environment if needed."]},{"cell_type":"code","execution_count":null,"id":"imports","metadata":{},"outputs":[],"source":["# %pip install -q tideway pandas pyyaml\n","\n","import os, sys, json\n","from pathlib import Path\n","import pandas as pd\n","import yaml\n","from typing import Any, Dict, List\n","import os\n","from datetime import datetime, UTC\n","import json\n","# Optional: for safely parsing list-like columns later\n","from ast import literal_eval\n"]},{"cell_type":"markdown","id":"select-appliance","metadata":{},"source":["## Select Appliance (optional)\n","\n","If your `config.yaml` defines multiple appliances under the `appliances:` list,\n","set `APPLIANCE_NAME` to one of their names (e.g., 'prod' or 'dev') or use the index.\n","Defaults to the first appliance if neither is set."]},{"cell_type":"code","execution_count":null,"id":"appliance-vars","metadata":{},"outputs":[],"source":["APPLIANCE_NAME = None   # e.g., 'prod' or 'dev'\n","APPLIANCE_INDEX = 0     # integer index if not using name selection\n","\n","# Optional filter: if set to a credential UUID, runs devices_with_cred flow\n","DEVICES_WITH_CRED_UUID = None  # e.g., '7636fe3b4bd69466ab487f0000010700'\n"]},{"cell_type":"markdown","id":"config-load","metadata":{},"source":["## Configuration (from config.yaml)\n","\n","Reads settings from `../config.yaml` including target, token/token_file,\n","API version, and SSL verification preference.\n","Saves the CSV to `../output_<target>/devices.csv` (or `devices_with_cred.csv`)."]},{"cell_type":"code","execution_count":null,"id":"config-cell","metadata":{},"outputs":[],"source":["def _find_repo_root(start: Path) -> Path:\n","    for p in [start] + list(start.parents):\n","        if (p / 'config.yaml').exists():\n","            return p\n","    return start.parent\n","\n","repo_root = _find_repo_root(Path.cwd())\n","config_path = repo_root / 'config.yaml'\n","with open(config_path, 'r') as fh:\n","    cfg = yaml.safe_load(fh) or {}\n","\n","# Appliance selection\n","apps = cfg.get('appliances') or []\n","selected = None\n","if isinstance(apps, list) and apps:\n","    if APPLIANCE_NAME:\n","        selected = next((a for a in apps if a.get('name') == APPLIANCE_NAME), None)\n","        if selected is None:\n","            raise ValueError(f\"No appliance named '{APPLIANCE_NAME}' in config.yaml\")\n","    else:\n","        try:\n","            selected = apps[int(APPLIANCE_INDEX)]\n","        except Exception:\n","            selected = apps[0]\n","\n","target = ((selected or {}).get('target') or cfg.get('target') or '').strip()\n","if not target:\n","    raise ValueError('config.yaml missing \"target\"')\n","\n","# Token handling: inline token or token file\n","token = (((selected or {}).get('token') or cfg.get('token') or '').strip())\n","token_file = (selected or {}).get('token_file') or cfg.get('token_file') or cfg.get('f_token')\n","if not token and token_file:\n","    tf_path = Path(token_file)\n","    if not tf_path.is_absolute():\n","        tf_path = repo_root / tf_path\n","    with open(tf_path, 'r') as tf:\n","        token = tf.read().strip()\n","if not token:\n","    raise ValueError('API token not found in config.yaml (token or token_file)')\n","\n","# Version and SSL\n","API_VERSION = str((selected or {}).get('api_version') or cfg.get('api_version') or 'v1.14')\n","VERIFY_SSL = bool((selected or {}).get('verify_ssl', cfg.get('verify_ssl', True)))\n","\n","# Output path\n","sanitized = target.replace('.', '_').replace(':', '_').replace('/', '_')\n","output_dir = repo_root / f'output_{sanitized}'\n","output_dir.mkdir(parents=True, exist_ok=True)\n","\n","print('Base Host      :', target)\n","print('API Version    :', API_VERSION)\n","print('Verify SSL     :', VERIFY_SSL)\n","print('Output folder  :', output_dir)\n","print('Token set      :', bool(token))\n","\n","# Prefer local Tideway package in this repo if available\n","local_tideway = repo_root / 'Tideway'\n","if local_tideway.exists():\n","    sys.path.insert(0, str(local_tideway))\n","\n","import importlib\n","tideway = importlib.import_module('tideway')\n","\n","API_VERSION_NUM = API_VERSION.lstrip('v')\n","app = tideway.appliance(target, token, api_version=API_VERSION_NUM, ssl_verify=VERIFY_SSL)\n","twsearch = app.data()\n","twcreds = app.credentials()\n","\n","# Quick probe (optional)\n","try:\n","    about = app.api_about\n","    print('Appliance reachable:', about.status_code)\n","except Exception as e:\n","    print('Warning: failed to contact appliance /api/about:', e)\n"]},{"cell_type":"markdown","id":"helpers","metadata":{},"source":["## Normalization"]},{"cell_type":"code","execution_count":null,"id":"helpers-code","metadata":{},"outputs":[],"source":["# Minimal pandas-based normalizer for Tideway responses\n","def df_from_tw(resp):\n","    \"\"\"Return a pandas DataFrame from Tideway search/endpoint responses.\n","\n","    Supports:\n","    - dict with 'headings' + 'results'\n","    - list: [headings, row1, row2, ...]\n","    - list of dicts\n","    - Response-like object with .json()\n","    \"\"\"\n","    obj = resp\n","    if hasattr(resp, 'json'):\n","        try:\n","            obj = resp.json()\n","        except Exception:\n","            obj = None\n","    if isinstance(obj, dict) and 'headings' in obj and 'results' in obj:\n","        cols = obj.get('headings') or []\n","        rows = obj.get('results') or []\n","        return pd.DataFrame(rows, columns=cols)\n","    if isinstance(obj, list):\n","        if obj and isinstance(obj[0], list):\n","            return pd.DataFrame(obj[1:], columns=obj[0])\n","        if obj and isinstance(obj[0], dict):\n","            return pd.DataFrame(obj)\n","        return pd.DataFrame()\n","    if isinstance(obj, dict):\n","        return pd.json_normalize(obj)\n","    return pd.DataFrame()\n"]},{"cell_type":"markdown","id":"74bc6705-88fd-45c2-ba6c-a3feff46773f","metadata":{},"source":["# Cache Builder"]},{"cell_type":"code","execution_count":null,"id":"dc76f6fe-a48c-4de2-a2e3-32bf141b466c","metadata":{},"outputs":[],"source":["from pathlib import Path\n","from datetime import datetime, UTC\n","import pandas as pd\n","import json, hashlib\n","from ast import literal_eval\n","\n","CACHE_DIR = Path(\".cache\")\n","\n","def _paths(name: str):\n","    CACHE_DIR.mkdir(exist_ok=True)\n","    return CACHE_DIR / f\"{name}.csv\", CACHE_DIR / f\"{name}.meta.json\"\n","\n","def _now_utc_iso():\n","    return datetime.now(UTC).isoformat(timespec=\"seconds\")\n","\n","def _age_hours(iso_ts: str) -> float:\n","    try:\n","        dt = datetime.fromisoformat(iso_ts)\n","        return (datetime.now(UTC) - dt).total_seconds() / 3600.0\n","    except Exception:\n","        return 1e9  # force stale\n","\n","def _hash(s: str) -> str:\n","    return hashlib.sha1(s.encode(\"utf-8\")).hexdigest()[:12]\n","\n","def get_or_build_df(\n","    name: str,\n","    build_fn,                     # () -> pd.DataFrame  (calls API + normalises)\n","    parse_list_cols=None,         # ['ips_all', ...] if you stored lists as strings\n","    transform_fn=None,            # (pd.DataFrame) -> pd.DataFrame (post-load tweaks)\n","    use_cache=True,\n","    force_refresh=False,\n","    max_age_hours=None,           # e.g. 12 (if None: never expires)\n","    query_text: str | None = None, # if provided, ties cache to query hash\n","    prefer_csv_without_meta=True,\n","    debug=True):\n","    csv_path, meta_path = _paths(name)\n","\n","    def _debug(msg):\n","        if debug: print(f\"[cache:{name}] {msg}\")\n","\n","    # CSV exists but meta missing\n","    if use_cache and not force_refresh and csv_path.exists() and (prefer_csv_without_meta and not meta_path.exists()):\n","        _debug(f\"meta missing; loading CSV-only at {csv_path}\")\n","        df = pd.read_csv(csv_path)\n","        if transform_fn:\n","            df = transform_fn(df)\n","        # Write a minimal meta so future loads are clean\n","        meta = {\n","            \"saved_at_utc\": _now_utc_iso(),\n","            \"row_count\": int(len(df)),\n","            \"columns\": list(df.columns),\n","            \"note\": \"meta reconstructed from CSV-only cache\"\n","        }\n","        with open(meta_path, \"w\") as f: json.dump(meta, f, indent=2)\n","        return df\n","\n","    # Existing logic when both CSV and meta are present\n","    if use_cache and not force_refresh and csv_path.exists() and meta_path.exists():\n","        with open(meta_path) as f: meta = json.load(f)\n","        if query_text and meta.get(\"query_hash\") != _hash(query_text.strip()):\n","            _debug(\"query hash mismatch -> rebuild\")\n","        elif max_age_hours is not None and _age_hours(meta.get(\"saved_at_utc\",\"1970-01-01T00:00:00+00:00\")) > max_age_hours:\n","            _debug(\"cache expired -> rebuild\")\n","        else:\n","            _debug(f\"loading from cache {csv_path}\")\n","            df = pd.read_csv(csv_path)\n","            if transform_fn: df = transform_fn(df)\n","            return df\n","\n","    # Rebuild\n","    _debug(\"building via API\")\n","    df = build_fn()\n","    df.to_csv(csv_path, index=False)\n","    meta = {\n","        \"saved_at_utc\": _now_utc_iso(),\n","        \"row_count\": int(len(df)),\n","        \"columns\": list(df.columns),\n","    }\n","    if query_text: meta[\"query_hash\"] = _hash(query_text.strip())\n","    with open(meta_path, \"w\") as f: json.dump(meta, f, indent=2)\n","    if transform_fn: df = transform_fn(df)\n","    _debug(f\"cached -> {csv_path.name}\")\n","    return df\n","\n","# --- Usage ---\n","# 1) Use cache if present, else call API:\n","# id_df = get_identities_df(twsearch, df_from_tw, limit=500, use_cache=True, force_refresh=False)\n","\n","# 2) Force refresh (ignore cache and re-save):\n","# id_df = get_identities_df(twsearch, df_from_tw, limit=500, use_cache=True, force_refresh=True)\n","\n","# 3) Load only from cache without any API call:\n","# id_df = load_identities_from_cache()\n","# if id_df is None:\n","#     print(\"No cache found. Run get_identities_df(..., force_refresh=True) once to create it.\")"]},{"cell_type":"markdown","id":"queries","metadata":{},"source":["## Queries\n","\n","These TWQL queries mirror the DisMAL devices flow and the optional devices_with_cred lookup."]},{"cell_type":"code","execution_count":null,"id":"queries-code","metadata":{},"outputs":[],"source":["# devices_with_cred flow\n","qry_sessions_for_cred = lambda uuid: f\"\"\"\n","search SessionResult where credential = '{uuid}'\n","show\n","(#Detail:Metadata:DiscoveryAccess:DiscoveryAccess.#Associate:Inference:InferredElement:.name\n","  or #Detail:Metadata:DiscoveryAccess:DiscoveryAccess.#DiscoveryAccess:DiscoveryAccessResult:DiscoveryResult:DeviceInfo.hostname) as 'device_name',\n","(kind(#Detail:Metadata:DiscoveryAccess:DiscoveryAccess.#Associate:Inference:InferredElement:)\n","  or #Detail:Metadata:DiscoveryAccess:DiscoveryAccess.inferred_kind\n","  or #Detail:Metadata:DiscoveryAccess:DiscoveryAccess.#DiscoveryAccess:DiscoveryAccessResult:DiscoveryResult:DeviceInfo.kind) as 'inferred_node',\n","#Detail:Metadata:DiscoveryAccess:DiscoveryAccess.endpoint as 'scanned_endpoint',\n","credential as 'credential',\n","success as 'success',\n","message as 'message',\n","friendlyTime(time_index) as 'date_time',\n","#Detail:Metadata:DiscoveryAccess:DiscoveryAccess.#id as 'node_id'\n","\"\"\"\n","\n","qry_di_for_cred = lambda uuid: f\"\"\"\n","search DeviceInfo where last_credential = '{uuid}' or last_slave = '{uuid}' or __preserved_last_credential = '{uuid}'\n","ORDER BY hostname\n","show\n","(hostname or sysname) as 'device_name',\n","kind as 'inferred_node',\n","#DiscoveryResult:DiscoveryAccessResult:DiscoveryResult:DiscoveryAccess.endpoint as 'scanned_endpoint',\n","#DiscoveryResult:DiscoveryAccessResult:DiscoveryResult:DiscoveryAccess.#id as 'da_node_id',\n","#DiscoveryResult:DiscoveryAccessResult:DiscoveryResult:DiscoveryAccess.reason as 'message',\n","method_success as 'success',\n","method_failure as 'failure',\n","friendlyTime(request_time) as 'date_time'\n","\"\"\"\n"]},{"cell_type":"markdown","id":"run-devices","metadata":{},"source":["## Run Report\n","\n","When `DEVICES_WITH_CRED_UUID` is set, runs the devices_with_cred flow; otherwise generates the consolidated devices report."]},{"cell_type":"code","execution_count":null,"id":"04b41f05-6b11-40c4-b0f9-3ef16ec9ad1c","metadata":{},"outputs":[],"source":["# --- Setup ---\n","print(f\"- Target: {target}\")\n","\n","# Optional credential filter (UUID or None)\n","cred_uuid = DEVICES_WITH_CRED_UUID if DEVICES_WITH_CRED_UUID else None\n","\n","# Load credentials as DataFrame (replace custom mapping with pandas)\n","try:\n","    creds_payload = twcreds.get_vault_credentials  # Response-like\n","    creds_list = creds_payload.json() if hasattr(creds_payload, \"json\") else (creds_payload or [])\n","except Exception:\n","    creds_list = []\n","\n","creds_df = pd.DataFrame(creds_list)\n","display(creds_df.sample(5))\n","\n","if creds_df.empty:\n","    print(\"Issue retrieving credentials!\")"]},{"cell_type":"code","execution_count":null,"id":"run-code","metadata":{},"outputs":[],"source":["# Short UUID and coalesced username\n","creds_df['short_uuid'] = creds_df.get('uuid', '').astype(str).str.split('/').str[-1].str.lower()\n","user_cols = ['username', 'snmp.v3.securityname', 'aws.access_key_id', 'azure.application_id']\n","\n","print(f\"- Credentials loaded: {len(creds_df)}\")\n","display(creds_df[['short_uuid','label','username']].sample(5))"]},{"cell_type":"code","execution_count":null,"id":"d2733b68-9cfa-4e31-8052-89cbf9598307","metadata":{},"outputs":[],"source":["# Credential filter\n","if cred_uuid:\n","    want = str(cred_uuid).split('/')[-1].lower()\n","    detail = creds_df.loc[creds_df['short_uuid'] == want, ['short_uuid','label','username']].head(1)\n","    if not detail.empty:\n","        row = detail.iloc[0]\n","        print(f\"- Credential UUID: {row['short_uuid']}\")\n","        print(f\"- Label: {row.get('label')}\")\n","    else:\n","        print(f\"- Credential UUID: {want} (not found in creds_df)\")\n","    # keep normalized short UUID for later filtering\n","    cred_uuid = want\n"]},{"cell_type":"code","execution_count":null,"id":"4925830f-9c52-4b44-ada2-7e06454fa2aa","metadata":{},"outputs":[],"source":["# Build Identities\n","\n","qry_device_ids = '''\n","                    search DiscoveryAccess\n","                    show\n","                    #::InferredElement:.name as 'InferredElement.name',\n","                    #::InferredElement:.hostname as 'InferredElement.hostname',\n","                    #::InferredElement:.local_fqdn as 'InferredElement.local_fqdn',\n","                    #::InferredElement:.sysname as 'InferredElement.sysname',\n","                    endpoint as 'DiscoveryAccess.endpoint',\n","                    #DiscoveryAccess:Endpoint:Endpoint:Endpoint.endpoint as 'Endpoint.endpoint',\n","                    #DiscoveryAccess:DiscoveryAccessResult:DiscoveryResult:DiscoveredIPAddressList.#List:List:Member:DiscoveredIPAddress.ip_addr as 'DiscoveredIPAddress.ip_addr',\n","                    #::InferredElement:.__all_ip_addrs as 'InferredElement.__all_ip_addrs',\n","                    #::InferredElement:.#DeviceWithInterface:DeviceInterface:InterfaceOfDevice:NetworkInterface.ip_addr as 'NetworkInterface.ip_addr',\n","                    #::InferredElement:.#DeviceWithInterface:DeviceInterface:InterfaceOfDevice:NetworkInterface.fqdns as 'NetworkInterface.fqdns'\n","                    '''\n","\n","# Run query and normalize to DataFrame\n","#resp_ids = twsearch.search({'query': qry_device_ids}, format='object', limit=500)\n","#id_df = df_from_tw(resp_ids)\n","\n","def build_identities_df():\n","    resp = twsearch.search({'query': qry_device_ids}, format='object', limit=500)\n","    return df_from_tw(resp)\n","\n","id_df = get_or_build_df(\n","    name=\"identities\",\n","    build_fn=build_identities_df,\n","    use_cache=True,\n","    force_refresh=False,\n","    max_age_hours=24,        # refresh daily\n","    query_text=qry_device_ids\n",")\n","print(f\"- Identity rows: {len(id_df)}\")"]},{"cell_type":"code","execution_count":null,"id":"4349dfce-4571-43ec-9a3e-595113f1378d","metadata":{},"outputs":[],"source":["# Check random rows\n","display(id_df.sample(5))"]},{"cell_type":"code","execution_count":null,"id":"9f79ea2f-cf39-451b-8c31-2a12e5144cc0","metadata":{},"outputs":[],"source":["# Aggregate names and IPs per originating DiscoveryAccess.endpoint using pandas\n","ip_fields = [\n","    'DiscoveryAccess.endpoint', 'Endpoint.endpoint',\n","    'DiscoveredIPAddress.ip_addr', 'InferredElement.__all_ip_addrs',\n","    'NetworkInterface.ip_addr'\n","]\n","name_fields = [\n","    'InferredElement.name', 'InferredElement.hostname',\n","    'InferredElement.local_fqdn', 'InferredElement.sysname',\n","    'NetworkInterface.fqdns'\n","]\n","for col in ip_fields + name_fields + ['DiscoveryAccess.endpoint']:\n","    if col not in id_df.columns:\n","        id_df[col] = None\n","to_list = lambda x: x if isinstance(x, list) else ([] if pd.isna(x) or x == '' else [x])\n","ips_lists = id_df[ip_fields].map(to_list)\n","names_lists = id_df[name_fields].map(to_list)\n","id_df['ips'] = ips_lists.apply(lambda row: sorted({str(v) for lst in row for v in lst if v is not None}), axis=1)\n","id_df['names'] = names_lists.apply(lambda row: sorted({str(v) for lst in row for v in lst if v is not None}), axis=1)\n","\n","identities_df = (\n","    id_df[['DiscoveryAccess.endpoint','ips','names']]\n","    .groupby('DiscoveryAccess.endpoint', as_index=False)\n","    .agg({\n","        'ips': lambda s: sorted({v for lst in s for v in lst}),\n","        'names': lambda s: sorted({v for lst in s for v in lst}),\n","    })\n","    .rename(columns={\n","        'DiscoveryAccess.endpoint': 'Identities.endpoint',\n","        'ips': 'list_of_ips',\n","        'names': 'list_of_names',\n","    })\n",")\n","\n","print(f\"- Unique endpoints: {len(identities_df)}\")\n","display(identities_df.head(5))"]},{"cell_type":"code","execution_count":null,"id":"11e1f00d-c627-46fb-b723-c95eebbd1e15","metadata":{},"outputs":[],"source":["# Trimmed last discovery access view with key fields needed for devices summary\n","qry_last_disco = '''\n","search DiscoveryAccess where endtime\n","ORDER BY discovery_endtime DESC\n","show\n","endpoint as 'DiscoveryAccess.endpoint',\n","#DiscoveryAccess:DiscoveryAccessResult:DiscoveryResult:DeviceInfo.hostname as 'DeviceInfo.hostname',\n","#Member:List:List:DiscoveryRun.label as 'DiscoveryRun.label',\n","friendlyTime(discovery_starttime) as 'DiscoveryAccess.scan_starttime',\n","friendlyTime(discovery_endtime) as 'DiscoveryAccess.scan_endtime',\n","discovery_endtime as 'DiscoveryAccess.scan_endtime_raw',\n","whenWasThat(discovery_endtime) as 'DiscoveryAccess.when_last_scan',\n","(#DiscoveryAccess:DiscoveryAccessResult:DiscoveryResult:DeviceInfo.last_access_method in ['windows', 'rcmd']\n","    and #DiscoveryAccess:DiscoveryAccessResult:DiscoveryResult:DeviceInfo.last_slave\n","        or #DiscoveryAccess:DiscoveryAccessResult:DiscoveryResult:DeviceInfo.probed_os and 'Probe'\n","            or #DiscoveryAccess:DiscoveryAccessResult:DiscoveryResult:DeviceInfo.last_access_method) as 'DiscoveryAccess.current_access',\n","(kind(#Associate:Inference:InferredElement:)\n","    or inferred_kind\n","        or #DiscoveryAccess:DiscoveryAccessResult:DiscoveryResult:DeviceInfo.kind) as 'DiscoveryAccess.node_kind',\n","(#DiscoveryAccess:Metadata:Detail:SessionResult.credential and success\n","    or #DiscoveryAccess:DiscoveryAccessResult:DiscoveryResult:DeviceInfo.last_credential\n","    or #DiscoveryAccess:DiscoveryAccessResult:DiscoveryResult:DeviceInfo.last_slave\n","    or #DiscoveryAccess:DiscoveryAccessResult:DiscoveryResult:DeviceInfo.__preserved_last_credential) as 'DeviceInfo.last_credential',\n","#DiscoveryAccess:DiscoveryAccessResult:DiscoveryResult:DeviceInfo.os_version as 'DeviceInfo.os_version',\n","(nodecount(traverse DiscoveryAccess:DiscoveryAccessResult:DiscoveryResult:DeviceInfo\n","  traverse flags(include_destroyed) Primary:Inference:InferredElement: where not destroyed(#)) > 0) as 'DiscoveryAccess.host_node_updated',\n","end_state as 'DiscoveryAccess.end_state',\n","result as 'DiscoveryAccess.result'\n","'''\n","\n","#resp_ld = twsearch.search({'query': qry_last_disco}, format='object', limit=500)\n","#ld_df = df_from_tw(resp_ld)\n","\n","def build_last_discovery_df():\n","    resp = twsearch.search({'query': qry_last_disco}, format='object', limit=500)\n","    return df_from_tw(resp)\n","\n","def normalise_last_discovery(df: pd.DataFrame) -> pd.DataFrame:\n","    df = df.copy()\n","    if 'DeviceInfo.last_credential' not in df.columns:\n","        df['DeviceInfo.last_credential'] = None\n","    df['last_cred_short'] = (\n","        df['DeviceInfo.last_credential']\n","        .astype(str).str.split('/').str[-1].str.lower()\n","    )\n","    return df\n","\n","ld_df = get_or_build_df(\n","    name=\"last_discovery\",\n","    build_fn=build_last_discovery_df,\n","    transform_fn=normalise_last_discovery,\n","    use_cache=True,\n","    force_refresh=False,\n","    max_age_hours=12,        # refresh twice a day\n","    query_text=qry_last_disco\n",")\n","\n","# Optional filter by credential\n","if cred_uuid:\n","    ld_df = ld_df[ld_df['last_cred_short'] == cred_uuid].copy()\n","\n","print(f\"- Last Discovery rows: {len(ld_df)}\")"]},{"cell_type":"code","execution_count":null,"id":"4598ee93-d53b-49ee-b4f7-7e6793366078","metadata":{},"outputs":[],"source":["# View results\n","\n","display(ld_df.sample(5))"]},{"cell_type":"code","execution_count":null,"id":"fd064b24-7c25-46fc-a178-b4537255ba7d","metadata":{},"outputs":[],"source":["# Sanity Check for dupes\n","\n","dup_counts = ld_df['DiscoveryAccess.endpoint'].value_counts()\n","dup_counts = dup_counts[dup_counts > 1]\n","print(f\"Endpoints with duplicates: {len(dup_counts)}\")\n","\n","# Inspect a few duplicated endpoints\n","sample_dups = dup_counts.index[:5]\n","display(ld_df[ld_df['DiscoveryAccess.endpoint'].isin(sample_dups)]\n","        .sort_values(['DiscoveryAccess.endpoint','DiscoveryAccess.scan_endtime_raw'], ascending=[True, False]))"]},{"cell_type":"code","execution_count":null,"id":"5ea0a5f5-0e3a-4d4d-b043-0083aa8faab4","metadata":{},"outputs":[],"source":["# Merge identities with last discovery by endpoint\n","\n","# Explode identity IPs and join to discovery rows\n","ip_map = identities_df[['Identities.endpoint','list_of_ips']].explode('list_of_ips')\n","merged = ip_map.merge(ld_df, left_on='list_of_ips', right_on='DiscoveryAccess.endpoint', how='left')\n","\n","# Enrich with credential label/username for convenience\n","merged = merged.merge(\n","    creds_df[['short_uuid','label','username']],\n","    left_on='last_cred_short',\n","    right_on='short_uuid',\n","    how='left'\n",")\n","\n","# Numeric scan end time for ranking\n","merged['scan_end_raw'] = pd.to_numeric(merged.get('DiscoveryAccess.scan_endtime_raw'), errors='coerce').fillna(-1)\n","\n","print(f\"- Merged rows: {len(merged)}\")"]},{"cell_type":"code","execution_count":null,"id":"48de3ecf-4720-42e6-8c21-d1977ef5dcf3","metadata":{},"outputs":[],"source":["# Review some samples\n","\n","display(merged.sample(5))\n","\n","# Keep only rows where list_of_ips has >1 values\n","multi_ip_rows = ip_map[ip_map['list_of_ips'].apply(lambda x: isinstance(x, list) and len(x) > 1)]\n","\n","print(f\"Rows with multiple IPs: {len(multi_ip_rows)}\")\n","display(multi_ip_rows.head())\n","\n","ip_map['ip_count'] = ip_map['list_of_ips'].apply(\n","    lambda x: len(x) if isinstance(x, list) else (0 if pd.isna(x) else 1)\n",")\n","\n","print(ip_map['ip_count'].value_counts())"]},{"cell_type":"code","execution_count":null,"id":"191f08ab-a09a-4c6d-9532-ebce163aa476","metadata":{},"outputs":[],"source":["print(\"\\n=== Aggregate per originating endpoint and select latest/last successful ===\")\n","\n","grp = merged.groupby('Identities.endpoint', dropna=False)\n","\n","# Aggregated sets\n","agg_device_names = grp['DeviceInfo.hostname'].apply(lambda s: sorted(set(x for x in s.dropna().astype(str)))).rename('all_device_names')\n","agg_endpoints = grp['DiscoveryAccess.endpoint'].apply(lambda s: sorted(set(x for x in s.dropna().astype(str)))).rename('all_endpoints')\n","agg_runs = grp['DiscoveryRun.label'].apply(lambda s: sorted(set(x for x in s.dropna().astype(str)))).rename('all_discovery_runs')\n","merged['cred_display'] = merged.apply(\n","    lambda r: f\"{r['label']} ({r['last_cred_short']})\" if pd.notna(r.get('label')) and pd.notna(r.get('last_cred_short')) else (r.get('last_cred_short')),\n","    axis=1\n",")\n","agg_creds = grp['cred_display'].apply(lambda s: sorted(set(x for x in s.dropna().astype(str)))).rename('all_credentials_used')\n","\n","# Latest rows\n","idx_latest = grp['scan_end_raw'].idxmax()\n","latest = merged.loc[idx_latest, [\n","    'Identities.endpoint',\n","    'DiscoveryAccess.endpoint',\n","    'DeviceInfo.hostname',\n","    'DiscoveryAccess.node_kind',\n","    'last_cred_short',\n","    'label',\n","    'username',\n","    'DiscoveryAccess.scan_starttime',\n","    'DiscoveryRun.label',\n","    'DiscoveryAccess.end_state',\n","    'DiscoveryAccess.result',\n","    'DiscoveryAccess.current_access'\n","]].rename(columns={\n","    'DiscoveryAccess.endpoint': 'last_scanned_ip',\n","    'DeviceInfo.hostname': 'last_identity',\n","    'DiscoveryAccess.node_kind': 'last_kind',\n","    'last_cred_short': 'last_credential',\n","    'label': 'last_credential_label',\n","    'username': 'last_credential_username',\n","    'DiscoveryAccess.scan_starttime': 'last_start_time',\n","    'DiscoveryRun.label': 'last_run',\n","    'DiscoveryAccess.end_state': 'last_endstate',\n","    'DiscoveryAccess.result': 'last_result',\n","    'DiscoveryAccess.current_access': 'last_access_method',\n","})\n","\n","# Latest successful rows\n","succ = merged[merged.get('DiscoveryAccess.host_node_updated').astype(bool)]\n","idx_succ = succ.groupby('Identities.endpoint')['scan_end_raw'].idxmax()\n","last_succ = succ.loc[idx_succ, [\n","    'Identities.endpoint',\n","    'DeviceInfo.hostname',\n","    'DiscoveryAccess.endpoint',\n","    'last_cred_short',\n","    'label',\n","    'username',\n","    'DiscoveryAccess.scan_starttime',\n","    'DiscoveryRun.label',\n","    'DiscoveryAccess.end_state'\n","]].rename(columns={\n","    'DeviceInfo.hostname': 'last_successful_identity',\n","    'DiscoveryAccess.endpoint': 'last_successful_ip',\n","    'last_cred_short': 'last_successful_credential',\n","    'label': 'last_successful_credential_label',\n","    'username': 'last_successful_credential_username',\n","    'DiscoveryAccess.scan_starttime': 'last_successful_start_time',\n","    'DiscoveryRun.label': 'last_successful_run',\n","    'DiscoveryAccess.end_state': 'last_successful_endstate',\n","})\n","\n","# Assemble final DataFrame\n","df_out = (\n","    latest\n","    .merge(agg_device_names, on='Identities.endpoint', how='left')\n","    .merge(agg_endpoints, on='Identities.endpoint', how='left')\n","    .merge(agg_runs, on='Identities.endpoint', how='left')\n","    .merge(agg_creds, on='Identities.endpoint', how='left')\n","    .merge(last_succ, on='Identities.endpoint', how='left')\n","    .merge(identities_df[['Identities.endpoint', 'list_of_names']], on='Identities.endpoint', how='left')\n",")\n","\n","# Fallback last_identity to a name seen in identities if missing\n","df_out['last_identity'] = df_out.apply(\n","    lambda r: r['last_identity'] if pd.notna(r['last_identity']) and str(r['last_identity']).strip() else (r['list_of_names'][0] if isinstance(r['list_of_names'], list) and r['list_of_names'] else None),\n","    axis=1\n",")\n","\n","# Reorder columns to original schema\n","df_out = df_out[[\n","    'last_scanned_ip',\n","    'last_identity',\n","    'last_kind',\n","    'all_device_names',\n","    'all_endpoints',\n","    'all_credentials_used',\n","    'all_discovery_runs',\n","    'last_credential',\n","    'last_credential_label',\n","    'last_credential_username',\n","    'last_start_time',\n","    'last_run',\n","    'last_endstate',\n","    'last_result',\n","    'last_access_method',\n","    'last_successful_identity',\n","    'last_successful_ip',\n","    'last_successful_credential',\n","    'last_successful_credential_label',\n","    'last_successful_credential_username',\n","    'last_successful_start_time',\n","    'last_successful_run',\n","    'last_successful_endstate',\n","]]\n","\n","# Add Discovery Instance col and sample\n","df_out.insert(0, 'Discovery Instance', target)\n","REPORT_NAME = 'devices'\n","print(f\"- Output shape: {df_out.shape}\")\n","display(df_out.head(5))\n"]},{"cell_type":"markdown","id":"save-csv","metadata":{},"source":["## Save to CSV\n","\n","Writes the report to the standard output folder in the project root."]},{"cell_type":"code","execution_count":null,"id":"save-csv-code","metadata":{},"outputs":[],"source":["OUTPUT_CSV = str(output_dir / f'{REPORT_NAME}.csv')\n","df_out.to_csv(OUTPUT_CSV, index=False)\n","print(f'Saved to {OUTPUT_CSV}')\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.3"}},"nbformat":4,"nbformat_minor":5}