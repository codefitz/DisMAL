{"cells":[{"cell_type":"markdown","id":"intro-title","metadata":{},"source":["# Discovery Analysis Report (BMC Discovery)\n","\n","This notebook reproduces the DisMAL `discovery_analysis` report but runs independently of DisMAL modules.\n","It uses the Tideway Python library for queries, is broken into small sections,\n","and saves the final CSV to the standard `output_<target>` folder."]},{"cell_type":"markdown","id":"requirements","metadata":{},"source":["## Requirements\n","\n","Uncomment the next cell to install dependencies in your environment if needed.\n","\n","> **NOTE:** Right now this takes a long time to run over API if there are a lot of DiscoveryAccesses. This is the way the API an search is designed, not much of a way around it at current!"]},{"cell_type":"code","execution_count":null,"id":"imports","metadata":{},"outputs":[],"source":["# %pip install -q tideway pandas pyyaml\n","\n","import os, sys, json, datetime, importlib\n","from pathlib import Path\n","from typing import Any, Dict, List\n","import pandas as pd\n","import numpy as np\n","import yaml\n","from collections.abc import Mapping, Sequence\n","from pandas.errors import EmptyDataError\n","import hashlib\n"]},{"cell_type":"markdown","id":"select-appliance","metadata":{},"source":["## Select Appliance (optional)\n","\n","If `config.yaml` has multiple appliances, set by name or index. Defaults to the first."]},{"cell_type":"code","execution_count":null,"id":"appliance-vars","metadata":{},"outputs":[],"source":["APPLIANCE_NAME = None   # e.g., 'prod' or 'dev'\n","APPLIANCE_INDEX = 0     # integer index if not using name selection\n"]},{"cell_type":"markdown","id":"config-load","metadata":{},"source":["## Configuration (from config.yaml)\n","\n","Finds `config.yaml` in the repo root (or its parents), reads `target`, token/token_file, API version, and SSL flag.\n","Saves the CSV to `output_<target>/discovery_analysis.csv`."]},{"cell_type":"code","execution_count":null,"id":"config-cell","metadata":{},"outputs":[],"source":["def _find_repo_root(start: Path) -> Path:\n","    for p in [start] + list(start.parents):\n","        if (p / 'config.yaml').exists():\n","            return p\n","    return start.parent\n","\n","repo_root = _find_repo_root(Path.cwd())\n","config_path = repo_root / 'config.yaml'\n","with open(config_path, 'r') as fh:\n","    cfg = yaml.safe_load(fh) or {}\n","\n","apps = cfg.get('appliances') or []\n","selected = None\n","if isinstance(apps, list) and apps:\n","    if APPLIANCE_NAME:\n","        selected = next((a for a in apps if a.get('name') == APPLIANCE_NAME), None)\n","        if selected is None:\n","            raise ValueError(f\"No appliance named '{APPLIANCE_NAME}' in config.yaml\")\n","    else:\n","        try:\n","            selected = apps[int(APPLIANCE_INDEX)]\n","        except Exception:\n","            selected = apps[0]\n","\n","target = ((selected or {}).get('target') or cfg.get('target') or '').strip()\n","if not target:\n","    raise ValueError('config.yaml missing \"target\"')\n","\n","token = (((selected or {}).get('token') or cfg.get('token') or '').strip())\n","token_file = (selected or {}).get('token_file') or cfg.get('token_file') or cfg.get('f_token')\n","if not token and token_file:\n","    tf_path = Path(token_file)\n","    if not tf_path.is_absolute():\n","        tf_path = repo_root / tf_path\n","    with open(tf_path, 'r') as tf:\n","        token = tf.read().strip()\n","if not token:\n","    raise ValueError('API token not found in config.yaml (token or token_file)')\n","\n","API_VERSION = str((selected or {}).get('api_version') or cfg.get('api_version') or 'v1.14')\n","VERIFY_SSL = bool((selected or {}).get('verify_ssl', cfg.get('verify_ssl', True)))\n","\n","sanitized = target.replace('.', '_').replace(':', '_').replace('/', '_')\n","output_dir = repo_root / f'output_{sanitized}'\n","output_dir.mkdir(parents=True, exist_ok=True)\n","\n","print('Base Host     :', target)\n","print('API Version   :', API_VERSION)\n","print('Verify SSL    :', VERIFY_SSL)\n","print('Output folder :', output_dir)\n","\n","# Prefer local Tideway package if present\n","local_tideway = repo_root / 'Tideway'\n","if local_tideway.exists():\n","    sys.path.insert(0, str(local_tideway))\n","tideway = importlib.import_module('tideway')\n","API_VERSION_NUM = API_VERSION.lstrip('v')\n","app = tideway.appliance(target, token, api_version=API_VERSION_NUM, ssl_verify=VERIFY_SSL)\n","twsearch = app.data()\n","twcreds = app.credentials()\n","try:\n","    about = app.api_about\n","    print('Appliance reachable:', about.status_code)\n","except Exception as e:\n","    print('Warning: failed to contact appliance /api/about:', e)\n"]},{"cell_type":"markdown","id":"helpers","metadata":{},"source":["## Helpers\n","\n","Utility helpers for Tideway result normalization, identity building, and credential mapping."]},{"cell_type":"code","execution_count":null,"id":"helpers-code","metadata":{},"outputs":[],"source":["def list_table_to_json(table_like):\n","    if not table_like or not isinstance(table_like, list):\n","        return []\n","    if not table_like or not isinstance(table_like[0], list):\n","        return []\n","    headers = table_like[0]\n","    rows = table_like[1:]\n","    out = []\n","    for r in rows:\n","        try:\n","            out.append(dict(zip(headers, r)))\n","        except Exception:\n","            continue\n","    return out\n","\n","def to_rows(payload):\n","    if isinstance(payload, list):\n","        if payload and isinstance(payload[0], list):\n","            return list_table_to_json(payload)\n","        if payload and isinstance(payload[0], dict):\n","            return payload\n","        return []\n","    if hasattr(payload, 'json'):\n","        try:\n","            js = payload.json()\n","        except Exception:\n","            return []\n","        if isinstance(js, list):\n","            if js and isinstance(js[0], list):\n","                return list_table_to_json(js)\n","            if js and isinstance(js[0], dict):\n","                return js\n","        if isinstance(js, dict) and 'results' in js and 'headings' in js:\n","            table_like = [js['headings']] + list(js.get('results') or [])\n","            return list_table_to_json(table_like)\n","        return []\n","    if isinstance(payload, dict) and 'results' in payload and 'headings' in payload:\n","        table_like = [payload['headings']] + list(payload.get('results') or [])\n","        return list_table_to_json(table_like)\n","    return []\n","\n","def tw_search_all(search, query: str, limit: int = 500):\n","    resp = search.search({'query': query}, format='object', limit=limit)\n","    return to_rows(resp)\n","\n","def flatten_list(value):\n","    if value is None:\n","        return []\n","    if isinstance(value, list):\n","        out = []\n","        for v in value:\n","            if isinstance(v, list):\n","                out.extend(v)\n","            else:\n","                out.append(v)\n","        return out\n","    return [value]\n","\n","def sort_unique(items):\n","    return sorted(set([x for x in items if x is not None]))\n","\n","def get_credential_map(twcreds_handle):\n","    resp = twcreds_handle.get_vault_credentials\n","    try:\n","        items = resp.json()\n","    except Exception:\n","        items = []\n","    mapping = {}\n","    for c in items or []:\n","        if not isinstance(c, dict):\n","            continue\n","        uuid = str(c.get('uuid') or '').split('/')[-1]\n","        mapping[uuid] = {\n","            'label': c.get('label'),\n","            'username': c.get('username')\n","                        or c.get('snmp.v3.securityname')\n","                        or c.get('aws.access_key_id')\n","                        or c.get('azure.application_id'),\n","        }\n","    return mapping\n","\n","def build_identities(id_rows: pd.DataFrame):\n","    if id_rows is None or id_rows.empty:\n","        return []\n","\n","    # Restrict to relevant columns if present\n","    cols = [\n","        'DiscoveryAccess.endpoint','Endpoint.endpoint',\n","        'DiscoveredIPAddress.ip_addr','InferredElement.__all_ip_addrs',\n","        'NetworkInterface.ip_addr',\n","        'InferredElement.name','InferredElement.hostname',\n","        'InferredElement.local_fqdn','InferredElement.sysname',\n","        'NetworkInterface.fqdns'\n","    ]\n","    present = [c for c in cols if c in id_rows.columns]\n","    df = id_rows[present].copy()\n","\n","    endpoint_map: Dict[str, Dict[str, set]] = {}\n","    ip_fields = [f for f in [\n","        'DiscoveredIPAddress.ip_addr','InferredElement.__all_ip_addrs','NetworkInterface.ip_addr'\n","    ] if f in df.columns]\n","    name_fields = [f for f in [\n","        'InferredElement.name','InferredElement.hostname','InferredElement.local_fqdn','InferredElement.sysname','NetworkInterface.fqdns'\n","    ] if f in df.columns]\n","\n","    for rec in df.to_dict('records'):\n","        ep = rec.get('DiscoveryAccess.endpoint') or rec.get('Endpoint.endpoint')\n","        if not ep:\n","            continue\n","        data = endpoint_map.setdefault(ep, {'ips': set(), 'names': set()})\n","        for f in ip_fields:\n","            vals = flatten_list(rec.get(f))\n","            data['ips'].update([v for v in vals if v])\n","        for f in name_fields:\n","            vals = flatten_list(rec.get(f))\n","            data['names'].update([v for v in vals if v])\n","\n","    identities = []\n","    for ep, sets in endpoint_map.items():\n","        ips = sort_unique(list(sets['ips']))\n","        names = sort_unique(list(sets['names']))\n","        identities.append({\n","            'originating_endpoint': ep,\n","            'list_of_ips': ips,\n","            'list_of_names': names,\n","        })\n","\n","    return identities\n","\n","def calc_when(ts: datetime.datetime):\n","    bins = [0, 59, 1440, 10080, 43830, 131487, 262974, 525949, 525950]\n","    labels = [\n","        'Less than 60 minutes ago',\n","        'Less than 24 hours ago',\n","        'Less than 7 days ago',\n","        'Less than 1 month ago',\n","        'Less than 3 months ago',\n","        'Less than 6 months ago',\n","        'Less than 12 months ago',\n","        'Over a year ago',\n","    ]\n","    if ts is None:\n","        return None\n","    delta = datetime.datetime.now(ts.tzinfo) - ts\n","    mins = delta.days * 24 * 60 + delta.seconds / 60\n","    df = pd.DataFrame({'in_minutes': [mins]})\n","    df['when'] = pd.cut(df['in_minutes'], bins=bins, labels=labels, right=False)\n","    return df.to_dict().get('when').get(0)\n"]},{"cell_type":"markdown","id":"cache-builder","metadata":{},"source":["# Cache Builder"]},{"cell_type":"code","execution_count":null,"id":"cache-builder-code","metadata":{},"outputs":[],"source":["CACHE_DIR = Path(\".cache\")\n","\n","def _paths(name: str):\n","    CACHE_DIR.mkdir(exist_ok=True)\n","    return CACHE_DIR / f\"{name}.csv\", CACHE_DIR / f\"{name}.meta.json\"\n","\n","def _now_utc_iso():\n","    return datetime.datetime.now(datetime.timezone.utc).isoformat(timespec=\"seconds\")\n","\n","def _age_hours(iso_ts: str) -> float:\n","    try:\n","        dt = datetime.datetime.fromisoformat(iso_ts)\n","        return (datetime.datetime.now(datetime.timezone.utc) - dt).total_seconds() / 3600.0\n","    except Exception:\n","        return 1e9  # force stale\n","\n","def _hash(s: str) -> str:\n","    return hashlib.sha1(s.encode(\"utf-8\")).hexdigest()[:12]\n","\n","def get_or_build_df(\n","    name: str,\n","    build_fn,                      # () -> pd.DataFrame | None\n","    parse_list_cols=None,          # kept for compatibility (unused here)\n","    transform_fn=None,             # (pd.DataFrame) -> pd.DataFrame\n","    use_cache=True,\n","    force_refresh=False,\n","    max_age_hours=None,            # None = never expires\n","    query_text: str | None = None, # ties cache to query hash\n","    prefer_csv_without_meta=True,\n","    expected_columns: list[str] | None = None,\n","    trust_existing_csv=True,      # if CSV exists and not forcing, load & update meta\n","    debug=True):\n","    csv_path, meta_path = _paths(name)\n","\n","    def _debug(msg):\n","        if debug: print(f\"[cache:{name}] {msg}\")\n","\n","    def _read_csv_with_schema(columns_from_meta=None):\n","        try:\n","            df = pd.read_csv(csv_path)\n","        except EmptyDataError:\n","            df = pd.DataFrame(columns=columns_from_meta or [])\n","        return df\n","\n","    # CSV exists but meta missing\n","    if use_cache and not force_refresh and csv_path.exists() and (prefer_csv_without_meta and not meta_path.exists()):\n","        _debug(f\"meta missing; loading CSV-only at {csv_path}\")\n","        df = _read_csv_with_schema()\n","        if transform_fn: df = transform_fn(df)\n","        meta = {\n","            \"saved_at_utc\": _now_utc_iso(),\n","            \"row_count\": int(len(df)),\n","            \"columns\": list(df.columns),\n","            \"no_results\": len(df) == 0,\n","            \"note\": \"meta reconstructed from CSV-only cache\"\n","        }\n","        if query_text: meta[\"query_hash\"] = _hash(query_text.strip())\n","        with open(meta_path, \"w\") as f: json.dump(meta, f, indent=2)\n","        return df\n","\n","    # Normal cached path\n","    if use_cache and not force_refresh and csv_path.exists() and meta_path.exists():\n","        with open(meta_path) as f: meta = json.load(f)\n","        qh_ok = (query_text is None) or (meta.get(\"query_hash\") == _hash(query_text.strip()))\n","        fresh = (max_age_hours is None) or (_age_hours(meta.get(\"saved_at_utc\",\"1970-01-01T00:00:00+00:00\")) <= max_age_hours)\n","\n","        if qh_ok and fresh:\n","            _debug(f\"loading from cache {csv_path}\")\n","            df = _read_csv_with_schema(columns_from_meta=meta.get(\"columns\", []))\n","            if transform_fn: df = transform_fn(df)\n","            return df\n","        elif trust_existing_csv:\n","            _debug(\"stale/mismatch meta; trusting existing CSV and updating meta\")\n","            df = _read_csv_with_schema(columns_from_meta=meta.get(\"columns\", []))\n","            new_meta = {\n","                \"saved_at_utc\": _now_utc_iso(),\n","                \"row_count\": int(len(df)),\n","                \"columns\": list(df.columns),\n","                \"no_results\": len(df) == 0,\n","            }\n","            if query_text: new_meta[\"query_hash\"] = _hash(query_text.strip())\n","            with open(meta_path, \"w\") as f: json.dump(new_meta, f, indent=2)\n","            if transform_fn: df = transform_fn(df)\n","            return df\n","        else:\n","            _debug(\"query hash mismatch or cache expired -> rebuild\")\n","\n","    # Rebuild\n","    _debug(\"building via API\")\n","    df = build_fn()\n","    if df is None:\n","        df = pd.DataFrame(columns=expected_columns or [])\n","    elif not isinstance(df, pd.DataFrame):\n","        df = pd.DataFrame(df)\n","\n","    # Ensure we always write a CSV with headers, even for 0 rows\n","    if df.empty and len(df.columns) == 0 and expected_columns:\n","        df = pd.DataFrame(columns=expected_columns)\n","\n","    df.to_csv(csv_path, index=False)\n","\n","    meta = {\n","        \"saved_at_utc\": _now_utc_iso(),\n","        \"row_count\": int(len(df)),\n","        \"columns\": list(df.columns),\n","        \"no_results\": len(df) == 0,\n","    }\n","    if query_text: meta[\"query_hash\"] = _hash(query_text.strip())\n","    with open(meta_path, \"w\") as f: json.dump(meta, f, indent=2)\n","\n","    if transform_fn: df = transform_fn(df)\n","    _debug(f\"cached -> {csv_path.name}\")\n","    return df\n"]},{"cell_type":"markdown","id":"fetch-key-facts","metadata":{},"source":["## Fetch Discovery Extracts\n","\n","Retrieves the minimal key mapping and the fact tables used for the analysis."]},{"cell_type":"code","execution_count":null,"id":"68ddb394-1c7d-4e37-8f38-7cd20cac5f27","metadata":{},"outputs":[],"source":["def df_of(q, label, n=5):\n","    rows = tw_search_all(twsearch, q) or []\n","    df = pd.DataFrame(rows)\n","\n","    if df.empty:\n","        print(f'- {label}: 0 rows')\n","        return df\n","\n","    # make a hashable view for dedupe\n","    def _hashable(x):\n","        if isinstance(x, (list, tuple)):\n","            return tuple(_hashable(v) for v in x)\n","        if isinstance(x, dict):\n","            return tuple(sorted((k, _hashable(v)) for k, v in x.items()))\n","        return x\n","\n","    df_h = df.map(_hashable)\n","    idx = df_h.drop_duplicates().index\n","    dfu = df.loc[idx].reset_index(drop=True)\n","\n","    print(f'- {label}: {len(rows)} rows (raw), {len(dfu)} rows (unique)')\n","    display(dfu.head(n))\n","    return dfu"]},{"cell_type":"code","execution_count":null,"id":"e50bcba5-2519-46b4-8a65-0c814433ee1f","metadata":{},"outputs":[],"source":["#search DiscoveryAccess where endtime and #id = \"0684b5680100264824e0a6256e446973636f76657279416363657373\"\n","qry_key = '''\n","search DiscoveryAccess where endtime\n","show\n","#id as \"DiscoveryAccess.id\",\n","#Next:Sequential:Previous:DiscoveryAccess.#id as \"DiscoveryAccess.previous_id\",\n","#Previous:Sequential:Next:DiscoveryAccess.#id as \"DiscoveryAccess.next_id\",\n","#DiscoveryAccess:DiscoveryAccessResult:DiscoveryResult:DeviceInfo.#id as \"DeviceInfo.id\",\n","#Member:List:List:DiscoveryRun.#id as \"DiscoveryRun.id\",\n","#::InferredElement:.#id as \"InferredElement.id\",\n","#DiscoveryAccess:Metadata:Detail:SessionResult.#id as \"SessionResult.id\",\n","explode #::InferredElement:.#DeviceWithInterface:DeviceInterface:InterfaceOfDevice:NetworkInterface.#id as \"NetworkInterface.id\"\n","'''\n","\n","print('Running extracts…')\n","\n","def build_key_map_df():\n","    rows = tw_search_all(twsearch, qry_key) or []\n","    return pd.DataFrame(rows)\n","\n","key_df = get_or_build_df(\n","    name=\"key_map\",\n","    build_fn=build_key_map_df,\n","    use_cache=True,\n","    force_refresh=False,\n","    max_age_hours=24,        # refresh daily\n","    query_text=qry_key\n",")"]},{"cell_type":"code","execution_count":null,"id":"3bb27601-9a81-4791-9dfb-7f16bd4a28a4","metadata":{},"outputs":[],"source":["# Check query resuts\n","\n","display(key_df.head(10))\n","display(key_df.shape)"]},{"cell_type":"code","execution_count":null,"id":"2e5c6685-3939-4e7e-b5dd-802607b5bacf","metadata":{},"outputs":[],"source":["# Make columns hashable\n","\n","def _hashable(x):\n","    if isinstance(x, Mapping):\n","        return tuple(sorted((k, _hashable(v)) for k, v in x.items()))\n","    if isinstance(x, (list, tuple, set)):\n","        return tuple(_hashable(v) for v in x)\n","    return x\n","\n","obj_cols = key_df.select_dtypes(include=\"object\").columns\n","key_df[obj_cols] = key_df[obj_cols].map(_hashable)"]},{"cell_type":"code","execution_count":null,"id":"4c71e176-1349-4c17-a55c-e959e69ad91b","metadata":{},"outputs":[],"source":["# Dedupe and preview (mirror df_of)\n","key_df = key_df.drop_duplicates().reset_index(drop=True)\n","print(f'- Key Map: {len(key_df)} rows (unique)')\n","display(key_df.head(10))\n","display(key_df.shape)"]},{"cell_type":"code","execution_count":null,"id":"f9c94500-e14c-41b8-b101-2e408e5ba492","metadata":{},"outputs":[],"source":["qry_access = '''\n","search DiscoveryAccess where endtime\n","ORDER BY discovery_endtime DESC\n","show\n","#id as \"DiscoveryAccess.id\",\n","#Next:Sequential:Previous:DiscoveryAccess.#id as \"DiscoveryAccess.previous_id\",\n","#Previous:Sequential:Next:DiscoveryAccess.#id as \"DiscoveryAccess.next_id\",\n","endpoint as 'DiscoveryAccess.endpoint',\n","friendlyTime(discovery_starttime) as 'DiscoveryAccess.scan_starttime',\n","friendlyTime(discovery_endtime) as 'DiscoveryAccess.scan_endtime',\n","discovery_endtime as 'DiscoveryAccess.scan_endtime_raw',\n","discovery_endtime as 'DiscoveryAccess.discovery_endtime',\n","whenWasThat(discovery_endtime) as 'DiscoveryAccess.when_last_scan',\n","end_state as 'DiscoveryAccess.end_state',\n","reason as 'DiscoveryAccess.reason_not_updated',\n","result as 'DiscoveryAccess.result',\n","_last_marker as 'DiscoveryAccess._last_marker',\n","_first_marker as 'DiscoveryAccess._first_marker',\n","_last_interesting as 'DiscoveryAccess._last_interesting',\n","__had_inference as 'DiscoveryAccess.__had_inference',\n","best_ip_score as 'DiscoveryAccess.best_ip_score',\n","(#DiscoveryAccess:Metadata:Detail:SessionResult.success or access_success) as 'DiscoveryAccess.access_success',\n","access_failure as 'DiscoveryAccess.access_failure',\n","message as 'DiscoveryAccess.message'\n","'''\n","\n","def build_access_df():\n","    rows = tw_search_all(twsearch, qry_access) or []\n","    return pd.DataFrame(rows)\n","\n","access_df = get_or_build_df(\n","    name=\"access\",\n","    build_fn=build_access_df,\n","    use_cache=True,\n","    force_refresh=False,\n","    max_age_hours=24,        # refresh daily\n","    query_text=qry_access\n",")\n","\n","# Check query resuts\n","display(access_df.head(5))\n","display(access_df.shape)"]},{"cell_type":"code","execution_count":null,"id":"e3513886-ecc4-47b5-b899-a5a0b66383d9","metadata":{},"outputs":[],"source":["qry_deviceinfo = '''\n","search DeviceInfo\n","show\n","#id as \"DeviceInfo.id\",\n","hostname as 'DeviceInfo.hostname',\n","os_type as 'DeviceInfo.os_type',\n","os_class as 'DeviceInfo.os_class',\n","os_version as 'DeviceInfo.os_version',\n","kind as 'DeviceInfo.kind',\n","inferred_kind as 'DeviceInfo.inferred_kind',\n","last_access_method as 'DeviceInfo.last_access_method',\n","probed_os as 'DeviceInfo.probed_os',\n","last_credential as 'DeviceInfo.last_credential',\n","last_slave as 'DeviceInfo.last_slave',\n","__preserved_last_credential as 'DeviceInfo.__preserved_last_credential'\n","'''\n","\n","def build_device_df():\n","    rows = tw_search_all(twsearch, qry_deviceinfo) or []\n","    return pd.DataFrame(rows)\n","\n","device_df = get_or_build_df(\n","    name=\"device_info\",\n","    build_fn=build_device_df,\n","    use_cache=True,\n","    force_refresh=False,\n","    max_age_hours=24,        # refresh daily\n","    query_text=qry_deviceinfo\n",")\n","\n","# Check query resuts\n","display(device_df.head(5))\n","display(device_df.shape)"]},{"cell_type":"code","execution_count":null,"id":"c3d8487e-a778-4dde-9e04-eebb833f71c3","metadata":{},"outputs":[],"source":["qry_run = '''\n","search DiscoveryRun\n","show\n","#id as \"DiscoveryRun.id\",\n","label as 'DiscoveryRun.label',\n","friendlyTime(starttime) as 'DiscoveryRun.starttime',\n","friendlyTime(endtime) as 'DiscoveryRun.endtime'\n","'''\n","\n","def build_run_df():\n","    rows = tw_search_all(twsearch, qry_run) or []\n","    return pd.DataFrame(rows)\n","\n","run_df = get_or_build_df(\n","    name=\"runs\",\n","    build_fn=build_run_df,\n","    use_cache=True,\n","    force_refresh=False,\n","    max_age_hours=24,        # refresh daily\n","    query_text=qry_run\n",")\n","\n","# Check query resuts\n","display(run_df.head(5))\n","display(run_df.shape)"]},{"cell_type":"code","execution_count":null,"id":"5a8a5332-c711-4b48-bcea-9a7a1f641945","metadata":{},"outputs":[],"source":["qry_session = '''\n","search SessionResult\n","show\n","#id as \"SessionResult.id\",\n","success as \"SessionResult.success\",\n","session_type as \"SessionResult.session_type\",\n","provider as \"SessionResult.provider\"\n","'''\n","\n","def build_session_df():\n","    rows = tw_search_all(twsearch, qry_session) or []\n","    return pd.DataFrame(rows)\n","\n","session_df = get_or_build_df(\n","    name=\"sessions\",\n","    build_fn=build_session_df,\n","    use_cache=True,\n","    force_refresh=False,\n","    max_age_hours=24,        # refresh daily\n","    query_text=qry_session\n",")\n","\n","# Check query resuts\n","display(session_df.head(5))\n","display(session_df.shape)"]},{"cell_type":"code","execution_count":null,"id":"ec8f4516-b12d-49ec-b11a-7ab97edea15e","metadata":{},"outputs":[],"source":["qry_inferred = '''\n","search InferredElement\n","show\n","#id as \"InferredElement.id\",\n","__all_ip_addrs as 'InferredElement.__all_ip_addrs'\n","'''\n","\n","def build_inferred_df():\n","    rows = tw_search_all(twsearch, qry_inferred) or []\n","    return pd.DataFrame(rows)\n","\n","inferred_df = get_or_build_df(\n","    name=\"inferred\",\n","    build_fn=build_inferred_df,\n","    use_cache=True,\n","    force_refresh=False,\n","    max_age_hours=24,        # refresh daily\n","    query_text=qry_inferred\n",")\n","\n","# Check query resuts\n","display(inferred_df.head(5))\n","display(inferred_df.shape)"]},{"cell_type":"code","execution_count":null,"id":"fetch-extracts","metadata":{},"outputs":[],"source":["qry_interface = '''\n","search NetworkInterface\n","show\n","#id as \"NetworkInterface.id\",\n","ip_addr as 'NetworkInterface.ip_addr'\n","'''\n","\n","def build_interface_df():\n","    rows = tw_search_all(twsearch, qry_interface) or []\n","    return pd.DataFrame(rows)\n","\n","interface_df = get_or_build_df(\n","    name=\"interfaces\",\n","    build_fn=build_interface_df,\n","    use_cache=True,\n","    force_refresh=False,\n","    max_age_hours=24,        # refresh daily\n","    query_text=qry_interface\n",")\n","\n","# Check query resuts\n","display(interface_df.head(5))\n","display(interface_df.shape)"]},{"cell_type":"markdown","id":"merge-facts","metadata":{},"source":["## Merge Extracts\n","\n","Merges the key map with fact tables to reassemble the wide view, adds helper fields\n","(session_results_logged, previous_end_state, access_method/current_access)."]},{"cell_type":"code","execution_count":null,"id":"merge-code","metadata":{},"outputs":[],"source":["# --- Prep: index and trim columns you need ---\n","acc = access_df.set_index('DiscoveryAccess.id')\n","dev = device_df.set_index('DeviceInfo.id')\n","run = run_df.set_index('DiscoveryRun.id')\n","\n","# Optional tables (only if present & non-empty)\n","ses = session_df.set_index('SessionResult.id') if 'SessionResult.id' in key_df and not session_df.empty else None\n","inf = inferred_df.set_index('InferredElement.id') if 'InferredElement.id' in key_df and not inferred_df.empty else None\n","nic = interface_df.set_index('NetworkInterface.id') if 'NetworkInterface.id' in key_df and not interface_df.empty else None\n","\n","# --- Join chain (all left joins on their IDs) ---\n","merged = (\n","    key_df\n","      .join(acc, on='DiscoveryAccess.id', how='left', rsuffix='_acc')\n","      .join(dev, on='DeviceInfo.id', how='left', rsuffix='_dev')\n","      .join(run, on='DiscoveryRun.id', how='left', rsuffix='_run')\n",")\n","\n","if ses is not None:\n","    merged = merged.join(ses, on='SessionResult.id', how='left', rsuffix='_ses')\n","if inf is not None:\n","    merged = merged.join(inf, on='InferredElement.id', how='left', rsuffix='_inf')\n","if nic is not None:\n","    merged = merged.join(nic, on='NetworkInterface.id', how='left', rsuffix='_nic')\n","\n","# --- Session results flag (fully vectorised) ---\n","if 'SessionResult.provider' in merged.columns:\n","    merged['DiscoveryAccess.session_results_logged'] = (\n","        merged['SessionResult.provider'].isna()\n","        .groupby(merged['DiscoveryAccess.id'])\n","        .transform('any')\n","    )\n","else:\n","    merged['DiscoveryAccess.session_results_logged'] = False\n","\n","# --- Previous end state via map (unchanged logic) ---\n","if 'DiscoveryAccess.previous_id' in merged.columns and 'DiscoveryAccess.end_state' in access_df.columns:\n","    prev_map = access_df.set_index('DiscoveryAccess.id')['DiscoveryAccess.end_state']\n","    merged['DiscoveryAccess.previous_end_state'] = merged['DiscoveryAccess.previous_id'].map(prev_map)\n","else:\n","    merged['DiscoveryAccess.previous_end_state'] = None\n","\n","# --- Access method fields ---\n","merged['DiscoveryAccess.access_method'] = merged.get('DeviceInfo.last_access_method')\n","\n","lam = merged['DeviceInfo.last_access_method']\n","slave = merged['DeviceInfo.last_slave'].astype(bool, errors='ignore')\n","probed = merged['DeviceInfo.probed_os'].astype(bool, errors='ignore')\n","\n","cond1 = lam.isin(['windows', 'rcmd']) & slave\n","merged['DiscoveryAccess.current_access'] = np.where(\n","    cond1, lam, np.where(probed, 'Probe', lam)\n",")\n","\n","print('Merged rows:', len(merged))\n","merged.head(3)\n"]},{"cell_type":"markdown","id":"identities-section","metadata":{},"source":["## Build Identities\n","\n","Retrieves identity hints per endpoint (IPs and names) and builds a simple identity list."]},{"cell_type":"code","execution_count":null,"id":"identities-code","metadata":{},"outputs":[],"source":["qry_device_ids = '''\n","search DiscoveryAccess\n","show\n","#::InferredElement:.name as 'InferredElement.name',\n","#::InferredElement:.hostname as 'InferredElement.hostname',\n","#::InferredElement:.local_fqdn as 'InferredElement.local_fqdn',\n","#::InferredElement:.sysname as 'InferredElement.sysname',\n","endpoint as 'DiscoveryAccess.endpoint',\n","#DiscoveryAccess:Endpoint:Endpoint:Endpoint.endpoint as 'Endpoint.endpoint',\n","#DiscoveryAccess:DiscoveryAccessResult:DiscoveryResult:DiscoveredIPAddressList.#List:List:Member:DiscoveredIPAddress.ip_addr as 'DiscoveredIPAddress.ip_addr',\n","#::InferredElement:.__all_ip_addrs as 'InferredElement.__all_ip_addrs',\n","#::InferredElement:.#DeviceWithInterface:DeviceInterface:InterfaceOfDevice:NetworkInterface.ip_addr as 'NetworkInterface.ip_addr',\n","#::InferredElement:.#DeviceWithInterface:DeviceInterface:InterfaceOfDevice:NetworkInterface.fqdns as 'NetworkInterface.fqdns'\n","'''\n","\n","def build_id_rows():\n","    rows = tw_search_all(twsearch, qry_device_ids) or []\n","    return pd.DataFrame(rows)\n","\n","id_rows = get_or_build_df(\n","    name=\"identity_rows\",\n","    build_fn=build_id_rows,\n","    use_cache=True,\n","    force_refresh=False,\n","    max_age_hours=24,        # refresh daily\n","    query_text=qry_device_ids\n",")\n","\n","print('Identity rows:', len(id_rows))\n","identities = build_identities(id_rows)\n","print('Unique identities:', len(identities))\n","identities[:3]\n"]},{"cell_type":"markdown","id":"dropped-section","metadata":{},"source":["## Dropped Endpoints\n","\n","Fetches dropped endpoints to include in the endpoint-level view."]},{"cell_type":"code","execution_count":null,"id":"dropped-code","metadata":{},"outputs":[],"source":["qry_dropped = '''\n","search DroppedEndpoints\n","show explode endpoints as 'Endpoint',\n","reason as 'Reason_Not_Updated',\n","__reason as 'End_State',\n","friendlyTime(starttime) as 'Start',\n","friendlyTime(endtime) as 'End',\n","endtime as 'End_Raw',\n","whenWasThat(endtime) as 'When_Last_Scan',\n","#EndpointRange:EndpointRange:DiscoveryRun:DiscoveryRun.label as \"Run\"\n","'''\n","\n","def build_dropped():\n","    rows = tw_search_all(twsearch, qry_dropped) or []\n","    return pd.DataFrame(rows)\n","\n","dropped = get_or_build_df(\n","    name=\"dropped\",\n","    build_fn=build_dropped,\n","    use_cache=True,\n","    force_refresh=False,\n","    max_age_hours=24,        # refresh daily\n","    query_text=qry_dropped\n",")\n","\n","print('Dropped rows:', len(dropped))\n","dropped[:3]"]},{"cell_type":"markdown","id":"assemble-section","metadata":{},"source":["## Assemble Endpoint Records\n","\n","Groups merged facts and dropped entries by endpoint, enriches with credential labels,\n","computes consistency and \"when\" buckets, then selects the best/latest record per endpoint."]},{"cell_type":"code","execution_count":null,"id":"assemble-code","metadata":{},"outputs":[],"source":["cred_map = get_credential_map(twcreds)\n","id_map = {ip: ident for ident in identities for ip in (ident.get('list_of_ips') or []) if ip}\n","\n","by_endpoint = {}\n","for _, r in merged.iterrows():\n","    ep = r.get('DiscoveryAccess.endpoint')\n","    if not ep:\n","        continue\n","    by_endpoint.setdefault(ep, {'discos': [], 'dropped': []})['discos'].append(r.to_dict())\n","for r in dropped or []:\n","    ep = r.get('Endpoint')\n","    if not ep:\n","        continue\n","    by_endpoint.setdefault(ep, {'discos': [], 'dropped': []})['dropped'].append(r)\n","\n","def _parse_ts(scan_end_raw, friendly_end):\n","    ts = None\n","    if scan_end_raw:\n","        try:\n","            ts = datetime.datetime.fromisoformat(str(scan_end_raw).replace('Z','+00:00'))\n","        except Exception:\n","            ts = None\n","    if ts is None and friendly_end:\n","        try:\n","            ts = datetime.datetime.strptime(' '.join(str(friendly_end).split(' ')[:2]), '%Y-%m-%d %H:%M:%S')\n","        except Exception:\n","            ts = None\n","    return ts\n","\n","def _consistency(states: List[str]):\n","    states = [s for s in states if s]\n","    if not states:\n","        return None\n","    total = len(states)\n","    counts = {}\n","    for s in states:\n","        counts[s] = counts.get(s, 0) + 1\n","    top = max(counts, key=counts.get)\n","    if counts[top] == total:\n","        return f'Always {top}'\n","    if counts[top] >= total - 2:\n","        return f'Usually {top}'\n","    return f'Most Often {top}'\n","\n","endpoint_rows = []\n","for ep, recs in by_endpoint.items():\n","    ident = id_map.get(ep, {})\n","    names = ident.get('list_of_names') or []\n","    eps = ident.get('list_of_ips') or []\n","\n","    states = [d.get('DiscoveryAccess.end_state') for d in recs['discos']] + [d.get('End_State') for d in recs['dropped']]\n","    consistency = _consistency(states)\n","\n","    endpoint_records = []\n","\n","    for d in recs['discos']:\n","        scan_end_raw = d.get('DiscoveryAccess.scan_endtime_raw')\n","        friendly_end = d.get('DiscoveryAccess.scan_endtime')\n","        ts = _parse_ts(scan_end_raw, friendly_end)\n","        when = calc_when(ts) if ts else None\n","        hostname = d.get('DeviceInfo.hostname') or (names[0] if names else None)\n","        node_kind = d.get('DeviceInfo.kind') or d.get('DeviceInfo.inferred_kind')\n","        last_cred = d.get('DeviceInfo.last_credential')\n","        last_cred_uuid = str(last_cred).split('/')[-1] if last_cred else None\n","        cred_info = cred_map.get(last_cred_uuid or '', {})\n","        endpoint_records.append({\n","            'endpoint': ep,\n","            'hostname': hostname,\n","            'list_of_names': names,\n","            'list_of_endpoints': eps,\n","            'node_kind': node_kind,\n","            'os_type': d.get('DeviceInfo.os_type'),\n","            'os_version': d.get('DeviceInfo.os_version'),\n","            'os_class': d.get('DeviceInfo.os_class'),\n","            'disco_run': d.get('DiscoveryRun.label'),\n","            'run_start': d.get('DiscoveryRun.starttime'),\n","            'run_end': d.get('DiscoveryRun.endtime'),\n","            'scan_start': d.get('DiscoveryAccess.scan_starttime'),\n","            'scan_end': d.get('DiscoveryAccess.scan_endtime'),\n","            'scan_end_raw': scan_end_raw,\n","            'when_was_that': when,\n","            'consistency': consistency,\n","            'current_access': d.get('DiscoveryAccess.current_access'),\n","            'access_method': d.get('DiscoveryAccess.access_method') or d.get('DeviceInfo.last_access_method'),\n","            'inferred_node_updated': d.get('DiscoveryAccess.host_node_updated'),\n","            'reason_not_updated': d.get('DiscoveryAccess.reason_not_updated'),\n","            'end_state': d.get('DiscoveryAccess.end_state'),\n","            'previous_end_state': d.get('DiscoveryAccess.previous_end_state'),\n","            'session_results_logged': d.get('DiscoveryAccess.session_results_logged'),\n","            'last_credential': last_cred_uuid,\n","            'credential_name': cred_info.get('label'),\n","            'credential_login': cred_info.get('username'),\n","            'timestamp': ts,\n","            'da_id': d.get('DiscoveryAccess.id'),\n","            'prev_da_id': d.get('DiscoveryAccess.previous_id'),\n","            'next_node_id': d.get('DiscoveryAccess.next_id'),\n","        })\n","\n","    for d in recs['dropped']:\n","        ts = _parse_ts(d.get('End_Raw'), d.get('End'))\n","        when = calc_when(ts) if ts else None\n","        endpoint_records.append({\n","            'endpoint': ep,\n","            'hostname': names[0] if names else None,\n","            'list_of_names': names,\n","            'list_of_endpoints': eps,\n","            'disco_run': d.get('Run'),\n","            'run_start': d.get('Start'),\n","            'run_end': d.get('End'),\n","            'when_was_that': when,\n","            'consistency': consistency,\n","            'reason_not_updated': d.get('Reason_Not_Updated'),\n","            'end_state': d.get('End_State'),\n","            'timestamp': ts,\n","            'scan_end_raw': d.get('End_Raw'),\n","        })\n","\n","    if not endpoint_records:\n","        continue\n","    # choose latest, but keep identifying fields when available\n","    latest = max(endpoint_records, key=lambda r: r.get('timestamp') or datetime.datetime.min)\n","    named = [r for r in endpoint_records if r.get('hostname') or r.get('credential_name')]\n","    chosen = max(named, key=lambda r:   r.get('timestamp') or datetime.datetime.min) if named else latest\n","    # backfill from latest\n","    for k, v in latest.items():\n","        if chosen.get(k) in (None, '') and v not in (None, ''):\n","            chosen[k] = v\n","    endpoint_rows.append(chosen)\n","\n","len(endpoint_rows)\n"]},{"cell_type":"markdown","id":"format-output","metadata":{},"source":["## Format Output\n","\n","Builds the final DataFrame with the same columns as the DisMAL `discovery_analysis` CSV."]},{"cell_type":"code","execution_count":null,"id":"format-output-code","metadata":{},"outputs":[],"source":["rows = []\n","for r in endpoint_rows:\n","    rows.append([\n","        r.get('endpoint'),\n","        r.get('hostname'),\n","        r.get('list_of_names'),\n","        r.get('list_of_endpoints'),\n","        r.get('node_kind'),\n","        r.get('os_type'),\n","        r.get('os_version'),\n","        r.get('os_class'),\n","        r.get('disco_run'),\n","        r.get('run_start'),\n","        r.get('run_end'),\n","        r.get('scan_start'),\n","        r.get('scan_end'),\n","        r.get('scan_end_raw'),\n","        r.get('when_was_that'),\n","        r.get('consistency'),\n","        r.get('current_access'),\n","        r.get('access_method'),\n","        r.get('inferred_node_updated'),\n","        r.get('reason_not_updated'),\n","        r.get('end_state'),\n","        r.get('previous_end_state'),\n","        (f\"{r.get('previous_end_state')} -> {r.get('end_state')}\" if r.get('end_state') is not None else None),\n","        r.get('session_results_logged'),\n","        r.get('last_credential'),\n","        r.get('credential_name'),\n","        r.get('credential_login'),\n","        r.get('timestamp'),\n","        r.get('da_id'),\n","        r.get('prev_da_id'),\n","        r.get('next_node_id'),\n","    ])\n","headers = [\n","    'endpoint',\n","    'device_name',\n","    'list_of_device_names',\n","    'list_of_endpoints',\n","    'node_kind',\n","    'os_type',\n","    'os_version',\n","    'os_class',\n","    'discovery_run',\n","    'discovery_run_start',\n","    'discovery_run_end',\n","    'scan_start',\n","    'scan_end',\n","    'scan_end_raw',\n","    'when_was_that',\n","    'consistency',\n","    'current_access',\n","    'access_method',\n","    'inferred_node_updated',\n","    'reason_not_updated',\n","    'end_state',\n","    'previous_end_state',\n","    'end_state_change',\n","    'session_results_logged',\n","    'last_credential',\n","    'credential_name',\n","    'credential_login',\n","    'timestamp',\n","    'da_id',\n","    'prev_da_id',\n","    'next_node_id',\n","]\n","df_out = pd.DataFrame(rows, columns=headers)\n","df_out.insert(0, 'Discovery Instance', target)\n","df_out.head(10)\n"]},{"cell_type":"markdown","id":"save-csv","metadata":{},"source":["## Save to CSV\n","\n","Writes to the standard output folder."]},{"cell_type":"code","execution_count":null,"id":"save-csv-code","metadata":{},"outputs":[],"source":["OUTPUT_CSV = str(output_dir / 'discovery_analysis.csv')\n","df_out.to_csv(OUTPUT_CSV, index=False)\n","print(f'Saved to {OUTPUT_CSV}')\n"]},{"cell_type":"code","execution_count":null,"id":"dbff7508-f62a-4163-99e3-bf52a670ad6c","metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.3"}},"nbformat":4,"nbformat_minor":5}