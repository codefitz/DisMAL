{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro-title",
   "metadata": {},
   "source": [
    "# Sensitive Data (BMC Discovery)\n",
    "\n",
    "This notebook reproduces the DisMAL `sensitive_data` report using the Discovery Data API.\n",
    "It runs the same TWQL query, normalizes the results, and writes `sensitive_data.csv` to `output_<target>`.\n",
    "\n",
    "> **NOTE:** May timeout due to limitations of the API if there are a large set of records."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "requirements",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "We use `requests` for HTTP, `pandas` for tabular handling, and `PyYAML` for configuration.\n",
    "Uncomment below to install if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -q requests pandas pyyaml\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "from urllib.parse import urljoin\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "select-appliance",
   "metadata": {},
   "source": [
    "## Select Appliance (optional)\n",
    "If your `config.yaml` defines multiple appliances, set `APPLIANCE_NAME` or `APPLIANCE_INDEX`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "appliance-vars",
   "metadata": {},
   "outputs": [],
   "source": [
    "APPLIANCE_NAME = None   # e.g., 'prod' or 'dev'\n",
    "APPLIANCE_INDEX = 0     # integer index if not using name selection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "config-load",
   "metadata": {},
   "source": [
    "## Configuration (from config.yaml)\n",
    "Locates `../config.yaml`, reads target and token, and prepares the output directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _find_repo_root(start: Path) -> Path:\n",
    "    for p in [start] + list(start.parents):\n",
    "        if (p / 'config.yaml').exists():\n",
    "            return p\n",
    "    return start.parent\n",
    "\n",
    "repo_root = _find_repo_root(Path.cwd())\n",
    "config_path = repo_root / 'config.yaml'\n",
    "with open(config_path, 'r') as fh:\n",
    "    cfg = yaml.safe_load(fh) or {}\n",
    "\n",
    "apps = cfg.get('appliances') or []\n",
    "selected = None\n",
    "if isinstance(apps, list) and apps:\n",
    "    if APPLIANCE_NAME:\n",
    "        selected = next((a for a in apps if a.get('name') == APPLIANCE_NAME), None)\n",
    "        if selected is None:\n",
    "            raise ValueError(f\"No appliance named '{APPLIANCE_NAME}' in config.yaml\")\n",
    "    else:\n",
    "        try:\n",
    "            selected = apps[int(APPLIANCE_INDEX)]\n",
    "        except Exception:\n",
    "            selected = apps[0]\n",
    "\n",
    "target = ((selected or {}).get('target') or cfg.get('target') or '').strip()\n",
    "if not target:\n",
    "    raise ValueError('config.yaml missing \"target\"')\n",
    "BASE_URL = target if ('://' in target) else f'https://{target}'\n",
    "\n",
    "token = (((selected or {}).get('token') or cfg.get('token') or '').strip())\n",
    "token_file = (selected or {}).get('token_file') or cfg.get('token_file') or cfg.get('f_token')\n",
    "if not token and token_file:\n",
    "    tf_path = Path(token_file)\n",
    "    if not tf_path.is_absolute():\n",
    "        tf_path = repo_root / tf_path\n",
    "    with open(tf_path, 'r') as tf:\n",
    "        token = tf.read().strip()\n",
    "if not token:\n",
    "    raise ValueError('API token not found in config.yaml (token or token_file)')\n",
    "\n",
    "API_VERSION = str((selected or {}).get('api_version') or cfg.get('api_version') or 'v1.14')\n",
    "VERIFY_SSL = bool((selected or {}).get('verify_ssl', cfg.get('verify_ssl', True)))\n",
    "\n",
    "sanitized = target.replace('.', '_').replace(':', '_').replace('/', '_')\n",
    "output_dir = repo_root / f'output_{sanitized}'\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print('Base URL      :', BASE_URL)\n",
    "print('API Version   :', API_VERSION)\n",
    "print('Verify SSL    :', VERIFY_SSL)\n",
    "print('Output folder :', output_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "session-helpers",
   "metadata": {},
   "source": [
    "## Session and helpers\n",
    "Create a session and a helper to return a unified object table {'headings': [...], 'results': [...]}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "session-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "session = requests.Session()\n",
    "auth_value = token if token.lower().startswith('bearer ') else f'Bearer {token}'\n",
    "session.headers.update({'Authorization': auth_value, 'Accept': 'application/json'})\n",
    "session.verify = VERIFY_SSL\n",
    "\n",
    "def api_url(path: str) -> str:\n",
    "    base = BASE_URL.rstrip('/') + f'/api/{API_VERSION}/'\n",
    "    return urljoin(base, path.lstrip('/'))\n",
    "\n",
    "def post_search(query: str, *, limit: int | None = None, page_size: int = 500):\n",
    "    url = api_url('data/search')\n",
    "    headings = None\n",
    "    results = []\n",
    "    offset = 0\n",
    "    fetch_all = (limit == 0)\n",
    "    while True:\n",
    "        payload = {'query': query, 'format': 'object'}\n",
    "        if fetch_all:\n",
    "            payload['limit'] = page_size\n",
    "            if offset:\n",
    "                payload['offset'] = offset\n",
    "        elif limit is not None:\n",
    "            payload['limit'] = limit\n",
    "        r = session.post(url, json=payload)\n",
    "        if r.status_code >= 400:\n",
    "            print(f'Error {r.status_code} POST {url}: {r.text[:200]}')\n",
    "            return {'headings': [], 'results': []}\n",
    "        try:\n",
    "            data = r.json()\n",
    "        except Exception:\n",
    "            data = []\n",
    "        table = None\n",
    "        if isinstance(data, list):\n",
    "            for x in data:\n",
    "                if isinstance(x, dict) and 'headings' in x and 'results' in x:\n",
    "                    table = x\n",
    "                    break\n",
    "        elif isinstance(data, dict) and 'headings' in data and 'results' in data:\n",
    "            table = data\n",
    "        if not table:\n",
    "            return {'headings': [], 'results': []}\n",
    "        if headings is None:\n",
    "            headings = table.get('headings', [])\n",
    "        page_rows = table.get('results') or []\n",
    "        results.extend(page_rows)\n",
    "        if not fetch_all or len(page_rows) < page_size:\n",
    "            break\n",
    "        offset += page_size\n",
    "    return {'headings': (headings or []), 'results': results}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "twql",
   "metadata": {},
   "source": [
    "## TWQL for Sensitive Data\n",
    "Mirrors `core/queries.py:sensitive_data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "query-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "qry_sensitive = r'''\n",
    "search DiscoveredProcess\n",
    "where ((args has subword 'user' or args has substring 'username')\n",
    "    and (args has subword 'pass' or args has substring 'password'))\n",
    "  or (args matches regex '(?i)\\s-u(\\s+|=)\\S+'\n",
    "    and args matches regex '(?i)\\s-p(\\s+|=)\\S+')\n",
    "show\n",
    "  #Member:List:List:ProcessList.#DiscoveryResult:DiscoveryAccessResult:DiscoveryAccess:DiscoveryAccess.#DiscoveryAccess:DiscoveryAccessResult:DiscoveryResult:DeviceInfo.hostname as 'Host',\n",
    "  #Member:List:List:ProcessList.#DiscoveryResult:DiscoveryAccessResult:DiscoveryAccess:DiscoveryAccess.endpoint as 'Endpoint',\n",
    "  username,\n",
    "  cmd,\n",
    "  args,\n",
    "  (extract(args, regex '(?i)(user(name)?.*?\\S+)', raw '\\1')\n",
    "      or extract(args, regex '(?i)(-u.*?\\S+)', raw '\\1')) as 'Matched Username String',\n",
    "  extract(args, regex '(?i)(password.*?\\S+|\\s-p.*?\\S+)', raw '\\1') as 'Matched Password String'\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exec",
   "metadata": {},
   "source": [
    "## Execute and save\n",
    "Execute the query, build a DataFrame, insert 'Discovery Instance' as the first column, and write CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run-save",
   "metadata": {},
   "outputs": [],
   "source": [
    "tbl = post_search(qry_sensitive, limit=0)\n",
    "heads = tbl.get('headings', []) if isinstance(tbl, dict) else []\n",
    "rows = tbl.get('results', []) if isinstance(tbl, dict) else []\n",
    "df = pd.DataFrame(rows, columns=heads) if rows else pd.DataFrame()\n",
    "\n",
    "if not df.empty:\n",
    "    df.insert(0, 'Discovery Instance', target)\n",
    "    display(df.head(20))\n",
    "else:\n",
    "    print('No rows returned')\n",
    "\n",
    "OUTPUT_CSV = str(output_dir / 'sensitive_data.csv')\n",
    "if not df.empty:\n",
    "    df.to_csv(OUTPUT_CSV, index=False)\n",
    "    print(f'Saved to {OUTPUT_CSV} (rows: {len(df)})')\n",
    "else:\n",
    "    # Create empty file with headers for consistency\n",
    "    pd.DataFrame(columns=['Discovery Instance'] + heads).to_csv(OUTPUT_CSV, index=False)\n",
    "    print(f'No data; created empty file at {OUTPUT_CSV}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "footer",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
