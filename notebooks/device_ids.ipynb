{"cells":[{"cell_type":"markdown","id":"645dff05","metadata":{},"source":["# Device Identities (device_ids)\n","\n","Fetch device identity rows from every configured BMC Discovery appliance using the Tideway SDK (bulk search), build unique identities per originating endpoint, and save CSVs to the standard DisMAL output folders.\n","\n","> **NOTE:** Due to limitations of the API, this may take a while to run.\n"]},{"cell_type":"code","execution_count":null,"id":"55208880","metadata":{},"outputs":[],"source":["# %pip install -q pandas pyyaml\n","import pandas as pd\n","import yaml\n","from pathlib import Path\n","from typing import Any, Dict, List, Optional\n","import xml.etree.ElementTree as ET\n","import tideway\n","import re\n","import ast\n"]},{"cell_type":"markdown","id":"5a6827aa","metadata":{},"source":["## Query configuration\n","\n","Set the query title and optionally restrict the appliances to run against by name. Leave `APPLIANCE_NAMES` empty to run against every appliance defined in `config.yaml`.\n"]},{"cell_type":"code","execution_count":null,"id":"be80b6ca","metadata":{},"outputs":[],"source":["QUERY_TITLE = 'Device IDs'\n","APPLIANCE_NAMES: List[str] = []  # e.g., ['prod', 'dev']; empty -> all appliances\n"]},{"cell_type":"markdown","id":"6241b8a1","metadata":{},"source":["## Optional filters\n","\n","Apply filters after fetching the results from each appliance to narrow the output.\n"]},{"cell_type":"code","execution_count":null,"id":"cabff28f","metadata":{},"outputs":[],"source":["DEVICE_NAME_FILTER = None  # e.g., 'host-name'\n","INCLUDE_ENDPOINTS: List[str] = []     # e.g., ['10.1.2.3']\n","ENDPOINT_PREFIX = None     # e.g., '10.1.'\n"]},{"cell_type":"markdown","id":"6aa4ae64","metadata":{},"source":["## Load configuration and prepare helpers\n","\n","Locate the repository root, resolve the shared XML query definitions and raw export directories, and build helper utilities for connecting to each appliance when necessary.\n"]},{"cell_type":"code","execution_count":null,"id":"95d53710","metadata":{},"outputs":[],"source":["def _find_repo_root(start: Path) -> Path:\n","    for p in [start] + list(start.parents):\n","        if (p / 'config.yaml').exists():\n","            return p\n","    return start.parent\n","\n","repo_root = _find_repo_root(Path.cwd())\n","\n","queries_xml_candidates = [\n","    repo_root / 'queries' / 'dismal_queries.xml',\n","    repo_root / 'DisMAL' / 'queries' / 'dismal_queries.xml',\n","]\n","for candidate in queries_xml_candidates:\n","    if candidate.exists():\n","        queries_xml = candidate\n","        break\n","else:\n","    raise FileNotFoundError('Unable to locate dismal_queries.xml')\n","\n","def load_query_from_xml(title: str, xml_path: Optional[Path] = None) -> str:\n","    xml_path = xml_path or queries_xml\n","    tree = ET.parse(xml_path)\n","    root = tree.getroot()\n","    for elem in root.findall('query'):\n","        if (elem.get('title') or '').strip() == title:\n","            search_text = (elem.findtext('search') or '').strip()\n","            if not search_text:\n","                raise ValueError(f\"Query '{title}' missing search text\")\n","            return search_text\n","    raise ValueError(f\"Query '{title}' not found in {xml_path}\")\n","\n","QUERY_TEXT = load_query_from_xml(QUERY_TITLE)\n","\n","cfg_path = repo_root / 'config.yaml'\n","cfg: Dict[str, Any] = yaml.safe_load(cfg_path.read_text()) or {}\n","RAW_EXPORT_ROOT = repo_root / 'raw_exports'\n","\n","def _resolve_token(appliance_cfg: Dict[str, Any], global_cfg: Dict[str, Any]) -> str:\n","    token = str(appliance_cfg.get('token') or global_cfg.get('token') or '').strip()\n","    token_file = appliance_cfg.get('token_file') or global_cfg.get('token_file') or global_cfg.get('f_token')\n","    if not token and token_file:\n","        tf_path = Path(token_file)\n","        if not tf_path.is_absolute():\n","            tf_path = repo_root / tf_path\n","        token = tf_path.read_text().strip()\n","    if not token:\n","        raise ValueError('API token not found for appliance (token or token_file)')\n","    return token\n","\n","def _sanitize_target(target: str) -> str:\n","    return target.replace('.', '_').replace(':', '_').replace('/', '_')\n","\n","def _slugify(value: str) -> str:\n","    slug = re.sub(r'[^A-Za-z0-9]+', '_', value).strip('_').lower()\n","    return slug or 'unnamed'\n","\n","def build_appliance_contexts(include_names: Optional[List[str]] = None) -> List[Dict[str, Any]]:\n","    include = [name.strip() for name in (include_names or []) if str(name).strip()]\n","    appliances = cfg.get('appliances') or []\n","    contexts: List[Dict[str, Any]] = []\n","\n","    if appliances:\n","        iterable = list(enumerate(appliances))\n","    else:\n","        iterable = [(0, cfg)]\n","\n","    for index, raw in iterable:\n","        name = str(raw.get('name') or raw.get('target') or f'appliance_{index + 1}').strip()\n","        if include and name not in include:\n","            continue\n","\n","        target = str(raw.get('target') or cfg.get('target') or '').strip()\n","        if not target:\n","            print(f\"Skipping {name or f'appliance_{index + 1}'}: missing target\")\n","            continue\n","\n","        token = None\n","        token_error: Optional[Exception] = None\n","        try:\n","            token = _resolve_token(raw, cfg)\n","        except Exception as exc:\n","            token_error = exc\n","\n","        api_version = str(raw.get('api_version') or cfg.get('api_version') or 'v1.14')\n","        verify_ssl = bool(raw.get('verify_ssl', cfg.get('verify_ssl', True)))\n","        output_dir = repo_root / ('output_' + _sanitize_target(target))\n","        output_dir.mkdir(parents=True, exist_ok=True)\n","\n","        slug = _slugify(name or target)\n","        raw_export_dir = RAW_EXPORT_ROOT / slug\n","        raw_csv_path = raw_export_dir / 'device_ids.csv'\n","\n","        contexts.append({\n","            'name': name or target,\n","            'target': target,\n","            'token': token,\n","            'token_error': token_error,\n","            'api_version': api_version,\n","            'verify_ssl': verify_ssl,\n","            'output_dir': output_dir,\n","            'raw_export_dir': raw_export_dir,\n","            'raw_csv_path': raw_csv_path,\n","            'app': None,\n","            'data': None,\n","        })\n","\n","    if include and not contexts:\n","        raise RuntimeError(f\"No appliances matched the provided names: {include}\")\n","    if not contexts:\n","        raise RuntimeError('No appliances could be initialised from config.yaml')\n","\n","    return contexts\n","\n","def initialise_instances(include_names: Optional[List[str]] = None) -> List[Dict[str, Any]]:\n","    contexts = build_appliance_contexts(include_names=include_names)\n","    for ctx in contexts:\n","        print(f\"Configured {ctx['name']} ({ctx['target']}):\")\n","        raw_csv = ctx['raw_csv_path']\n","        if raw_csv.exists():\n","            print('  Raw export    :', raw_csv)\n","        else:\n","            print('  Raw export    : missing -> will attempt API fallback')\n","            if ctx['token'] is None:\n","                warn = ctx.get('token_error') or 'API token not available'\n","                print('  Warning       :', warn)\n","            else:\n","                print('  API fallback  : available')\n","        print('  Output dir    :', ctx['output_dir'])\n","    return contexts\n","\n","NAME_COLUMNS = [\n","    'InferredElement.name',\n","    'InferredElement.hostname',\n","    'InferredElement.local_fqdn',\n","    'InferredElement.sysname',\n","    'NetworkInterface.fqdns',\n","]\n","IP_COLUMNS = [\n","    'DiscoveryAccess.endpoint',\n","    'Endpoint.endpoint',\n","    'DiscoveredIPAddress.ip_addr',\n","    'InferredElement.__all_ip_addrs',\n","    'NetworkInterface.ip_addr',\n","]\n","ENDPOINT_COLUMN = 'DiscoveryAccess.endpoint'\n","\n","def _coerce_iterable(value):\n","    if isinstance(value, list):\n","        return value\n","    if value is None or pd.isna(value):\n","        return []\n","    if isinstance(value, str):\n","        stripped = value.strip()\n","        if not stripped or stripped.lower() in {'none', 'nan'}:\n","            return []\n","        if stripped.startswith('[') and stripped.endswith(']'):\n","            try:\n","                parsed = ast.literal_eval(stripped)\n","            except Exception:\n","                return [stripped]\n","            if isinstance(parsed, list):\n","                return [item for item in parsed if item not in (None, 'None')]\n","            if parsed in (None,):\n","                return []\n","            return [parsed]\n","        return [value]\n","    return [value]\n","\n","def _ensure_api_data(instance: Dict[str, Any]):\n","    if instance.get('data') is not None:\n","        return instance['data']\n","    if instance.get('token') is None:\n","        token_error = instance.get('token_error') or 'API token unavailable'\n","        raise RuntimeError(f\"API fallback unavailable: {token_error}\")\n","    api_number = (instance.get('api_version') or '').lstrip('vV')\n","    app = tideway.appliance(instance['target'], instance['token'], api_version=api_number or None, ssl_verify=instance['verify_ssl'])\n","    data_ep = app.data()\n","    try:\n","        about = app.api_about\n","        status = getattr(about, 'status_code', 'ok')\n","        print('  About status   :', status)\n","    except Exception as exc:\n","        print('  Warning        : failed to reach /api/about ->', exc)\n","    instance['app'] = app\n","    instance['data'] = data_ep\n","    return data_ep\n","\n","def fetch_device_rows(instance: Dict[str, Any]) -> pd.DataFrame:\n","    csv_path = instance.get('raw_csv_path')\n","    if csv_path and csv_path.exists():\n","        print('  Source         : raw CSV export')\n","        return pd.read_csv(csv_path, na_values=['None'])\n","\n","    print('  Source         : API query (CSV missing)')\n","    try:\n","        data_ep = _ensure_api_data(instance)\n","    except Exception as exc:\n","        print('  Error          :', exc)\n","        return pd.DataFrame()\n","\n","    payload = data_ep.search({'query': QUERY_TEXT}, format='object', limit=0)\n","\n","    if hasattr(payload, 'json'):\n","        payload = payload.json()\n","\n","    if isinstance(payload, dict):\n","        payload = payload.get('results') or payload.get('result') or []\n","\n","    if not payload:\n","        return pd.DataFrame()\n","\n","    if isinstance(payload, list):\n","        first = payload[0] if payload else None\n","        if isinstance(first, list):\n","            headers, *rows = payload\n","            payload = [dict(zip(headers, row)) for row in rows]\n","        elif not isinstance(first, dict):\n","            return pd.DataFrame()\n","    else:\n","        return pd.DataFrame()\n","\n","    return pd.json_normalize(payload)\n","\n","def prepare_device_identities(rows: pd.DataFrame) -> tuple[pd.DataFrame, pd.DataFrame]:\n","    if rows.empty:\n","        empty_cols = [ENDPOINT_COLUMN, 'List of IPs', 'List of Names']\n","        return rows.copy(), pd.DataFrame(columns=empty_cols)\n","\n","    df = rows.copy()\n","\n","    required_cols = set(NAME_COLUMNS + IP_COLUMNS + [ENDPOINT_COLUMN])\n","    for col in required_cols:\n","        if col not in df.columns:\n","            df[col] = pd.NA\n","\n","    if DEVICE_NAME_FILTER:\n","        needle = str(DEVICE_NAME_FILTER).lower()\n","        name_mask = pd.Series(False, index=df.index)\n","        for col in NAME_COLUMNS:\n","            col_str = df[col].astype(str).str.lower()\n","            name_mask = name_mask | col_str.str.contains(needle, na=False)\n","        df = df[name_mask]\n","\n","    if INCLUDE_ENDPOINTS:\n","        df = df[df[ENDPOINT_COLUMN].isin(INCLUDE_ENDPOINTS)]\n","    elif ENDPOINT_PREFIX:\n","        df = df[df[ENDPOINT_COLUMN].astype(str).str.startswith(str(ENDPOINT_PREFIX))]\n","\n","    if df.empty:\n","        empty_cols = [ENDPOINT_COLUMN, 'List of IPs', 'List of Names']\n","        return df, pd.DataFrame(columns=empty_cols)\n","\n","    df['ips_all'] = df.apply(lambda r: [item for col in IP_COLUMNS for item in _coerce_iterable(r[col])], axis=1)\n","    df['names_all'] = df.apply(lambda r: [item for col in NAME_COLUMNS for item in _coerce_iterable(r[col])], axis=1)\n","\n","    df_ips = df[[ENDPOINT_COLUMN, 'ips_all']].explode('ips_all').explode('ips_all')\n","    df_ips = df_ips[df_ips['ips_all'].notna()]\n","    if not df_ips.empty:\n","        ips_agg = df_ips.groupby(ENDPOINT_COLUMN, dropna=True)['ips_all'].agg(lambda s: sorted(pd.unique(s.astype(str))))\n","    else:\n","        ips_agg = pd.Series(dtype=object, name='ips_all')\n","\n","    df_names = df[[ENDPOINT_COLUMN, 'names_all']].explode('names_all').explode('names_all')\n","    df_names = df_names[df_names['names_all'].notna()]\n","    if not df_names.empty:\n","        names_agg = df_names.groupby(ENDPOINT_COLUMN, dropna=True)['names_all'].agg(lambda s: sorted(pd.unique(s.astype(str))))\n","    else:\n","        names_agg = pd.Series(dtype=object, name='names_all')\n","\n","    agg_df = pd.concat([ips_agg, names_agg], axis=1).reset_index()\n","    agg_df = agg_df.rename(columns={'ips_all': 'List of IPs', 'names_all': 'List of Names'})\n","\n","    return df, agg_df\n"]},{"cell_type":"markdown","id":"da2c0f03","metadata":{},"source":["## Initialise appliances and run query\n","\n","Load each appliance's device ID data from raw CSV exports when present, falling back to live API queries only when necessary.\n"]},{"cell_type":"code","execution_count":null,"id":"ed1e92a5","metadata":{},"outputs":[],"source":["instances = initialise_instances(include_names=APPLIANCE_NAMES)\n","\n","results: List[Dict[str, Any]] = []\n","for inst in instances:\n","    print()\n","    print(f\"Running query for {inst['name']} ({inst['target']}):\")\n","    rows = fetch_device_rows(inst)\n","    print('  Raw rows         :', len(rows))\n","    if not rows.empty:\n","        display(rows.head(5))\n","\n","    filtered_rows, agg_df = prepare_device_identities(rows)\n","    print('  Rows after filter:', len(filtered_rows))\n","\n","    if not filtered_rows.empty:\n","        display(filtered_rows.head(5))\n","\n","    agg_df = agg_df.copy()\n","    agg_df.insert(0, 'Discovery Instance', inst['target'])\n","\n","    if not agg_df.empty:\n","        display(agg_df.head(5))\n","    else:\n","        print('  Aggregated results are empty.')\n","\n","    results.append({'instance': inst, 'rows': rows, 'filtered': filtered_rows, 'aggregated': agg_df})\n"]},{"cell_type":"markdown","id":"cb0c1581","metadata":{},"source":["## Combined results preview\n","\n","Concatenate the aggregated outputs for a quick multi-appliance view.\n"]},{"cell_type":"code","execution_count":null,"id":"678019f6-c67a-40d7-965a-4c875591b062","metadata":{},"outputs":[],"source":["empty_cols = ['Discovery Instance', ENDPOINT_COLUMN, 'List of IPs', 'List of Names']\n","if results:\n","    combined_frames = [item['aggregated'] for item in results if not item['aggregated'].empty]\n","    combined_results = pd.concat(combined_frames, ignore_index=True) if combined_frames else pd.DataFrame(columns=empty_cols)\n","else:\n","    combined_results = pd.DataFrame(columns=empty_cols)\n","\n","display(combined_results.head(10))\n"]},{"cell_type":"markdown","id":"abf28a48-1032-4ccf-9ab7-70c1bedc7a13","metadata":{},"source":["## Save outputs\n","\n","Persist each appliance's aggregated results to its configured output directory.\n"]},{"cell_type":"code","execution_count":null,"id":"29f5d030-4b38-4613-8d48-bd73b1bfe6bc","metadata":{},"outputs":[],"source":["for item in results:\n","    inst = item['instance']\n","    out_df = item['aggregated']\n","    output_csv = inst['output_dir'] / 'device_ids.csv'\n","    out_df.to_csv(output_csv, index=False)\n","    print(f\"Saved to {output_csv} ({len(out_df)} rows)\")\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.13.7"}},"nbformat":4,"nbformat_minor":5}