{"cells":[{"cell_type":"markdown","id":"a59ae008","metadata":{"id":"title"},"source":["# Devices Report (CSV Exports)\n","\n","This notebook rebuilds the DisMAL `devices` report using raw Discovery CSV exports.\n","It reads appliance details from `config.yaml`, loops over each instance, and\n","writes the merged dataset to the standard output folders.\n"]},{"cell_type":"markdown","id":"1f9b875a","metadata":{"id":"requirements"},"source":["## Requirements\n","\n","We rely on `pandas` and `PyYAML` for data wrangling. Uncomment the cell below to install them if needed.\n"]},{"cell_type":"code","execution_count":null,"id":"e48e142e","metadata":{"id":"imports"},"outputs":[],"source":["# %pip install -q pandas pyyaml\n","\n","import math\n","from ast import literal_eval\n","from pathlib import Path\n","from typing import Any, Dict, List\n","\n","import pandas as pd\n","import yaml\n"]},{"cell_type":"markdown","id":"c72f38e7","metadata":{"id":"configuration"},"source":["## Configuration\n","\n","Adjust these values to control where CSV exports are loaded from and where results are written.\n"]},{"cell_type":"code","execution_count":null,"id":"25bf9539","metadata":{"id":"config"},"outputs":[],"source":["# Root folder that contains sub-folders per appliance (e.g., raw_exports/dev)\n","RAW_EXPORT_ROOT = Path(\"../raw_exports\")\n","\n","# Optional filters (set to a list like [\"prod\"] to limit processing)\n","INCLUDE_INSTANCES = None  # or list of appliance names\n","EXCLUDE_INSTANCES = []    # names to skip even if present in config\n","\n","# Optional credential UUID filter (last segment, with or without prefix)\n","DEVICES_WITH_CRED_UUID = None  # e.g., \"7636fe3b4bd69466ab487f0000010700\"\n","\n","# Optional base directory override for outputs (per appliance sub-folder is created automatically)\n","OUTPUT_BASE_DIR = None  # e.g., Path(\"../../csv_outputs\")\n","OUTPUT_FILENAME = \"devices.csv\"\n"]},{"cell_type":"code","execution_count":null,"id":"9a0ffa26","metadata":{"id":"paths_config"},"outputs":[],"source":["def find_repo_root(start: Path) -> Path:\n","    for candidate in [start] + list(start.parents):\n","        if (candidate / \"config.yaml\").exists() or (candidate / \".git\").is_dir():\n","            return candidate\n","    return start\n","\n","NOTEBOOK_DIR = Path.cwd()\n","REPO_ROOT = find_repo_root(NOTEBOOK_DIR)\n","CONFIG_PATH = REPO_ROOT / \"config.yaml\"\n","\n","if not CONFIG_PATH.exists():\n","    raise FileNotFoundError(f\"config.yaml not found at {CONFIG_PATH}\")\n","\n","with CONFIG_PATH.open(\"r\", encoding=\"utf-8\") as fh:\n","    cfg = yaml.safe_load(fh) or {}\n","\n","appliance_entries = cfg.get(\"appliances\") or []\n","if isinstance(appliance_entries, dict):\n","    appliance_entries = [appliance_entries]\n","\n","if not appliance_entries:\n","    default_target = cfg.get(\"target\")\n","    name = cfg.get(\"name\") or (default_target or \"default\")\n","    appliance_entries = [{\"name\": name, \"target\": default_target}]\n","\n","available_appliances: List[Dict[str, Any]] = []\n","for entry in appliance_entries:\n","    name = str(entry.get(\"name\") or \"\").strip()\n","    target = str(entry.get(\"target\") or \"\").strip()\n","    if not name:\n","        continue\n","    available_appliances.append({\"name\": name, \"target\": target})\n","\n","if not available_appliances:\n","    raise ValueError(\"No appliances with a name found in config.yaml\")\n","\n","exports_root = RAW_EXPORT_ROOT if RAW_EXPORT_ROOT.is_absolute() else (NOTEBOOK_DIR / RAW_EXPORT_ROOT).resolve()\n","if not exports_root.exists():\n","    raise FileNotFoundError(f\"Raw export root not found: {exports_root}\")\n","\n","available_dirs = {path.name: path for path in exports_root.iterdir() if path.is_dir()}\n","\n","include_set = set(INCLUDE_INSTANCES or [])\n","exclude_set = set(EXCLUDE_INSTANCES or [])\n","\n","selected_appliances: List[Dict[str, Any]] = []\n","skipped_missing: List[str] = []\n","skipped_filtered: List[str] = []\n","\n","for appliance in available_appliances:\n","    name = appliance[\"name\"]\n","    if include_set and name not in include_set:\n","        skipped_filtered.append(name)\n","        continue\n","    if name in exclude_set:\n","        skipped_filtered.append(name)\n","        continue\n","    export_dir = available_dirs.get(name)\n","    if not export_dir:\n","        skipped_missing.append(name)\n","        continue\n","    selected_appliances.append({\n","        \"name\": name,\n","        \"target\": appliance.get(\"target\", \"\"),\n","        \"export_dir\": export_dir,\n","    })\n","\n","print(f\"Repo root     : {REPO_ROOT}\")\n","print(f\"Config path   : {CONFIG_PATH}\")\n","print(f\"Exports root  : {exports_root}\")\n","print(f\"Appliances in config : {[a['name'] for a in available_appliances]}\")\n","print(f\"Raw export dirs      : {sorted(available_dirs)}\")\n","print(f\"Selected appliances  : {[a['name'] for a in selected_appliances]}\")\n","if skipped_missing:\n","    print(f\"Missing export folders: {skipped_missing}\")\n","if skipped_filtered:\n","    print(f\"Skipped by filter     : {skipped_filtered}\")\n","\n","if not selected_appliances:\n","    raise RuntimeError(\"No appliances selected for processing â€“ check raw exports and filters.\")\n"]},{"cell_type":"code","execution_count":null,"id":"58a2b9ec","metadata":{"id":"helpers"},"outputs":[],"source":["INVALID_STRINGS = {\"\", \"none\", \"nan\", \"null\"}\n","\n","def is_missing(value) -> bool:\n","    if value is None:\n","        return True\n","    if isinstance(value, float):\n","        return math.isnan(value)\n","    if isinstance(value, str):\n","        return value.strip().lower() in INVALID_STRINGS\n","    return False\n","\n","def to_clean_str(value):\n","    if is_missing(value):\n","        return None\n","    return str(value).strip()\n","\n","def parse_listish(value):\n","    if isinstance(value, list):\n","        return [str(v).strip() for v in value if not is_missing(v)]\n","    if is_missing(value):\n","        return []\n","    text = str(value).strip()\n","    if text.startswith(\"[\") and text.endswith(\"]\"):\n","        try:\n","            parsed = literal_eval(text)\n","        except (ValueError, SyntaxError):\n","            return [text]\n","        if isinstance(parsed, list):\n","            return [str(v).strip() for v in parsed if not is_missing(v)]\n","    return [text]\n","\n","def merge_lists(series) -> List[str]:\n","    merged: List[str] = []\n","    for values in series:\n","        if not values:\n","            continue\n","        if isinstance(values, list):\n","            merged.extend(values)\n","    return sorted({item for item in merged if not is_missing(item)})\n","\n","def gather_values(row, columns) -> List[str]:\n","    collected: List[str] = []\n","    for col in columns:\n","        if col not in row:\n","            continue\n","        cell = row[col]\n","        if isinstance(cell, list):\n","            collected.extend(cell)\n","        elif not is_missing(cell):\n","            collected.append(str(cell).strip())\n","    return sorted({item for item in collected if not is_missing(item)})\n","\n","def clean_uuid(value):\n","    text = to_clean_str(value)\n","    if text is None:\n","        return None\n","    return text.split(\"/\")[-1].lower()\n","\n","def to_bool(value) -> bool:\n","    if isinstance(value, bool):\n","        return value\n","    text = to_clean_str(value)\n","    if text is None:\n","        return False\n","    lowered = text.lower()\n","    if lowered in {\"true\", \"1\", \"yes\"}:\n","        return True\n","    if lowered in {\"false\", \"0\", \"no\"}:\n","        return False\n","    return False\n","\n","def collect_unique(series) -> List[str]:\n","    return sorted({str(v).strip() for v in series if not is_missing(v)})\n"]},{"cell_type":"code","execution_count":null,"id":"701f8344","metadata":{"id":"processing"},"outputs":[],"source":["IDENTITY_IP_COLUMNS = [\n","    \"DiscoveryAccess.endpoint\",\n","    \"Endpoint.endpoint\",\n","    \"DiscoveredIPAddress.ip_addr\",\n","    \"InferredElement.__all_ip_addrs\",\n","    \"NetworkInterface.ip_addr\",\n","]\n","IDENTITY_NAME_COLUMNS = [\n","    \"InferredElement.name\",\n","    \"InferredElement.hostname\",\n","    \"InferredElement.local_fqdn\",\n","    \"InferredElement.sysname\",\n","    \"NetworkInterface.fqdns\",\n","]\n","\n","\n","def build_output_dir(target: str) -> Path:\n","    sanitized = (target or \"unknown\").replace(\".\", \"_\").replace(\":\", \"_\").replace(\"/\", \"_\")\n","    if OUTPUT_BASE_DIR is None:\n","        return REPO_ROOT / f\"output_{sanitized}\"\n","    base_root = OUTPUT_BASE_DIR if isinstance(OUTPUT_BASE_DIR, Path) else Path(OUTPUT_BASE_DIR)\n","    base_root = base_root.expanduser().resolve()\n","    return base_root / f\"output_{sanitized}\"\n","\n","\n","def process_instance(instance: Dict[str, Any]) -> Dict[str, Any]:\n","    name = instance[\"name\"]\n","    target = instance.get(\"target\") or name\n","    export_dir: Path = instance[\"export_dir\"]\n","    print(f\"=== Processing {name} ({target}) ===\")\n","    identities_csv = export_dir / \"devices_report_identities.csv\"\n","    last_discovery_csv = export_dir / \"devices_report_last_discovery.csv\"\n","\n","    missing_files = [str(p) for p in [identities_csv, last_discovery_csv] if not p.exists()]\n","    if missing_files:\n","        print(f\"Missing required CSV files for {name}: {missing_files}\")\n","        return {\"instance\": name, \"target\": target, \"output_path\": None, \"rows\": 0, \"status\": \"missing_csv\"}\n","\n","    id_df = pd.read_csv(identities_csv)\n","\n","    if \"DiscoveryAccess.endpoint\" not in id_df.columns:\n","        raise ValueError(\"Identities export missing 'DiscoveryAccess.endpoint' column\")\n","\n","    identity_lists = id_df[\"DiscoveryAccess.endpoint\"].apply(parse_listish)\n","    id_df[\"identity_endpoint\"] = identity_lists.apply(lambda values: values[0] if values else None)\n","    id_df[\"identity_endpoint\"] = id_df[\"identity_endpoint\"].apply(to_clean_str)\n","    id_df[\"DiscoveryAccess.endpoint\"] = identity_lists\n","\n","    for col in IDENTITY_IP_COLUMNS + IDENTITY_NAME_COLUMNS:\n","        if col == \"DiscoveryAccess.endpoint\":\n","            continue\n","        if col in id_df.columns:\n","            id_df[col] = id_df[col].apply(parse_listish)\n","\n","    id_df[\"ips\"] = id_df.apply(lambda row: gather_values(row, IDENTITY_IP_COLUMNS), axis=1)\n","    id_df[\"names\"] = id_df.apply(lambda row: gather_values(row, IDENTITY_NAME_COLUMNS), axis=1)\n","\n","    identities_df = (\n","        id_df[[\"identity_endpoint\", \"ips\", \"names\"]]\n","        .rename(columns={\"identity_endpoint\": \"Identities.endpoint\"})\n","        .groupby(\"Identities.endpoint\", dropna=False)\n","        .agg({\n","            \"ips\": merge_lists,\n","            \"names\": merge_lists,\n","        })\n","        .reset_index()\n","        .rename(columns={\"ips\": \"list_of_ips\", \"names\": \"list_of_names\"})\n","    )\n","\n","    print(f\"Identity rows: {len(id_df)} | Unique endpoints: {len(identities_df)}\")\n","\n","    ld_df = pd.read_csv(last_discovery_csv)\n","\n","    required_ld_columns = [\n","        \"DiscoveryAccess.endpoint\",\n","        \"DeviceInfo.hostname\",\n","        \"DiscoveryAccess.node_kind\",\n","        \"DiscoveryAccess.scan_starttime\",\n","        \"DiscoveryRun.label\",\n","        \"DiscoveryAccess.end_state\",\n","        \"DiscoveryAccess.result\",\n","        \"DiscoveryAccess.current_access\",\n","        \"DiscoveryAccess.host_node_updated\",\n","        \"DiscoveryAccess.scan_endtime_raw\",\n","        \"label\",\n","        \"username\",\n","    ]\n","    for col in required_ld_columns:\n","        if col not in ld_df.columns:\n","            ld_df[col] = None\n","\n","    if \"DeviceInfo.last_credential\" in ld_df.columns:\n","        ld_df[\"last_cred_short\"] = ld_df[\"DeviceInfo.last_credential\"].apply(clean_uuid)\n","    else:\n","        ld_df[\"last_cred_short\"] = None\n","\n","    if \"DiscoveryAccess.host_node_updated\" in ld_df.columns:\n","        ld_df[\"DiscoveryAccess.host_node_updated\"] = ld_df[\"DiscoveryAccess.host_node_updated\"].apply(to_bool)\n","\n","    ld_df[\"scan_end_ts\"] = pd.to_datetime(ld_df.get(\"DiscoveryAccess.scan_endtime_raw\"), utc=True, errors=\"coerce\")\n","    ld_df[\"scan_end_rank\"] = ld_df[\"scan_end_ts\"].apply(lambda ts: ts.value if pd.notna(ts) else -1)\n","\n","    if DEVICES_WITH_CRED_UUID:\n","        want_uuid = clean_uuid(DEVICES_WITH_CRED_UUID)\n","        ld_df = ld_df[ld_df[\"last_cred_short\"] == want_uuid].copy()\n","        print(f\"Filtered last discovery rows to credential {want_uuid} -> {len(ld_df)} rows\")\n","    else:\n","        print(f\"Last discovery rows: {len(ld_df)}\")\n","\n","    output_dir = build_output_dir(target)\n","    output_dir.mkdir(parents=True, exist_ok=True)\n","\n","    ip_map = identities_df[[\"Identities.endpoint\", \"list_of_ips\"]].explode(\"list_of_ips\", ignore_index=True)\n","    ip_map[\"list_of_ips\"] = ip_map[\"list_of_ips\"].apply(to_clean_str)\n","    ip_map = ip_map.dropna(subset=[\"list_of_ips\"])\n","\n","    merged = ip_map.merge(\n","        ld_df,\n","        left_on=\"list_of_ips\",\n","        right_on=\"DiscoveryAccess.endpoint\",\n","        how=\"left\",\n","    )\n","\n","    if \"last_cred_short\" not in merged.columns:\n","        merged[\"last_cred_short\"] = None\n","    merged[\"last_cred_short\"] = merged[\"last_cred_short\"].apply(clean_uuid)\n","\n","    if \"label\" not in merged.columns:\n","        merged[\"label\"] = None\n","    if \"username\" not in merged.columns:\n","        merged[\"username\"] = None\n","\n","    merged[\"cred_display\"] = merged[\"last_cred_short\"].apply(to_clean_str)\n","\n","    if \"scan_end_rank\" not in merged.columns:\n","        merged[\"scan_end_rank\"] = -1\n","    else:\n","        merged[\"scan_end_rank\"] = merged[\"scan_end_rank\"].fillna(-1)\n","\n","    print(f\"Merged rows: {len(merged)}\")\n","\n","    grp = merged.groupby(\"Identities.endpoint\", dropna=False)\n","\n","    agg_device_names = grp[\"DeviceInfo.hostname\"].apply(collect_unique).reset_index(name=\"all_device_names\")\n","    agg_endpoints = grp[\"DiscoveryAccess.endpoint\"].apply(collect_unique).reset_index(name=\"all_endpoints\")\n","    agg_runs = grp[\"DiscoveryRun.label\"].apply(collect_unique).reset_index(name=\"all_discovery_runs\")\n","    agg_creds = grp[\"cred_display\"].apply(collect_unique).reset_index(name=\"all_credentials_used\")\n","\n","    latest_idx = grp[\"scan_end_rank\"].idxmax()\n","    latest = merged.loc[latest_idx.tolist(), [\n","        \"Identities.endpoint\",\n","        \"DiscoveryAccess.endpoint\",\n","        \"DeviceInfo.hostname\",\n","        \"DiscoveryAccess.node_kind\",\n","        \"last_cred_short\",\n","        \"label\",\n","        \"username\",\n","        \"DiscoveryAccess.scan_starttime\",\n","        \"DiscoveryRun.label\",\n","        \"DiscoveryAccess.end_state\",\n","        \"DiscoveryAccess.result\",\n","        \"DiscoveryAccess.current_access\",\n","    ]].copy()\n","\n","    latest = latest.rename(columns={\n","        \"DiscoveryAccess.endpoint\": \"last_scanned_ip\",\n","        \"DeviceInfo.hostname\": \"last_identity\",\n","        \"DiscoveryAccess.node_kind\": \"last_kind\",\n","        \"last_cred_short\": \"last_credential\",\n","        \"label\": \"last_credential_label\",\n","        \"username\": \"last_credential_username\",\n","        \"DiscoveryAccess.scan_starttime\": \"last_start_time\",\n","        \"DiscoveryRun.label\": \"last_run\",\n","        \"DiscoveryAccess.end_state\": \"last_endstate\",\n","        \"DiscoveryAccess.result\": \"last_result\",\n","        \"DiscoveryAccess.current_access\": \"last_access_method\",\n","    })\n","\n","    succ = merged[merged.get(\"DiscoveryAccess.host_node_updated\") == True].copy()\n","    if succ.empty:\n","        last_succ = pd.DataFrame(columns=[\n","            \"Identities.endpoint\",\n","            \"last_successful_identity\",\n","            \"last_successful_ip\",\n","            \"last_successful_credential\",\n","            \"last_successful_credential_label\",\n","            \"last_successful_credential_username\",\n","            \"last_successful_start_time\",\n","            \"last_successful_run\",\n","            \"last_successful_endstate\",\n","        ])\n","    else:\n","        succ_idx = succ.groupby(\"Identities.endpoint\", dropna=False)[\"scan_end_rank\"].idxmax()\n","        last_succ = succ.loc[succ_idx.tolist(), [\n","            \"Identities.endpoint\",\n","            \"DeviceInfo.hostname\",\n","            \"DiscoveryAccess.endpoint\",\n","            \"last_cred_short\",\n","            \"label\",\n","            \"username\",\n","            \"DiscoveryAccess.scan_starttime\",\n","            \"DiscoveryRun.label\",\n","            \"DiscoveryAccess.end_state\",\n","        ]].copy()\n","        last_succ = last_succ.rename(columns={\n","            \"DeviceInfo.hostname\": \"last_successful_identity\",\n","            \"DiscoveryAccess.endpoint\": \"last_successful_ip\",\n","            \"last_cred_short\": \"last_successful_credential\",\n","            \"label\": \"last_successful_credential_label\",\n","            \"username\": \"last_successful_credential_username\",\n","            \"DiscoveryAccess.scan_starttime\": \"last_successful_start_time\",\n","            \"DiscoveryRun.label\": \"last_successful_run\",\n","            \"DiscoveryAccess.end_state\": \"last_successful_endstate\",\n","        })\n","\n","    df_out = (\n","        latest\n","        .merge(agg_device_names, on=\"Identities.endpoint\", how=\"left\")\n","        .merge(agg_endpoints, on=\"Identities.endpoint\", how=\"left\")\n","        .merge(agg_runs, on=\"Identities.endpoint\", how=\"left\")\n","        .merge(agg_creds, on=\"Identities.endpoint\", how=\"left\")\n","        .merge(last_succ, on=\"Identities.endpoint\", how=\"left\")\n","        .merge(identities_df[[\"Identities.endpoint\", \"list_of_names\"]], on=\"Identities.endpoint\", how=\"left\")\n","    )\n","\n","    def fallback_identity(row):\n","        primary = to_clean_str(row.get(\"last_identity\"))\n","        if primary:\n","            return primary\n","        names = row.get(\"list_of_names\")\n","        if isinstance(names, list) and names:\n","            return names[0]\n","        return None\n","\n","    df_out[\"last_identity\"] = df_out.apply(fallback_identity, axis=1)\n","\n","    desired_columns = [\n","        \"last_scanned_ip\",\n","        \"last_identity\",\n","        \"last_kind\",\n","        \"all_device_names\",\n","        \"all_endpoints\",\n","        \"all_credentials_used\",\n","        \"all_discovery_runs\",\n","        \"last_credential\",\n","        \"last_credential_label\",\n","        \"last_credential_username\",\n","        \"last_start_time\",\n","        \"last_run\",\n","        \"last_endstate\",\n","        \"last_result\",\n","        \"last_access_method\",\n","        \"last_successful_identity\",\n","        \"last_successful_ip\",\n","        \"last_successful_credential\",\n","        \"last_successful_credential_label\",\n","        \"last_successful_credential_username\",\n","        \"last_successful_start_time\",\n","        \"last_successful_run\",\n","        \"last_successful_endstate\",\n","    ]\n","\n","    for col in desired_columns:\n","        if col not in df_out.columns:\n","            df_out[col] = None\n","\n","    df_out = df_out[desired_columns]\n","    df_out.insert(0, \"Discovery Instance\", target)\n","\n","    output_csv = output_dir / OUTPUT_FILENAME\n","    df_out.to_csv(output_csv, index=False)\n","\n","    print(f\"Output rows: {len(df_out)} | Saved to {output_csv}\")\n","    display(df_out.head(5))\n","\n","    return {\n","        \"instance\": name,\n","        \"target\": target,\n","        \"output_path\": output_csv,\n","        \"rows\": int(len(df_out)),\n","        \"status\": \"ok\",\n","    }\n"]},{"cell_type":"code","execution_count":null,"id":"49e06c44","metadata":{"id":"run_all"},"outputs":[],"source":["results = []\n","for appliance in selected_appliances:\n","    outcome = process_instance(appliance)\n","    results.append(outcome)\n","\n","summary_df = pd.DataFrame(results)\n","display(summary_df)\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.3"}},"nbformat":4,"nbformat_minor":5}