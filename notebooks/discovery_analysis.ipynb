{"cells":[{"cell_type":"markdown","id":"d432971d","metadata":{"id":"title"},"source":["# Discovery Analysis (CSV Exports)\n","\n","This notebook rebuilds the DisMAL `discovery_analysis` report from raw Discovery CSV exports.\n","It reads appliance definitions from `config.yaml`, iterates over the available export folders,\n","and writes the consolidated dataset for each instance without calling the Discovery API.\n"]},{"cell_type":"markdown","id":"2f989e6e","metadata":{"id":"requirements"},"source":["## Requirements\n","\n","We rely on `pandas`, `numpy`, and `PyYAML` for data wrangling. Uncomment the cell below to install them if needed.\n"]},{"cell_type":"code","execution_count":null,"id":"cefa31d5","metadata":{"id":"imports"},"outputs":[],"source":["# %pip install -q pandas numpy pyyaml\n","\n","import math\n","import datetime as dt\n","from ast import literal_eval\n","from pathlib import Path\n","from typing import Any, Dict, Iterable, List, Optional\n","\n","import numpy as np\n","import pandas as pd\n","import yaml\n","from pandas.errors import EmptyDataError\n"]},{"cell_type":"markdown","id":"b17efc0f","metadata":{"id":"configuration"},"source":["## Configuration\n","\n","Adjust these values to point at your raw CSV exports and to control which instances are processed.\n"]},{"cell_type":"code","execution_count":null,"id":"043d37cb","metadata":{"id":"config"},"outputs":[],"source":["# Root folder that contains raw_exports/<instance> subdirectories\n","RAW_EXPORT_ROOT = Path(\"../raw_exports\")\n","\n","# Optional filters (set INCLUDE_INSTANCES to something like [\"prod\"] to limit processing)\n","INCLUDE_INSTANCES: Optional[Iterable[str]] = None\n","EXCLUDE_INSTANCES: Iterable[str] = ()\n","\n","# Optional credential UUID filter (accepts full path or just the trailing UUID)\n","FILTER_CREDENTIAL_UUID = None  # e.g., \"7636fe3b4bd69466ab487f0000010700\"\n","\n","# Optional override for where outputs are written (per appliance sub-folder is created automatically)\n","OUTPUT_BASE_DIR = None  # e.g., Path(\"../../csv_outputs\")\n","OUTPUT_FILENAME = \"discovery_analysis.csv\"\n"]},{"cell_type":"code","execution_count":null,"id":"b3709fe2","metadata":{"id":"paths_config"},"outputs":[],"source":["def find_repo_root(start: Path) -> Path:\n","    for candidate in [start] + list(start.parents):\n","        if (candidate / \"config.yaml\").exists() or (candidate / \".git\").is_dir():\n","            return candidate\n","    return start\n","\n","NOTEBOOK_DIR = Path.cwd()\n","REPO_ROOT = find_repo_root(NOTEBOOK_DIR)\n","CONFIG_PATH = REPO_ROOT / \"config.yaml\"\n","\n","if not CONFIG_PATH.exists():\n","    raise FileNotFoundError(f\"config.yaml not found at {CONFIG_PATH}\")\n","\n","with CONFIG_PATH.open(\"r\", encoding=\"utf-8\") as fh:\n","    cfg = yaml.safe_load(fh) or {}\n","\n","appliance_entries = cfg.get(\"appliances\") or []\n","if isinstance(appliance_entries, dict):\n","    appliance_entries = [appliance_entries]\n","\n","if not appliance_entries:\n","    fallback_target = cfg.get(\"target\")\n","    fallback_name = cfg.get(\"name\") or (fallback_target or \"default\")\n","    appliance_entries = [{\"name\": fallback_name, \"target\": fallback_target}]\n","\n","available_appliances: List[Dict[str, Any]] = []\n","for entry in appliance_entries:\n","    name = str(entry.get(\"name\") or \"\").strip()\n","    target = str(entry.get(\"target\") or \"\").strip()\n","    if not name:\n","        continue\n","    available_appliances.append({\n","        \"name\": name,\n","        \"target\": target or name,\n","    })\n","\n","if not available_appliances:\n","    raise ValueError(\"No appliances with a name found in config.yaml\")\n","\n","exports_root = RAW_EXPORT_ROOT if RAW_EXPORT_ROOT.is_absolute() else (NOTEBOOK_DIR / RAW_EXPORT_ROOT).resolve()\n","if not exports_root.exists():\n","    raise FileNotFoundError(f\"Raw export root not found: {exports_root}\")\n","\n","include_set = {str(v).strip() for v in (INCLUDE_INSTANCES or []) if str(v).strip()}\n","exclude_set = {str(v).strip() for v in (EXCLUDE_INSTANCES or []) if str(v).strip()}\n","\n","available_dirs = {path.name: path for path in exports_root.iterdir() if path.is_dir()}\n","\n","selected_appliances: List[Dict[str, Any]] = []\n","skipped_missing: List[str] = []\n","skipped_filtered: List[str] = []\n","\n","for appliance in available_appliances:\n","    name = appliance[\"name\"]\n","    if include_set and name not in include_set:\n","        skipped_filtered.append(name)\n","        continue\n","    if name in exclude_set:\n","        skipped_filtered.append(name)\n","        continue\n","    export_dir = available_dirs.get(name)\n","    if not export_dir:\n","        skipped_missing.append(name)\n","        continue\n","    selected_appliances.append({\n","        \"name\": name,\n","        \"target\": appliance.get(\"target\") or name,\n","        \"export_dir\": export_dir,\n","    })\n","\n","print(f\"Repo root         : {REPO_ROOT}\")\n","print(f\"Config path       : {CONFIG_PATH}\")\n","print(f\"Exports root      : {exports_root}\")\n","print(f\"Config appliances : {[a['name'] for a in available_appliances]}\")\n","print(f\"Export directories: {sorted(available_dirs)}\")\n","print(f\"Selected          : {[a['name'] for a in selected_appliances]}\")\n","if skipped_missing:\n","    print(f\"Missing export dirs: {skipped_missing}\")\n","if skipped_filtered:\n","    print(f\"Skipped by filter  : {skipped_filtered}\")\n","\n","if not selected_appliances:\n","    raise RuntimeError(\"No appliances selected for processing â€“ check raw exports and filters.\")\n"]},{"cell_type":"code","execution_count":null,"id":"5521b609","metadata":{"id":"helpers"},"outputs":[],"source":["METADATA_COLUMNS = [\"Appliance Target\", \"Appliance Name\", \"Query Title\"]\n","IDENTITY_IP_COLUMNS = [\n","    \"DiscoveryAccess.endpoint\",\n","    \"Endpoint.endpoint\",\n","    \"DiscoveredIPAddress.ip_addr\",\n","    \"InferredElement.__all_ip_addrs\",\n","    \"NetworkInterface.ip_addr\",\n","]\n","IDENTITY_NAME_COLUMNS = [\n","    \"InferredElement.name\",\n","    \"InferredElement.hostname\",\n","    \"InferredElement.local_fqdn\",\n","    \"InferredElement.sysname\",\n","    \"NetworkInterface.fqdns\",\n","]\n","OUTPUT_COLUMNS = [\n","    \"endpoint\",\n","    \"device_name\",\n","    \"list_of_device_names\",\n","    \"list_of_endpoints\",\n","    \"node_kind\",\n","    \"os_type\",\n","    \"os_version\",\n","    \"os_class\",\n","    \"discovery_run\",\n","    \"discovery_run_start\",\n","    \"discovery_run_end\",\n","    \"scan_start\",\n","    \"scan_end\",\n","    \"scan_end_raw\",\n","    \"when_was_that\",\n","    \"consistency\",\n","    \"current_access\",\n","    \"access_method\",\n","    \"inferred_node_updated\",\n","    \"reason_not_updated\",\n","    \"end_state\",\n","    \"previous_end_state\",\n","    \"end_state_change\",\n","    \"session_results_logged\",\n","    \"last_credential\",\n","    \"credential_name\",\n","    \"credential_login\",\n","    \"timestamp\",\n","    \"da_id\",\n","    \"prev_da_id\",\n","    \"next_node_id\",\n","]\n","\n","INVALID_STRINGS = {\"\", \"none\", \"nan\", \"null\"}\n","\n","def load_csv(path: Path) -> pd.DataFrame:\n","    if not path.exists():\n","        print(f\"Missing CSV: {path}\")\n","        return pd.DataFrame()\n","    try:\n","        return pd.read_csv(path, low_memory=False)\n","    except EmptyDataError:\n","        print(f\"Empty CSV: {path}\")\n","        return pd.DataFrame()\n","\n","def drop_metadata(df: pd.DataFrame) -> pd.DataFrame:\n","    return df.drop(columns=[c for c in METADATA_COLUMNS if c in df.columns], errors=\"ignore\")\n","\n","def ensure_columns(df: pd.DataFrame, columns: List[str]) -> pd.DataFrame:\n","    for col in columns:\n","        if col not in df.columns:\n","            df[col] = None\n","    return df\n","\n","def is_missing(value) -> bool:\n","    if value is None:\n","        return True\n","    if isinstance(value, float) and math.isnan(value):\n","        return True\n","    if isinstance(value, str) and value.strip().lower() in INVALID_STRINGS:\n","        return True\n","    return False\n","\n","def to_clean_str(value) -> Optional[str]:\n","    if is_missing(value):\n","        return None\n","    return str(value).strip()\n","\n","def parse_listish(value):\n","    if isinstance(value, list):\n","        return [to_clean_str(v) for v in value if to_clean_str(v)]\n","    if is_missing(value):\n","        return []\n","    text = str(value).strip()\n","    if text.startswith(\"[\") and text.endswith(\"]\"):\n","        try:\n","            parsed = literal_eval(text)\n","        except (ValueError, SyntaxError):\n","            return [to_clean_str(text)]\n","        if isinstance(parsed, list):\n","            return [to_clean_str(v) for v in parsed if to_clean_str(v)]\n","    return [to_clean_str(text)]\n","\n","def combine_values(row: pd.Series, columns: List[str]) -> List[str]:\n","    collected: List[str] = []\n","    for col in columns:\n","        values = row.get(col, [])\n","        if isinstance(values, list):\n","            collected.extend([v for v in values if to_clean_str(v)])\n","        elif not is_missing(values):\n","            collected.append(str(values).strip())\n","    return sorted({to_clean_str(v) for v in collected if to_clean_str(v)})\n","\n","def merge_lists(series: pd.Series) -> List[str]:\n","    combined: List[str] = []\n","    for values in series:\n","        if not values:\n","            continue\n","        combined.extend(values)\n","    return sorted({to_clean_str(v) for v in combined if to_clean_str(v)})\n","\n","def collect_unique(series: pd.Series) -> List[str]:\n","    return sorted({to_clean_str(v) for v in series if to_clean_str(v)})\n","\n","def clean_uuid(value) -> Optional[str]:\n","    text = to_clean_str(value)\n","    if text is None:\n","        return None\n","    return text.split(\"/\")[-1].lower()\n","\n","def to_bool(value) -> bool:\n","    if isinstance(value, bool):\n","        return value\n","    text = to_clean_str(value)\n","    if text is None:\n","        return False\n","    lowered = text.lower()\n","    if lowered in {\"true\", \"1\", \"yes\"}:\n","        return True\n","    if lowered in {\"false\", \"0\", \"no\"}:\n","        return False\n","    return False\n","\n","def format_timestamp(value) -> Optional[str]:\n","    if isinstance(value, dt.datetime):\n","        if value.tzinfo is None:\n","            value = value.replace(tzinfo=dt.timezone.utc)\n","        return value.isoformat()\n","    return to_clean_str(value)\n","\n","def calc_when(ts: Optional[dt.datetime]) -> Optional[str]:\n","    if ts is None:\n","        return None\n","    if ts.tzinfo is None:\n","        ts = ts.replace(tzinfo=dt.timezone.utc)\n","    delta = dt.datetime.now(dt.timezone.utc) - ts\n","    minutes = delta.total_seconds() / 60.0\n","    bands = [\n","        (60, \"Less than 60 minutes ago\"),\n","        (1440, \"Less than 24 hours ago\"),\n","        (10080, \"Less than 7 days ago\"),\n","        (43830, \"Less than 1 month ago\"),\n","        (131487, \"Less than 3 months ago\"),\n","        (262974, \"Less than 6 months ago\"),\n","        (525949, \"Less than 12 months ago\"),\n","    ]\n","    for threshold, label in bands:\n","        if minutes < threshold:\n","            return label\n","    return \"Over a year ago\"\n","\n","def build_identity_table(id_df: pd.DataFrame) -> pd.DataFrame:\n","    if id_df is None or id_df.empty:\n","        return pd.DataFrame(columns=[\"Identities.endpoint\", \"list_of_ips\", \"list_of_names\"])\n","\n","    work = id_df.copy()\n","    length = len(work)\n","    for col in IDENTITY_IP_COLUMNS + IDENTITY_NAME_COLUMNS:\n","        if col in work.columns:\n","            work[col] = work[col].apply(parse_listish)\n","        else:\n","            work[col] = [[] for _ in range(length)]\n","\n","    primary: List[Optional[str]] = []\n","    for access_list, endpoint_list in zip(\n","        work[\"DiscoveryAccess.endpoint\"],\n","        work[\"Endpoint.endpoint\"] if \"Endpoint.endpoint\" in work.columns else [ [] for _ in range(length) ]\n","    ):\n","        candidate = None\n","        for value in (access_list or []) + (endpoint_list or []):\n","            clean = to_clean_str(value)\n","            if clean:\n","                candidate = clean\n","                break\n","        primary.append(candidate)\n","    work[\"Identities.endpoint\"] = primary\n","    work = work[work[\"Identities.endpoint\"].notna()].copy()\n","\n","    work[\"ips_tmp\"] = work.apply(lambda row: combine_values(row, IDENTITY_IP_COLUMNS), axis=1)\n","    work[\"names_tmp\"] = work.apply(lambda row: combine_values(row, IDENTITY_NAME_COLUMNS), axis=1)\n","\n","    aggregated = (\n","        work.groupby(\"Identities.endpoint\", dropna=False)\n","        .agg({\n","            \"ips_tmp\": merge_lists,\n","            \"names_tmp\": merge_lists,\n","        })\n","        .reset_index()\n","        .rename(columns={\"ips_tmp\": \"list_of_ips\", \"names_tmp\": \"list_of_names\"})\n","    )\n","    return aggregated\n"]},{"cell_type":"code","execution_count":null,"id":"fd534911","metadata":{"id":"processing"},"outputs":[],"source":["def build_output_dir(target: str) -> Path:\n","    sanitized = (target or \"unknown\").replace(\".\", \"_\").replace(\":\", \"_\").replace(\"/\", \"_\")\n","    if OUTPUT_BASE_DIR is None:\n","        return REPO_ROOT / f\"output_{sanitized}\"\n","    base_root = OUTPUT_BASE_DIR if isinstance(OUTPUT_BASE_DIR, Path) else Path(OUTPUT_BASE_DIR)\n","    return base_root.expanduser().resolve() / f\"output_{sanitized}\"\n","\n","\n","def empty_output(label: str) -> pd.DataFrame:\n","    df = pd.DataFrame(columns=OUTPUT_COLUMNS)\n","    df.insert(0, \"Discovery Instance\", label)\n","    return df.iloc[0:0]\n","\n","\n","def process_instance(instance: Dict[str, Any]) -> Dict[str, Any]:\n","    name = instance[\"name\"]\n","    target = instance.get(\"target\") or name\n","    export_dir: Path = instance[\"export_dir\"]\n","    label = target or name\n","    print(f\"=== Processing {name} ({label}) ===\")\n","\n","    output_dir = build_output_dir(label)\n","    output_dir.mkdir(parents=True, exist_ok=True)\n","\n","    def load_export(filename: str) -> pd.DataFrame:\n","        path = export_dir / filename\n","        df = load_csv(path)\n","        return drop_metadata(df)\n","\n","    key_df = load_export(\"discovery_analysis_key_map.csv\")\n","    access_df = load_export(\"discovery_analysis_access_summary.csv\")\n","    device_df = load_export(\"discovery_analysis_deviceinfo.csv\")\n","    run_df = load_export(\"discovery_analysis_discovery_runs.csv\")\n","    session_df = load_export(\"discovery_analysis_session_results.csv\")\n","    dropped_df = load_export(\"discovery_analysis_dropped_endpoints.csv\")\n","    identities_raw = load_export(\"devices_report_identities.csv\")\n","\n","    print(f\"Key map rows         : {len(key_df)}\")\n","    print(f\"Access summary rows  : {len(access_df)}\")\n","    print(f\"Device info rows     : {len(device_df)}\")\n","    print(f\"Discovery run rows   : {len(run_df)}\")\n","    print(f\"Session result rows  : {len(session_df)}\")\n","    print(f\"Dropped endpoint rows: {len(dropped_df)}\")\n","\n","    key_df = ensure_columns(key_df, [\n","        \"DiscoveryAccess.id\",\n","        \"DiscoveryAccess.previous_id\",\n","        \"DiscoveryAccess.next_id\",\n","        \"DeviceInfo.id\",\n","        \"DiscoveryRun.id\",\n","        \"SessionResult.id\",\n","    ])\n","\n","    access_df = ensure_columns(access_df, [\n","        \"DiscoveryAccess.id\",\n","        \"DiscoveryAccess.endpoint\",\n","        \"DiscoveryAccess.scan_starttime\",\n","        \"DiscoveryAccess.scan_endtime\",\n","        \"DiscoveryAccess.scan_endtime_raw\",\n","        \"DiscoveryAccess.when_last_scan\",\n","        \"DiscoveryAccess.current_access\",\n","        \"DiscoveryAccess.node_kind\",\n","        \"DiscoveryAccess.host_node_updated\",\n","        \"DiscoveryAccess.reason_not_updated\",\n","        \"DiscoveryAccess.end_state\",\n","    ])\n","    if \"DiscoveryAccess.host_node_updated\" in access_df.columns:\n","        access_df[\"DiscoveryAccess.host_node_updated\"] = access_df[\"DiscoveryAccess.host_node_updated\"].apply(to_bool)\n","\n","    device_df = ensure_columns(device_df, [\n","        \"DeviceInfo.id\",\n","        \"DeviceInfo.hostname\",\n","        \"DeviceInfo.os_type\",\n","        \"DeviceInfo.os_version\",\n","        \"DeviceInfo.os_class\",\n","        \"DeviceInfo.kind\",\n","        \"DeviceInfo.inferred_kind\",\n","        \"DeviceInfo.last_access_method\",\n","        \"DeviceInfo.probed_os\",\n","        \"DeviceInfo.last_credential\",\n","        \"DeviceInfo.last_slave\",\n","    ])\n","\n","    run_df = ensure_columns(run_df, [\n","        \"DiscoveryRun.id\",\n","        \"DiscoveryRun.label\",\n","        \"DiscoveryRun.starttime\",\n","        \"DiscoveryRun.endtime\",\n","    ])\n","\n","    session_df = ensure_columns(session_df, [\n","        \"SessionResult.id\",\n","        \"SessionResult.provider\",\n","        \"SessionResult.session_type\",\n","        \"SessionResult.success\",\n","    ])\n","\n","    identities_df = build_identity_table(identities_raw)\n","    identity_records = identities_df.to_dict(\"records\")\n","    id_map = {\n","        ip: rec\n","        for rec in identity_records\n","        for ip in rec.get(\"list_of_ips\", []) or []\n","        if ip\n","    }\n","    print(f\"Identity endpoints   : {len(identities_df)}\")\n","\n","    merged = key_df.copy()\n","    if not access_df.empty and \"DiscoveryAccess.id\" in access_df.columns:\n","        merged = merged.merge(access_df, on=\"DiscoveryAccess.id\", how=\"left\")\n","    if not device_df.empty and \"DeviceInfo.id\" in device_df.columns:\n","        merged = merged.merge(device_df, on=\"DeviceInfo.id\", how=\"left\")\n","    if not run_df.empty and \"DiscoveryRun.id\" in run_df.columns:\n","        merged = merged.merge(run_df, on=\"DiscoveryRun.id\", how=\"left\")\n","    if not session_df.empty and \"SessionResult.id\" in session_df.columns:\n","        merged = merged.merge(session_df, on=\"SessionResult.id\", how=\"left\")\n","\n","    if merged.empty or \"DiscoveryAccess.id\" not in merged.columns:\n","        print(\"No merged rows available; writing empty output\")\n","        df_out = empty_output(label)\n","        output_csv = output_dir / OUTPUT_FILENAME\n","        df_out.to_csv(output_csv, index=False)\n","        return {\n","            \"instance\": name,\n","            \"target\": label,\n","            \"output_path\": output_csv,\n","            \"rows\": 0,\n","            \"status\": \"no-data\",\n","        }\n","\n","    if \"DiscoveryAccess.host_node_updated\" in merged.columns:\n","        merged[\"DiscoveryAccess.host_node_updated\"] = merged[\"DiscoveryAccess.host_node_updated\"].apply(to_bool)\n","    else:\n","        merged[\"DiscoveryAccess.host_node_updated\"] = False\n","\n","    if \"DeviceInfo.last_credential\" in merged.columns:\n","        merged[\"last_cred_short\"] = merged[\"DeviceInfo.last_credential\"].apply(clean_uuid)\n","    else:\n","        merged[\"last_cred_short\"] = None\n","\n","    if FILTER_CREDENTIAL_UUID:\n","        wanted = clean_uuid(FILTER_CREDENTIAL_UUID)\n","        merged = merged[merged[\"last_cred_short\"] == wanted].copy()\n","        print(f\"Filtered merged rows to credential {wanted}: {len(merged)}\")\n","\n","    if \"SessionResult.provider\" in merged.columns:\n","        merged[\"DiscoveryAccess.session_results_logged\"] = (\n","            merged.groupby(\"DiscoveryAccess.id\")[\"SessionResult.provider\"].transform(lambda s: s.notna().any()).fillna(False)\n","        )\n","    else:\n","        merged[\"DiscoveryAccess.session_results_logged\"] = False\n","\n","    if not access_df.empty and \"DiscoveryAccess.id\" in access_df.columns and \"DiscoveryAccess.end_state\" in access_df.columns:\n","        prev_map = access_df.set_index(\"DiscoveryAccess.id\")[\"DiscoveryAccess.end_state\"].to_dict()\n","        prev_id_series = merged.get(\"DiscoveryAccess.previous_id\")\n","        if prev_id_series is not None:\n","            merged[\"DiscoveryAccess.previous_end_state\"] = prev_id_series.map(prev_map)\n","        else:\n","            merged[\"DiscoveryAccess.previous_end_state\"] = None\n","    else:\n","        merged[\"DiscoveryAccess.previous_end_state\"] = None\n","\n","    lam = merged.get(\"DeviceInfo.last_access_method\")\n","    if lam is None:\n","        lam = pd.Series([None] * len(merged), index=merged.index)\n","    slave = merged[\"DeviceInfo.last_slave\"].apply(to_bool) if \"DeviceInfo.last_slave\" in merged.columns else pd.Series([False] * len(merged), index=merged.index)\n","    probed = merged[\"DeviceInfo.probed_os\"].apply(to_bool) if \"DeviceInfo.probed_os\" in merged.columns else pd.Series([False] * len(merged), index=merged.index)\n","\n","    lam = lam.astype(\"object\")\n","    cond1 = lam.isin([\"windows\", \"rcmd\"]) & slave\n","    merged[\"DiscoveryAccess.access_method\"] = lam\n","    merged[\"DiscoveryAccess.current_access\"] = np.where(cond1, lam, np.where(probed, \"Probe\", lam))\n","\n","    print(f\"Merged rows          : {len(merged)}\")\n","\n","    dropped_records = dropped_df.to_dict(\"records\") if not dropped_df.empty else []\n","    cred_map: Dict[str, Dict[str, Optional[str]]] = {}\n","\n","    def parse_timestamp(scan_end_raw, friendly_end) -> Optional[dt.datetime]:\n","        ts = None\n","        raw = to_clean_str(scan_end_raw)\n","        if raw:\n","            try:\n","                ts = dt.datetime.fromisoformat(raw.replace(\"Z\", \"+00:00\"))\n","            except Exception:\n","                ts = None\n","        if ts is None:\n","            friendly = to_clean_str(friendly_end)\n","            if friendly:\n","                try:\n","                    ts = dt.datetime.strptime(\" \".join(friendly.split(\" \")[:2]), \"%Y-%m-%d %H:%M:%S\").replace(tzinfo=dt.timezone.utc)\n","                except Exception:\n","                    ts = None\n","        if ts is not None and ts.tzinfo is None:\n","            ts = ts.replace(tzinfo=dt.timezone.utc)\n","        return ts\n","\n","    def compute_consistency(states: List[Optional[str]]) -> Optional[str]:\n","        cleaned = [to_clean_str(s) for s in states if to_clean_str(s)]\n","        if not cleaned:\n","            return None\n","        total = len(cleaned)\n","        counts: Dict[str, int] = {}\n","        for state in cleaned:\n","            counts[state] = counts.get(state, 0) + 1\n","        top = max(counts, key=counts.get)\n","        if counts[top] == total:\n","            return f\"Always {top}\"\n","        if counts[top] >= total - 2:\n","            return f\"Usually {top}\"\n","        return f\"Most Often {top}\"\n","\n","    by_endpoint: Dict[str, Dict[str, List[Dict[str, Any]]]] = {}\n","    for _, row in merged.iterrows():\n","        ep = to_clean_str(row.get(\"DiscoveryAccess.endpoint\"))\n","        if not ep:\n","            continue\n","        by_endpoint.setdefault(ep, {\"discos\": [], \"dropped\": []})[\"discos\"].append(row.to_dict())\n","    for rec in dropped_records:\n","        ep = to_clean_str(rec.get(\"Endpoint\"))\n","        if not ep:\n","            continue\n","        by_endpoint.setdefault(ep, {\"discos\": [], \"dropped\": []})[\"dropped\"].append(rec)\n","\n","    endpoint_rows: List[Dict[str, Any]] = []\n","    for ep, recs in by_endpoint.items():\n","        ident = id_map.get(ep, {})\n","        names = ident.get(\"list_of_names\") or []\n","        eps = ident.get(\"list_of_ips\") or []\n","\n","        states = [r.get(\"DiscoveryAccess.end_state\") for r in recs[\"discos\"]] + [r.get(\"End_State\") for r in recs[\"dropped\"]]\n","        consistency = compute_consistency(states)\n","\n","        endpoint_records: List[Dict[str, Any]] = []\n","\n","        for d in recs[\"discos\"]:\n","            scan_end_raw = d.get(\"DiscoveryAccess.scan_endtime_raw\")\n","            friendly_end = d.get(\"DiscoveryAccess.scan_endtime\")\n","            ts = parse_timestamp(scan_end_raw, friendly_end)\n","            when = calc_when(ts) if ts else None\n","            hostname = d.get(\"DeviceInfo.hostname\") or (names[0] if names else None)\n","            node_kind = d.get(\"DiscoveryAccess.node_kind\") or d.get(\"DeviceInfo.kind\") or d.get(\"DeviceInfo.inferred_kind\")\n","            last_cred_uuid = d.get(\"last_cred_short\")\n","            cred_info = cred_map.get(last_cred_uuid or \"\", {})\n","            endpoint_records.append({\n","                \"endpoint\": ep,\n","                \"hostname\": hostname,\n","                \"list_of_names\": names,\n","                \"list_of_endpoints\": eps,\n","                \"node_kind\": node_kind,\n","                \"os_type\": d.get(\"DeviceInfo.os_type\"),\n","                \"os_version\": d.get(\"DeviceInfo.os_version\"),\n","                \"os_class\": d.get(\"DeviceInfo.os_class\"),\n","                \"disco_run\": d.get(\"DiscoveryRun.label\"),\n","                \"run_start\": d.get(\"DiscoveryRun.starttime\"),\n","                \"run_end\": d.get(\"DiscoveryRun.endtime\"),\n","                \"scan_start\": d.get(\"DiscoveryAccess.scan_starttime\"),\n","                \"scan_end\": d.get(\"DiscoveryAccess.scan_endtime\"),\n","                \"scan_end_raw\": scan_end_raw,\n","                \"when_was_that\": when,\n","                \"consistency\": consistency,\n","                \"current_access\": d.get(\"DiscoveryAccess.current_access\"),\n","                \"access_method\": d.get(\"DiscoveryAccess.access_method\"),\n","                \"inferred_node_updated\": d.get(\"DiscoveryAccess.host_node_updated\"),\n","                \"reason_not_updated\": d.get(\"DiscoveryAccess.reason_not_updated\"),\n","                \"end_state\": d.get(\"DiscoveryAccess.end_state\"),\n","                \"previous_end_state\": d.get(\"DiscoveryAccess.previous_end_state\"),\n","                \"session_results_logged\": d.get(\"DiscoveryAccess.session_results_logged\"),\n","                \"last_credential\": last_cred_uuid,\n","                \"credential_name\": cred_info.get(\"label\"),\n","                \"credential_login\": cred_info.get(\"username\"),\n","                \"timestamp\": ts,\n","                \"da_id\": d.get(\"DiscoveryAccess.id\"),\n","                \"prev_da_id\": d.get(\"DiscoveryAccess.previous_id\"),\n","                \"next_node_id\": d.get(\"DiscoveryAccess.next_id\"),\n","            })\n","\n","        for d in recs[\"dropped\"]:\n","            ts = parse_timestamp(d.get(\"End_Raw\"), d.get(\"End\"))\n","            when = calc_when(ts) if ts else None\n","            endpoint_records.append({\n","                \"endpoint\": ep,\n","                \"hostname\": names[0] if names else None,\n","                \"list_of_names\": names,\n","                \"list_of_endpoints\": eps,\n","                \"disco_run\": d.get(\"Run\"),\n","                \"run_start\": d.get(\"Start\"),\n","                \"run_end\": d.get(\"End\"),\n","                \"when_was_that\": when,\n","                \"consistency\": consistency,\n","                \"reason_not_updated\": d.get(\"Reason_Not_Updated\"),\n","                \"end_state\": d.get(\"End_State\"),\n","                \"timestamp\": ts,\n","                \"scan_end_raw\": d.get(\"End_Raw\"),\n","            })\n","\n","        if not endpoint_records:\n","            continue\n","        latest = max(endpoint_records, key=lambda r: r.get(\"timestamp\") or dt.datetime.min.replace(tzinfo=dt.timezone.utc))\n","        named = [r for r in endpoint_records if r.get(\"hostname\") or r.get(\"credential_name\")]\n","        chosen = max(named, key=lambda r: r.get(\"timestamp\") or dt.datetime.min.replace(tzinfo=dt.timezone.utc)) if named else latest\n","        for key, value in latest.items():\n","            if chosen.get(key) in (None, \"\") and value not in (None, \"\"):\n","                chosen[key] = value\n","        endpoint_rows.append(chosen)\n","\n","    rows = []\n","    for rec in endpoint_rows:\n","        rows.append([\n","            rec.get(\"endpoint\"),\n","            rec.get(\"hostname\"),\n","            rec.get(\"list_of_names\"),\n","            rec.get(\"list_of_endpoints\"),\n","            rec.get(\"node_kind\"),\n","            rec.get(\"os_type\"),\n","            rec.get(\"os_version\"),\n","            rec.get(\"os_class\"),\n","            rec.get(\"disco_run\"),\n","            rec.get(\"run_start\"),\n","            rec.get(\"run_end\"),\n","            rec.get(\"scan_start\"),\n","            rec.get(\"scan_end\"),\n","            rec.get(\"scan_end_raw\"),\n","            rec.get(\"when_was_that\"),\n","            rec.get(\"consistency\"),\n","            rec.get(\"current_access\"),\n","            rec.get(\"access_method\"),\n","            rec.get(\"inferred_node_updated\"),\n","            rec.get(\"reason_not_updated\"),\n","            rec.get(\"end_state\"),\n","            rec.get(\"previous_end_state\"),\n","            (f\"{rec.get('previous_end_state')} -> {rec.get('end_state')}\" if rec.get('end_state') is not None else None),\n","            rec.get(\"session_results_logged\"),\n","            rec.get(\"last_credential\"),\n","            rec.get(\"credential_name\"),\n","            rec.get(\"credential_login\"),\n","            format_timestamp(rec.get(\"timestamp\")),\n","            rec.get(\"da_id\"),\n","            rec.get(\"prev_da_id\"),\n","            rec.get(\"next_node_id\"),\n","        ])\n","\n","    df_out = pd.DataFrame(rows, columns=OUTPUT_COLUMNS)\n","    df_out.insert(0, \"Discovery Instance\", label)\n","\n","    output_csv = output_dir / OUTPUT_FILENAME\n","    df_out.to_csv(output_csv, index=False)\n","\n","    print(f\"Output rows: {len(df_out)} | Saved to {output_csv}\")\n","    display(df_out.head(5))\n","\n","    return {\n","        \"instance\": name,\n","        \"target\": label,\n","        \"output_path\": output_csv,\n","        \"rows\": int(len(df_out)),\n","        \"status\": \"ok\",\n","    }\n"]},{"cell_type":"code","execution_count":null,"id":"4388451f","metadata":{"id":"run_all"},"outputs":[],"source":["results: List[Dict[str, Any]] = []\n","for appliance in selected_appliances:\n","    outcome = process_instance(appliance)\n","    results.append(outcome)\n","\n","summary_df = pd.DataFrame(results)\n","if \"output_path\" in summary_df.columns:\n","    summary_df[\"output_path\"] = summary_df[\"output_path\"].map(lambda p: str(p) if p is not None else None)\n","display(summary_df)\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.3"}},"nbformat":4,"nbformat_minor":5}