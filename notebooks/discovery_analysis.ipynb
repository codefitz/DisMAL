{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro-title",
   "metadata": {},
   "source": [
    "# Discovery Analysis Report (BMC Discovery)\n",
    "\n",
    "This notebook reproduces the DisMAL `discovery_analysis` report but runs independently of DisMAL modules.\n",
    "It uses the Tideway Python library for queries, is broken into small sections,\n",
    "and saves the final CSV to the standard `output_<target>` folder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "requirements",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "\n",
    "Uncomment the next cell to install dependencies in your environment if needed.\n",
    "\n",
    "> **NOTE:** Right now this takes a long time to run over API if there are a lot of DiscoveryAccesses. This is the way the API an search is designed, not much of a way around it at current!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -q tideway pandas pyyaml\n",
    "\n",
    "import os, sys, json, datetime\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List\n",
    "import pandas as pd\n",
    "import yaml\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "select-appliance",
   "metadata": {},
   "source": [
    "## Select Appliance (optional)\n",
    "\n",
    "If `config.yaml` has multiple appliances, set by name or index. Defaults to the first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "appliance-vars",
   "metadata": {},
   "outputs": [],
   "source": [
    "APPLIANCE_NAME = None   # e.g., 'prod' or 'dev'\n",
    "APPLIANCE_INDEX = 0     # integer index if not using name selection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "config-load",
   "metadata": {},
   "source": [
    "## Configuration (from config.yaml)\n",
    "\n",
    "Finds `config.yaml` in the repo root (or its parents), reads `target`, token/token_file, API version, and SSL flag.\n",
    "Saves the CSV to `output_<target>/discovery_analysis.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _find_repo_root(start: Path) -> Path:\n",
    "    for p in [start] + list(start.parents):\n",
    "        if (p / 'config.yaml').exists():\n",
    "            return p\n",
    "    return start.parent\n",
    "\n",
    "repo_root = _find_repo_root(Path.cwd())\n",
    "config_path = repo_root / 'config.yaml'\n",
    "with open(config_path, 'r') as fh:\n",
    "    cfg = yaml.safe_load(fh) or {}\n",
    "\n",
    "apps = cfg.get('appliances') or []\n",
    "selected = None\n",
    "if isinstance(apps, list) and apps:\n",
    "    if APPLIANCE_NAME:\n",
    "        selected = next((a for a in apps if a.get('name') == APPLIANCE_NAME), None)\n",
    "        if selected is None:\n",
    "            raise ValueError(f\"No appliance named '{APPLIANCE_NAME}' in config.yaml\")\n",
    "    else:\n",
    "        try:\n",
    "            selected = apps[int(APPLIANCE_INDEX)]\n",
    "        except Exception:\n",
    "            selected = apps[0]\n",
    "\n",
    "target = ((selected or {}).get('target') or cfg.get('target') or '').strip()\n",
    "if not target:\n",
    "    raise ValueError('config.yaml missing \"target\"')\n",
    "\n",
    "token = (((selected or {}).get('token') or cfg.get('token') or '').strip())\n",
    "token_file = (selected or {}).get('token_file') or cfg.get('token_file') or cfg.get('f_token')\n",
    "if not token and token_file:\n",
    "    tf_path = Path(token_file)\n",
    "    if not tf_path.is_absolute():\n",
    "        tf_path = repo_root / tf_path\n",
    "    with open(tf_path, 'r') as tf:\n",
    "        token = tf.read().strip()\n",
    "if not token:\n",
    "    raise ValueError('API token not found in config.yaml (token or token_file)')\n",
    "\n",
    "API_VERSION = str((selected or {}).get('api_version') or cfg.get('api_version') or 'v1.14')\n",
    "VERIFY_SSL = bool((selected or {}).get('verify_ssl', cfg.get('verify_ssl', True)))\n",
    "\n",
    "sanitized = target.replace('.', '_').replace(':', '_').replace('/', '_')\n",
    "output_dir = repo_root / f'output_{sanitized}'\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print('Base Host     :', target)\n",
    "print('API Version   :', API_VERSION)\n",
    "print('Verify SSL    :', VERIFY_SSL)\n",
    "print('Output folder :', output_dir)\n",
    "\n",
    "# Prefer local Tideway package if present\n",
    "local_tideway = repo_root / 'Tideway'\n",
    "if local_tideway.exists():\n",
    "    sys.path.insert(0, str(local_tideway))\n",
    "import importlib\n",
    "tideway = importlib.import_module('tideway')\n",
    "API_VERSION_NUM = API_VERSION.lstrip('v')\n",
    "app = tideway.appliance(target, token, api_version=API_VERSION_NUM, ssl_verify=VERIFY_SSL)\n",
    "twsearch = app.data()\n",
    "twcreds = app.credentials()\n",
    "try:\n",
    "    about = app.api_about\n",
    "    print('Appliance reachable:', about.status_code)\n",
    "except Exception as e:\n",
    "    print('Warning: failed to contact appliance /api/about:', e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "helpers",
   "metadata": {},
   "source": [
    "## Helpers\n",
    "\n",
    "Utility helpers for Tideway result normalization, identity building, and credential mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "helpers-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_table_to_json(table_like):\n",
    "    if not table_like or not isinstance(table_like, list):\n",
    "        return []\n",
    "    if not table_like or not isinstance(table_like[0], list):\n",
    "        return []\n",
    "    headers = table_like[0]\n",
    "    rows = table_like[1:]\n",
    "    out = []\n",
    "    for r in rows:\n",
    "        try:\n",
    "            out.append(dict(zip(headers, r)))\n",
    "        except Exception:\n",
    "            continue\n",
    "    return out\n",
    "\n",
    "def to_rows(payload):\n",
    "    if isinstance(payload, list):\n",
    "        if payload and isinstance(payload[0], list):\n",
    "            return list_table_to_json(payload)\n",
    "        if payload and isinstance(payload[0], dict):\n",
    "            return payload\n",
    "        return []\n",
    "    if hasattr(payload, 'json'):\n",
    "        try:\n",
    "            js = payload.json()\n",
    "        except Exception:\n",
    "            return []\n",
    "        if isinstance(js, list):\n",
    "            if js and isinstance(js[0], list):\n",
    "                return list_table_to_json(js)\n",
    "            if js and isinstance(js[0], dict):\n",
    "                return js\n",
    "        if isinstance(js, dict) and 'results' in js and 'headings' in js:\n",
    "            table_like = [js['headings']] + list(js.get('results') or [])\n",
    "            return list_table_to_json(table_like)\n",
    "        return []\n",
    "    if isinstance(payload, dict) and 'results' in payload and 'headings' in payload:\n",
    "        table_like = [payload['headings']] + list(payload.get('results') or [])\n",
    "        return list_table_to_json(table_like)\n",
    "    return []\n",
    "\n",
    "def tw_search_all(search, query: str, limit: int = 500):\n",
    "    resp = search.search({'query': query}, format='object', limit=limit)\n",
    "    return to_rows(resp)\n",
    "\n",
    "def flatten_list(value):\n",
    "    if value is None:\n",
    "        return []\n",
    "    if isinstance(value, list):\n",
    "        out = []\n",
    "        for v in value:\n",
    "            if isinstance(v, list):\n",
    "                out.extend(v)\n",
    "            else:\n",
    "                out.append(v)\n",
    "        return out\n",
    "    return [value]\n",
    "\n",
    "def sort_unique(items):\n",
    "    return sorted(set([x for x in items if x is not None]))\n",
    "\n",
    "def get_credential_map(twcreds_handle):\n",
    "    resp = twcreds_handle.get_vault_credentials\n",
    "    try:\n",
    "        items = resp.json()\n",
    "    except Exception:\n",
    "        items = []\n",
    "    mapping = {}\n",
    "    for c in items or []:\n",
    "        if not isinstance(c, dict):\n",
    "            continue\n",
    "        uuid = str(c.get('uuid') or '').split('/')[-1]\n",
    "        mapping[uuid] = {\n",
    "            'label': c.get('label'),\n",
    "            'username': c.get('username')\n",
    "                        or c.get('snmp.v3.securityname')\n",
    "                        or c.get('aws.access_key_id')\n",
    "                        or c.get('azure.application_id'),\n",
    "        }\n",
    "    return mapping\n",
    "\n",
    "def build_identities(id_rows: List[Dict[str, Any]]):\n",
    "    endpoint_map: Dict[str, Dict[str, set]] = {}\n",
    "    ip_fields = [\n",
    "        'DiscoveryAccess.endpoint', 'Endpoint.endpoint',\n",
    "        'DiscoveredIPAddress.ip_addr', 'InferredElement.__all_ip_addrs',\n",
    "        'NetworkInterface.ip_addr'\n",
    "    ]\n",
    "    name_fields = [\n",
    "        'InferredElement.name', 'InferredElement.hostname',\n",
    "        'InferredElement.local_fqdn', 'InferredElement.sysname',\n",
    "        'NetworkInterface.fqdns'\n",
    "    ]\n",
    "    for rec in id_rows or []:\n",
    "        ep = rec.get('DiscoveryAccess.endpoint')\n",
    "        if not ep:\n",
    "            continue\n",
    "        data = endpoint_map.setdefault(ep, {'ips': set(), 'names': set()})\n",
    "        for f in ip_fields:\n",
    "            data['ips'].update([v for v in flatten_list(rec.get(f)) if v])\n",
    "        for f in name_fields:\n",
    "            data['names'].update([v for v in flatten_list(rec.get(f)) if v])\n",
    "    identities = []\n",
    "    for ep, sets in endpoint_map.items():\n",
    "        ips = sort_unique(list(sets['ips']))\n",
    "        names = sort_unique(list(sets['names']))\n",
    "        identities.append({'originating_endpoint': ep, 'list_of_ips': ips, 'list_of_names': names})\n",
    "    return identities\n",
    "\n",
    "def calc_when(ts: datetime.datetime):\n",
    "    bins = [0, 59, 1440, 10080, 43830, 131487, 262974, 525949, 525950]\n",
    "    labels = [\n",
    "        'Less than 60 minutes ago',\n",
    "        'Less than 24 hours ago',\n",
    "        'Less than 7 days ago',\n",
    "        'Less than 1 month ago',\n",
    "        'Less than 3 months ago',\n",
    "        'Less than 6 months ago',\n",
    "        'Less than 12 months ago',\n",
    "        'Over a year ago',\n",
    "    ]\n",
    "    if ts is None:\n",
    "        return None\n",
    "    delta = datetime.datetime.now(ts.tzinfo) - ts\n",
    "    mins = delta.days * 24 * 60 + delta.seconds / 60\n",
    "    df = pd.DataFrame({'in_minutes': [mins]})\n",
    "    df['when'] = pd.cut(df['in_minutes'], bins=bins, labels=labels, right=False)\n",
    "    return df.to_dict().get('when').get(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "queries",
   "metadata": {},
   "source": [
    "## Queries\n",
    "\n",
    "TWQL fragments to stitch discovery access facts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "queries-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "qry_device_ids = '''\n",
    "search DiscoveryAccess\n",
    "show\n",
    "#::InferredElement:.name as 'InferredElement.name',\n",
    "#::InferredElement:.hostname as 'InferredElement.hostname',\n",
    "#::InferredElement:.local_fqdn as 'InferredElement.local_fqdn',\n",
    "#::InferredElement:.sysname as 'InferredElement.sysname',\n",
    "endpoint as 'DiscoveryAccess.endpoint',\n",
    "#DiscoveryAccess:Endpoint:Endpoint:Endpoint.endpoint as 'Endpoint.endpoint',\n",
    "#DiscoveryAccess:DiscoveryAccessResult:DiscoveryResult:DiscoveredIPAddressList.#List:List:Member:DiscoveredIPAddress.ip_addr as 'DiscoveredIPAddress.ip_addr',\n",
    "#::InferredElement:.__all_ip_addrs as 'InferredElement.__all_ip_addrs',\n",
    "#::InferredElement:.#DeviceWithInterface:DeviceInterface:InterfaceOfDevice:NetworkInterface.ip_addr as 'NetworkInterface.ip_addr',\n",
    "#::InferredElement:.#DeviceWithInterface:DeviceInterface:InterfaceOfDevice:NetworkInterface.fqdns as 'NetworkInterface.fqdns'\n",
    "'''\n",
    "\n",
    "qry_access = '''\n",
    "search DiscoveryAccess where endtime\n",
    "ORDER BY discovery_endtime DESC\n",
    "show\n",
    "#id as \"DiscoveryAccess.id\",\n",
    "#Next:Sequential:Previous:DiscoveryAccess.#id as \"DiscoveryAccess.previous_id\",\n",
    "#Previous:Sequential:Next:DiscoveryAccess.#id as \"DiscoveryAccess.next_id\",\n",
    "endpoint as 'DiscoveryAccess.endpoint',\n",
    "friendlyTime(discovery_starttime) as 'DiscoveryAccess.scan_starttime',\n",
    "friendlyTime(discovery_endtime) as 'DiscoveryAccess.scan_endtime',\n",
    "discovery_endtime as 'DiscoveryAccess.scan_endtime_raw',\n",
    "discovery_endtime as 'DiscoveryAccess.discovery_endtime',\n",
    "whenWasThat(discovery_endtime) as 'DiscoveryAccess.when_last_scan',\n",
    "end_state as 'DiscoveryAccess.end_state',\n",
    "reason as 'DiscoveryAccess.reason_not_updated',\n",
    "result as 'DiscoveryAccess.result',\n",
    "_last_marker as 'DiscoveryAccess._last_marker',\n",
    "_first_marker as 'DiscoveryAccess._first_marker',\n",
    "_last_interesting as 'DiscoveryAccess._last_interesting',\n",
    "__had_inference as 'DiscoveryAccess.__had_inference',\n",
    "best_ip_score as 'DiscoveryAccess.best_ip_score',\n",
    "(#DiscoveryAccess:Metadata:Detail:SessionResult.success or access_success) as 'DiscoveryAccess.access_success',\n",
    "access_failure as 'DiscoveryAccess.access_failure',\n",
    "message as 'DiscoveryAccess.message'\n",
    "'''\n",
    "\n",
    "qry_deviceinfo = '''\n",
    "search DeviceInfo\n",
    "show\n",
    "#id as \"DeviceInfo.id\",\n",
    "hostname as 'DeviceInfo.hostname',\n",
    "os_type as 'DeviceInfo.os_type',\n",
    "os_class as 'DeviceInfo.os_class',\n",
    "os_version as 'DeviceInfo.os_version',\n",
    "kind as 'DeviceInfo.kind',\n",
    "inferred_kind as 'DeviceInfo.inferred_kind',\n",
    "last_access_method as 'DeviceInfo.last_access_method',\n",
    "probed_os as 'DeviceInfo.probed_os',\n",
    "last_credential as 'DeviceInfo.last_credential',\n",
    "last_slave as 'DeviceInfo.last_slave',\n",
    "__preserved_last_credential as 'DeviceInfo.__preserved_last_credential'\n",
    "'''\n",
    "\n",
    "qry_run = '''\n",
    "search DiscoveryRun\n",
    "show\n",
    "#id as \"DiscoveryRun.id\",\n",
    "label as 'DiscoveryRun.label',\n",
    "friendlyTime(starttime) as 'DiscoveryRun.starttime',\n",
    "friendlyTime(endtime) as 'DiscoveryRun.endtime'\n",
    "'''\n",
    "\n",
    "qry_session = '''\n",
    "search SessionResult\n",
    "show\n",
    "#id as \"SessionResult.id\",\n",
    "success as \"SessionResult.success\",\n",
    "session_type as \"SessionResult.session_type\",\n",
    "provider as \"SessionResult.provider\"\n",
    "'''\n",
    "\n",
    "qry_inferred = '''\n",
    "search InferredElement\n",
    "show\n",
    "#id as \"InferredElement.id\",\n",
    "__all_ip_addrs as 'InferredElement.__all_ip_addrs'\n",
    "'''\n",
    "\n",
    "qry_interface = '''\n",
    "search NetworkInterface\n",
    "show\n",
    "#id as \"NetworkInterface.id\",\n",
    "ip_addr as 'NetworkInterface.ip_addr'\n",
    "'''\n",
    "\n",
    "qry_dropped = '''\n",
    "search DroppedEndpoints\n",
    "show explode endpoints as 'Endpoint',\n",
    "reason as 'Reason_Not_Updated',\n",
    "__reason as 'End_State',\n",
    "friendlyTime(starttime) as 'Start',\n",
    "friendlyTime(endtime) as 'End',\n",
    "endtime as 'End_Raw',\n",
    "whenWasThat(endtime) as 'When_Last_Scan',\n",
    "#EndpointRange:EndpointRange:DiscoveryRun:DiscoveryRun.label as \"Run\"\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fetch-key-facts",
   "metadata": {},
   "source": [
    "## Fetch Discovery Extracts\n",
    "\n",
    "Retrieves the minimal key mapping and the fact tables used for the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ddb394-1c7d-4e37-8f38-7cd20cac5f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_of(q, label, n=5):\n",
    "    rows = tw_search_all(twsearch, q) or []\n",
    "    df = pd.DataFrame(rows)\n",
    "\n",
    "    if df.empty:\n",
    "        print(f'- {label}: 0 rows')\n",
    "        return df\n",
    "\n",
    "    # make a hashable view for dedupe\n",
    "    def _hashable(x):\n",
    "        if isinstance(x, (list, tuple)):\n",
    "            return tuple(_hashable(v) for v in x)\n",
    "        if isinstance(x, dict):\n",
    "            return tuple(sorted((k, _hashable(v)) for k, v in x.items()))\n",
    "        return x\n",
    "\n",
    "    df_h = df.map(_hashable)\n",
    "    idx = df_h.drop_duplicates().index\n",
    "    dfu = df.loc[idx].reset_index(drop=True)\n",
    "\n",
    "    print(f'- {label}: {len(rows)} rows (raw), {len(dfu)} rows (unique)')\n",
    "    display(dfu.head(n))\n",
    "    return dfu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667855a6-f748-4873-8f1a-7c40eda9850c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#search DiscoveryAccess where endtime and #id = \"0684b5680100264824e0a6256e446973636f76657279416363657373\"\n",
    "qry_key = '''\n",
    "search DiscoveryAccess where endtime\n",
    "show\n",
    "#id as \"DiscoveryAccess.id\",\n",
    "#Next:Sequential:Previous:DiscoveryAccess.#id as \"DiscoveryAccess.previous_id\",\n",
    "#Previous:Sequential:Next:DiscoveryAccess.#id as \"DiscoveryAccess.next_id\",\n",
    "#DiscoveryAccess:DiscoveryAccessResult:DiscoveryResult:DeviceInfo.#id as \"DeviceInfo.id\",\n",
    "#Member:List:List:DiscoveryRun.#id as \"DiscoveryRun.id\",\n",
    "#::InferredElement:.#id as \"InferredElement.id\",\n",
    "#DiscoveryAccess:Metadata:Detail:SessionResult.#id as \"SessionResult.id\",\n",
    "explode #::InferredElement:.#DeviceWithInterface:DeviceInterface:InterfaceOfDevice:NetworkInterface.#id as \"NetworkInterface.id\"\n",
    "'''\n",
    "\n",
    "print('Running extractsâ€¦')\n",
    "\n",
    "key_df = df_of(qry_key, 'Key Map', n=10)  # show first 10 rows\n",
    "assert not key_df.empty, 'Key map query returned no rows; cannot continue.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fetch-extracts",
   "metadata": {},
   "outputs": [],
   "source": [
    "access_df    = df_of(qry_access,     'DiscoveryAccess', n=10)\n",
    "device_df    = df_of(qry_deviceinfo, 'DeviceInfo',      n=10)\n",
    "run_df       = df_of(qry_run,        'DiscoveryRun',    n=10)\n",
    "session_df   = df_of(qry_session,    'SessionResult',   n=10)\n",
    "inferred_df  = df_of(qry_inferred,   'InferredElement', n=10)\n",
    "interface_df = df_of(qry_interface,  'NetworkInterface',n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "merge-facts",
   "metadata": {},
   "source": [
    "## Merge Extracts\n",
    "\n",
    "Merges the key map with fact tables to reassemble the wide view, adds helper fields\n",
    "(session_results_logged, previous_end_state, access_method/current_access)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "merge-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = key_df.merge(access_df, how='left', on='DiscoveryAccess.id')\n",
    "merged = merged.merge(device_df, how='left', on='DeviceInfo.id')\n",
    "merged = merged.merge(run_df, how='left', on='DiscoveryRun.id')\n",
    "if 'SessionResult.id' in merged.columns and not session_df.empty:\n",
    "    merged = merged.merge(session_df, how='left', on='SessionResult.id')\n",
    "if 'InferredElement.id' in merged.columns and not inferred_df.empty:\n",
    "    merged = merged.merge(inferred_df, how='left', on='InferredElement.id')\n",
    "if 'NetworkInterface.id' in merged.columns and not interface_df.empty:\n",
    "    merged = merged.merge(interface_df, how='left', on='NetworkInterface.id')\n",
    "\n",
    "if 'SessionResult.provider' in merged.columns:\n",
    "    merged['DiscoveryAccess.session_results_logged'] = (\n",
    "        merged.groupby('DiscoveryAccess.id')['SessionResult.provider'].transform(lambda s: s.isna().any())\n",
    "    )\n",
    "else:\n",
    "    merged['DiscoveryAccess.session_results_logged'] = False\n",
    "\n",
    "if 'DiscoveryAccess.previous_id' in merged.columns and 'DiscoveryAccess.end_state' in access_df.columns:\n",
    "    prev_map = access_df.set_index('DiscoveryAccess.id')['DiscoveryAccess.end_state']\n",
    "    merged['DiscoveryAccess.previous_end_state'] = merged['DiscoveryAccess.previous_id'].map(prev_map)\n",
    "else:\n",
    "    merged['DiscoveryAccess.previous_end_state'] = None\n",
    "\n",
    "merged['DiscoveryAccess.access_method'] = merged.get('DeviceInfo.last_access_method', None)\n",
    "def _curr(row):\n",
    "    method = row.get('DeviceInfo.last_access_method')\n",
    "    slave = row.get('DeviceInfo.last_slave')\n",
    "    probed = row.get('DeviceInfo.probed_os')\n",
    "    if method in ['windows','rcmd'] and slave:\n",
    "        return method\n",
    "    if probed:\n",
    "        return 'Probe'\n",
    "    return method\n",
    "merged['DiscoveryAccess.current_access'] = merged.apply(_curr, axis=1)\n",
    "\n",
    "print('Merged rows:', len(merged))\n",
    "merged.head(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "identities-section",
   "metadata": {},
   "source": [
    "## Build Identities\n",
    "\n",
    "Retrieves identity hints per endpoint (IPs and names) and builds a simple identity list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "identities-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_rows = tw_search_all(twsearch, qry_device_ids)\n",
    "print('Identity rows:', len(id_rows))\n",
    "identities = build_identities(id_rows)\n",
    "print('Unique identities:', len(identities))\n",
    "identities[:3]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dropped-section",
   "metadata": {},
   "source": [
    "## Dropped Endpoints\n",
    "\n",
    "Fetches dropped endpoints to include in the endpoint-level view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dropped-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "dropped = tw_search_all(twsearch, qry_dropped)\n",
    "print('Dropped rows:', len(dropped))\n",
    "dropped[:3]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assemble-section",
   "metadata": {},
   "source": [
    "## Assemble Endpoint Records\n",
    "\n",
    "Groups merged facts and dropped entries by endpoint, enriches with credential labels,\n",
    "computes consistency and \"when\" buckets, then selects the best/latest record per endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assemble-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "cred_map = get_credential_map(twcreds)\n",
    "id_map = {ip: ident for ident in identities for ip in (ident.get('list_of_ips') or []) if ip}\n",
    "\n",
    "by_endpoint = {}\n",
    "for _, r in merged.iterrows():\n",
    "    ep = r.get('DiscoveryAccess.endpoint')\n",
    "    if not ep:\n",
    "        continue\n",
    "    by_endpoint.setdefault(ep, {'discos': [], 'dropped': []})['discos'].append(r.to_dict())\n",
    "for r in dropped or []:\n",
    "    ep = r.get('Endpoint')\n",
    "    if not ep:\n",
    "        continue\n",
    "    by_endpoint.setdefault(ep, {'discos': [], 'dropped': []})['dropped'].append(r)\n",
    "\n",
    "def _parse_ts(scan_end_raw, friendly_end):\n",
    "    ts = None\n",
    "    if scan_end_raw:\n",
    "        try:\n",
    "            ts = datetime.datetime.fromisoformat(str(scan_end_raw).replace('Z','+00:00'))\n",
    "        except Exception:\n",
    "            ts = None\n",
    "    if ts is None and friendly_end:\n",
    "        try:\n",
    "            ts = datetime.datetime.strptime(' '.join(str(friendly_end).split(' ')[:2]), '%Y-%m-%d %H:%M:%S')\n",
    "        except Exception:\n",
    "            ts = None\n",
    "    return ts\n",
    "\n",
    "def _consistency(states: List[str]):\n",
    "    states = [s for s in states if s]\n",
    "    if not states:\n",
    "        return None\n",
    "    total = len(states)\n",
    "    counts = {}\n",
    "    for s in states:\n",
    "        counts[s] = counts.get(s, 0) + 1\n",
    "    top = max(counts, key=counts.get)\n",
    "    if counts[top] == total:\n",
    "        return f'Always {top}'\n",
    "    if counts[top] >= total - 2:\n",
    "        return f'Usually {top}'\n",
    "    return f'Most Often {top}'\n",
    "\n",
    "endpoint_rows = []\n",
    "for ep, recs in by_endpoint.items():\n",
    "    ident = id_map.get(ep, {})\n",
    "    names = ident.get('list_of_names') or []\n",
    "    eps = ident.get('list_of_ips') or []\n",
    "\n",
    "    states = [d.get('DiscoveryAccess.end_state') for d in recs['discos']] + [d.get('End_State') for d in recs['dropped']]\n",
    "    consistency = _consistency(states)\n",
    "\n",
    "    endpoint_records = []\n",
    "\n",
    "    for d in recs['discos']:\n",
    "        scan_end_raw = d.get('DiscoveryAccess.scan_endtime_raw')\n",
    "        friendly_end = d.get('DiscoveryAccess.scan_endtime')\n",
    "        ts = _parse_ts(scan_end_raw, friendly_end)\n",
    "        when = calc_when(ts) if ts else None\n",
    "        hostname = d.get('DeviceInfo.hostname') or (names[0] if names else None)\n",
    "        node_kind = d.get('DeviceInfo.kind') or d.get('DeviceInfo.inferred_kind')\n",
    "        last_cred = d.get('DeviceInfo.last_credential')\n",
    "        last_cred_uuid = str(last_cred).split('/')[-1] if last_cred else None\n",
    "        cred_info = cred_map.get(last_cred_uuid or '', {})\n",
    "        endpoint_records.append({\n",
    "            'endpoint': ep,\n",
    "            'hostname': hostname,\n",
    "            'list_of_names': names,\n",
    "            'list_of_endpoints': eps,\n",
    "            'node_kind': node_kind,\n",
    "            'os_type': d.get('DeviceInfo.os_type'),\n",
    "            'os_version': d.get('DeviceInfo.os_version'),\n",
    "            'os_class': d.get('DeviceInfo.os_class'),\n",
    "            'disco_run': d.get('DiscoveryRun.label'),\n",
    "            'run_start': d.get('DiscoveryRun.starttime'),\n",
    "            'run_end': d.get('DiscoveryRun.endtime'),\n",
    "            'scan_start': d.get('DiscoveryAccess.scan_starttime'),\n",
    "            'scan_end': d.get('DiscoveryAccess.scan_endtime'),\n",
    "            'scan_end_raw': scan_end_raw,\n",
    "            'when_was_that': when,\n",
    "            'consistency': consistency,\n",
    "            'current_access': d.get('DiscoveryAccess.current_access'),\n",
    "            'access_method': d.get('DiscoveryAccess.access_method') or d.get('DeviceInfo.last_access_method'),\n",
    "            'inferred_node_updated': d.get('DiscoveryAccess.host_node_updated'),\n",
    "            'reason_not_updated': d.get('DiscoveryAccess.reason_not_updated'),\n",
    "            'end_state': d.get('DiscoveryAccess.end_state'),\n",
    "            'previous_end_state': d.get('DiscoveryAccess.previous_end_state'),\n",
    "            'session_results_logged': d.get('DiscoveryAccess.session_results_logged'),\n",
    "            'last_credential': last_cred_uuid,\n",
    "            'credential_name': cred_info.get('label'),\n",
    "            'credential_login': cred_info.get('username'),\n",
    "            'timestamp': ts,\n",
    "            'da_id': d.get('DiscoveryAccess.id'),\n",
    "            'prev_da_id': d.get('DiscoveryAccess.previous_id'),\n",
    "            'next_node_id': d.get('DiscoveryAccess.next_id'),\n",
    "        })\n",
    "\n",
    "    for d in recs['dropped']:\n",
    "        ts = _parse_ts(d.get('End_Raw'), d.get('End'))\n",
    "        when = calc_when(ts) if ts else None\n",
    "        endpoint_records.append({\n",
    "            'endpoint': ep,\n",
    "            'hostname': names[0] if names else None,\n",
    "            'list_of_names': names,\n",
    "            'list_of_endpoints': eps,\n",
    "            'disco_run': d.get('Run'),\n",
    "            'run_start': d.get('Start'),\n",
    "            'run_end': d.get('End'),\n",
    "            'when_was_that': when,\n",
    "            'consistency': consistency,\n",
    "            'reason_not_updated': d.get('Reason_Not_Updated'),\n",
    "            'end_state': d.get('End_State'),\n",
    "            'timestamp': ts,\n",
    "            'scan_end_raw': d.get('End_Raw'),\n",
    "        })\n",
    "\n",
    "    if not endpoint_records:\n",
    "        continue\n",
    "    # choose latest, but keep identifying fields when available\n",
    "    latest = max(endpoint_records, key=lambda r: r.get('timestamp') or datetime.datetime.min)\n",
    "    named = [r for r in endpoint_records if r.get('hostname') or r.get('credential_name')]\n",
    "    chosen = max(named, key=lambda r:   r.get('timestamp') or datetime.datetime.min) if named else latest\n",
    "    # backfill from latest\n",
    "    for k, v in latest.items():\n",
    "        if chosen.get(k) in (None, '') and v not in (None, ''):\n",
    "            chosen[k] = v\n",
    "    endpoint_rows.append(chosen)\n",
    "\n",
    "len(endpoint_rows)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "format-output",
   "metadata": {},
   "source": [
    "## Format Output\n",
    "\n",
    "Builds the final DataFrame with the same columns as the DisMAL `discovery_analysis` CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "format-output-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "for r in endpoint_rows:\n",
    "    rows.append([\n",
    "        r.get('endpoint'),\n",
    "        r.get('hostname'),\n",
    "        r.get('list_of_names'),\n",
    "        r.get('list_of_endpoints'),\n",
    "        r.get('node_kind'),\n",
    "        r.get('os_type'),\n",
    "        r.get('os_version'),\n",
    "        r.get('os_class'),\n",
    "        r.get('disco_run'),\n",
    "        r.get('run_start'),\n",
    "        r.get('run_end'),\n",
    "        r.get('scan_start'),\n",
    "        r.get('scan_end'),\n",
    "        r.get('scan_end_raw'),\n",
    "        r.get('when_was_that'),\n",
    "        r.get('consistency'),\n",
    "        r.get('current_access'),\n",
    "        r.get('access_method'),\n",
    "        r.get('inferred_node_updated'),\n",
    "        r.get('reason_not_updated'),\n",
    "        r.get('end_state'),\n",
    "        r.get('previous_end_state'),\n",
    "        (f\"{r.get('previous_end_state')} -> {r.get('end_state')}\" if r.get('end_state') is not None else None),\n",
    "        r.get('session_results_logged'),\n",
    "        r.get('last_credential'),\n",
    "        r.get('credential_name'),\n",
    "        r.get('credential_login'),\n",
    "        r.get('timestamp'),\n",
    "        r.get('da_id'),\n",
    "        r.get('prev_da_id'),\n",
    "        r.get('next_node_id'),\n",
    "    ])\n",
    "headers = [\n",
    "    'endpoint',\n",
    "    'device_name',\n",
    "    'list_of_device_names',\n",
    "    'list_of_endpoints',\n",
    "    'node_kind',\n",
    "    'os_type',\n",
    "    'os_version',\n",
    "    'os_class',\n",
    "    'discovery_run',\n",
    "    'discovery_run_start',\n",
    "    'discovery_run_end',\n",
    "    'scan_start',\n",
    "    'scan_end',\n",
    "    'scan_end_raw',\n",
    "    'when_was_that',\n",
    "    'consistency',\n",
    "    'current_access',\n",
    "    'access_method',\n",
    "    'inferred_node_updated',\n",
    "    'reason_not_updated',\n",
    "    'end_state',\n",
    "    'previous_end_state',\n",
    "    'end_state_change',\n",
    "    'session_results_logged',\n",
    "    'last_credential',\n",
    "    'credential_name',\n",
    "    'credential_login',\n",
    "    'timestamp',\n",
    "    'da_id',\n",
    "    'prev_da_id',\n",
    "    'next_node_id',\n",
    "]\n",
    "df_out = pd.DataFrame(rows, columns=headers)\n",
    "df_out.insert(0, 'Discovery Instance', target)\n",
    "df_out.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "save-csv",
   "metadata": {},
   "source": [
    "## Save to CSV\n",
    "\n",
    "Writes to the standard output folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save-csv-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_CSV = str(output_dir / 'discovery_analysis.csv')\n",
    "df_out.to_csv(OUTPUT_CSV, index=False)\n",
    "print(f'Saved to {OUTPUT_CSV}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
