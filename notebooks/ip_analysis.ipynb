{"cells":[{"cell_type":"markdown","id":"intro-title","metadata":{},"source":["# IP Analysis (BMC Discovery)\n","\n","This notebook reproduces the DisMAL `ip_analysis` report using the raw CSV exports generated by DisMAL.\n","It loads the relevant query outputs from `raw_exports/<appliance>/` for preview and optional re-export to the standard output folders.\n"]},{"cell_type":"code","execution_count":null,"id":"081459df-2e17-4dd4-9aea-914ce722b13e","metadata":{},"outputs":[],"source":["# TODO: Fix report headers"]},{"cell_type":"markdown","id":"requirements","metadata":{},"source":["## Requirements\n","\n","We rely on `pandas` for tabular wrangling and `PyYAML` for configuration.\n","Uncomment the next cell to install them in your environment if needed.\n"]},{"cell_type":"code","execution_count":null,"id":"imports","metadata":{},"outputs":[],"source":["# %pip install -q pandas pyyaml\n","\n","import ast\n","import re\n","from pathlib import Path\n","\n","import pandas as pd\n","import yaml\n"]},{"cell_type":"markdown","id":"f680ade0-9e07-42b8-8e17-fd659293fbe4","metadata":{},"source":["## Configuration (from config.yaml)\n","\n","Loads target details, resolves the output folder, and points to the matching raw export directory for each appliance.\n"]},{"cell_type":"code","execution_count":null,"id":"756da44a-3dd9-4742-82a8-dbf5c7e83038","metadata":{},"outputs":[],"source":["from pathlib import Path\n","import re\n","import yaml\n","\n","def load_config_params(\n","    start: Path,\n","    appliance_name: str = None,\n","    appliance_index: int = 0,\n",") -> dict:\n","    def _find_repo_root(start: Path) -> Path:\n","        for p in [start] + list(start.parents):\n","            if (p / 'config.yaml').exists():\n","                return p\n","        return start.parent\n","\n","    def _slugify(value: str) -> str:\n","        return re.sub(r'[^A-Za-z0-9]+', '_', value).strip('_').lower() or 'default'\n","\n","    repo_root = _find_repo_root(start)\n","    config_path = repo_root / 'config.yaml'\n","\n","    with open(config_path, 'r') as fh:\n","        cfg = yaml.safe_load(fh) or {}\n","\n","    apps = cfg.get('appliances') or []\n","    selected = None\n","    if isinstance(apps, list) and apps:\n","        if appliance_name:\n","            selected = next((a for a in apps if a.get('name') == appliance_name), None)\n","            if selected is None:\n","                raise ValueError(f\"No appliance named '{appliance_name}' in config.yaml\")\n","        else:\n","            try:\n","                selected = apps[int(appliance_index)]\n","            except Exception:\n","                selected = apps[0]\n","\n","    target = ((selected or {}).get('target') or cfg.get('target') or '').strip()\n","    if not target:\n","        raise ValueError('config.yaml missing \"target\"')\n","\n","    token = (((selected or {}).get('token') or cfg.get('token') or '').strip())\n","    token_file = (selected or {}).get('token_file') or cfg.get('token_file') or cfg.get('f_token')\n","    if not token and token_file:\n","        tf_path = Path(token_file)\n","        if not tf_path.is_absolute():\n","            tf_path = repo_root / tf_path\n","        with open(tf_path, 'r') as tf:\n","            token = tf.read().strip()\n","    if not token:\n","        token = None\n","\n","    api_version = str((selected or {}).get('api_version') or cfg.get('api_version') or 'v1.14')\n","    verify_ssl = bool((selected or {}).get('verify_ssl', cfg.get('verify_ssl', True)))\n","\n","    sanitized = target.replace('.', '_').replace(':', '_').replace('/', '_')\n","    output_dir = repo_root / f'output_{sanitized}'\n","    output_dir.mkdir(parents=True, exist_ok=True)\n","\n","    export_name = ((selected or {}).get('name') or appliance_name or sanitized)\n","    raw_export_dir = repo_root / 'raw_exports' / _slugify(export_name)\n","\n","    return {\n","        \"repo_root\": repo_root,\n","        \"config_path\": config_path,\n","        \"cfg\": cfg,\n","        \"selected\": selected,\n","        \"target\": target,\n","        \"token\": token,\n","        \"api_version\": api_version,\n","        \"verify_ssl\": verify_ssl,\n","        \"output_dir\": output_dir,\n","        \"raw_export_dir\": raw_export_dir,\n","    }\n"]},{"cell_type":"code","execution_count":null,"id":"8cbb854e-5fbb-41f5-a4d2-ef87e3f19ada","metadata":{},"outputs":[],"source":["def init_appliance(appliance_name: str = \"prod\"):\n","    params = load_config_params(Path.cwd(), appliance_name=appliance_name)\n","\n","    target = params[\"target\"]\n","    api_version = params[\"api_version\"]\n","    verify_ssl = params[\"verify_ssl\"]\n","    output_dir = params[\"output_dir\"]\n","    raw_export_dir = params[\"raw_export_dir\"]\n","\n","    print('Appliance Name :', appliance_name)\n","    print('Target         :', target)\n","    print('API Version    :', api_version)\n","    print('Verify SSL     :', verify_ssl)\n","    print('Raw CSV folder :', raw_export_dir)\n","    print('Output folder  :', output_dir)\n","\n","    return {\n","        \"params\": params,\n","        \"target\": target,\n","        \"api_version\": api_version,\n","        \"verify_ssl\": verify_ssl,\n","        \"output_dir\": output_dir,\n","        \"raw_export_dir\": raw_export_dir,\n","        \"appliance_name\": appliance_name,\n","    }\n"]},{"cell_type":"markdown","id":"19713aa2-18e7-4e37-82ce-ac1d46baa6f7","metadata":{},"source":["# Initialise Instances"]},{"cell_type":"code","execution_count":null,"id":"541f0e97-d0f0-4508-aa89-26c98a965c27","metadata":{},"outputs":[],"source":["print(\"Initialise Prod:\")\n","twprod = init_appliance(\"prod\")\n","\n","print(\"Initialise Dev:\")\n","twdev = init_appliance(\"dev\")\n"]},{"cell_type":"code","execution_count":null,"id":"780271ca-8686-413d-9ec6-010ff1eda717","metadata":{},"outputs":[],"source":["BASE_EXPORT_COLUMNS = ['Appliance Target', 'Appliance Name', 'Query Title']\n","_SLUG_PATTERN = re.compile(r'[^A-Za-z0-9]+')\n","DEFAULT_SCHEDULE_TITLE = 'Scheduled Scan Ranges'\n","DEFAULT_EXCLUDE_TITLE = 'Exclude Ranges'\n","DEFAULT_SCHEDULE_COUNT_TITLE = 'DiscoveryAccess Schedule Counts'\n","DEFAULT_UNSCANNED_TITLE = 'Unscanned Connections'\n","\n","def slugify_title(value: str) -> str:\n","    slug = _SLUG_PATTERN.sub('_', value or '').strip('_').lower()\n","    return slug or 'unnamed'\n","\n","def load_export(instance, query_title: str, expected_columns=None, drop_duplicates=False):\n","    expected_columns = expected_columns or []\n","    csv_path = instance['raw_export_dir'] / f\"{slugify_title(query_title)}.csv\"\n","    if not csv_path.exists():\n","        raise FileNotFoundError(f'Missing export for {query_title}: {csv_path}')\n","\n","    df = pd.read_csv(csv_path)\n","    df = df.drop(columns=[c for c in BASE_EXPORT_COLUMNS if c in df.columns], errors='ignore')\n","\n","    for col in expected_columns:\n","        if col not in df.columns:\n","            df[col] = pd.NA\n","\n","    if drop_duplicates:\n","        df = df.drop_duplicates().reset_index(drop=True)\n","\n","    return df\n","\n","def parse_range_value(value):\n","    if pd.isna(value):\n","        return []\n","    if isinstance(value, list):\n","        return [str(v).strip() for v in value if str(v).strip()]\n","    if isinstance(value, str):\n","        stripped = value.strip()\n","        if stripped.startswith('[') and stripped.endswith(']'):\n","            try:\n","                parsed = ast.literal_eval(stripped)\n","            except Exception:\n","                parsed = [stripped]\n","            else:\n","                if not isinstance(parsed, (list, tuple)):\n","                    parsed = [parsed]\n","            return [str(v).strip() for v in parsed if str(v).strip()]\n","        if stripped:\n","            return [stripped]\n","        return []\n","    return [str(value).strip()]\n","\n","def normalise_scan_ranges(df, column='Scan_Range'):\n","    df = df.copy()\n","    df[column] = df[column].apply(parse_range_value)\n","    df = df.explode(column).dropna(subset=[column]).reset_index(drop=True)\n","    df[column] = df[column].astype(str)\n","    return df\n","\n","def attach_schedules(schedule_df: pd.DataFrame, range_df: pd.DataFrame) -> pd.DataFrame:\n","    if schedule_df.empty:\n","        return pd.DataFrame(columns=['endpoint', 'scan_schedules'])\n","\n","    expanded_ranges = normalise_scan_ranges(range_df)\n","    merged = schedule_df.merge(\n","        expanded_ranges[['Label', 'Scan_Range']],\n","        left_on='endpoint',\n","        right_on='Scan_Range',\n","        how='left'\n","    )\n","\n","    grouped = (\n","        merged.groupby('endpoint')['Label']\n","        .apply(lambda values: ', '.join(sorted({v for v in values.dropna() if v})))\n","        .reset_index()\n","    )\n","\n","    grouped['scan_schedules'] = grouped['Label'].replace('', pd.NA)\n","    grouped = grouped.drop(columns=['Label'])\n","    return grouped\n","\n","def add_unscanned(schedule_df: pd.DataFrame, unscanned_df: pd.DataFrame) -> pd.DataFrame:\n","    unscanned = unscanned_df[['endpoint']].copy()\n","    unscanned['scan_schedules'] = 'Seen but unscanned'\n","    combined = pd.concat([schedule_df, unscanned], ignore_index=True)\n","    combined['scan_schedules'] = combined['scan_schedules'].fillna(\n","        'Endpoint has previous DiscoveryAccess, but not currently scheduled.'\n","    )\n","    combined = combined.drop_duplicates(subset=['endpoint', 'scan_schedules']).reset_index(drop=True)\n","    return combined\n","\n","def build_ip_analysis(instance):\n","    range_columns = ['ID', 'Label', 'Scan_Range', 'Level', 'Date_Rules']\n","    scheduled = load_export(instance, DEFAULT_SCHEDULE_TITLE, range_columns)\n","    excludes = load_export(instance, DEFAULT_EXCLUDE_TITLE, ['ID', 'Label', 'Scan_Range', 'Date_Rules'])\n","    if 'Level' not in excludes.columns:\n","        excludes['Level'] = pd.NA\n","    combined_ranges = pd.concat([scheduled, excludes[range_columns]], ignore_index=True, sort=False)\n","\n","    schedules = load_export(instance, DEFAULT_SCHEDULE_COUNT_TITLE, ['endpoint', 'schedules'])\n","    if 'schedules' in schedules.columns:\n","        schedules['schedules'] = pd.to_numeric(schedules['schedules'], errors='coerce')\n","\n","    schedule_map = attach_schedules(schedules, combined_ranges)\n","\n","    unscanned = load_export(instance, DEFAULT_UNSCANNED_TITLE, ['endpoint'])\n","    final = add_unscanned(schedule_map, unscanned)\n","    final.insert(0, 'Discovery Instance', instance['target'])\n","    return final.sort_values(['endpoint', 'scan_schedules']).reset_index(drop=True)\n"]},{"cell_type":"markdown","id":"logic","metadata":{},"source":["## Load and Preview\n","\n","Build the endpoint schedule mapping from the raw exports and preview the results.\n"]},{"cell_type":"code","execution_count":null,"id":"4e9448df-aaf8-4ac3-a5a8-d8abc3821ca4","metadata":{},"outputs":[],"source":["from pathlib import Path\n","import pandas as pd\n","\n","def save(df: pd.DataFrame, output_dir: Path, filename: str):\n","    output_csv = str(output_dir / f\"{filename}.csv\")\n","    df.to_csv(output_csv, index=False)\n","    print(f\"Saved to {output_csv}\")\n"]},{"cell_type":"code","execution_count":null,"id":"57f45d5f-2ba7-41af-88af-8f693e9f54c3","metadata":{},"outputs":[],"source":["prod_df = build_ip_analysis(twprod)\n","print(twprod['target'])\n","display(prod_df.head(5))\n","\n","dev_df = build_ip_analysis(twdev)\n","print(twdev['target'])\n","display(dev_df.head(5))\n"]},{"cell_type":"code","execution_count":null,"id":"e2f7ab0d-adba-4639-a3ee-c63f4c9447cd","metadata":{},"outputs":[],"source":["save(prod_df, twprod['output_dir'], 'ip_analysis')\n","save(dev_df, twdev['output_dir'], 'ip_analysis')\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.3"}},"nbformat":4,"nbformat_minor":5}