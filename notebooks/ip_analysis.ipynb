{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro-title",
   "metadata": {},
   "source": [
    "# IP Analysis (BMC Discovery)\n",
    "\n",
    "This notebook reproduces the DisMAL `ip_analysis` report logic using the Discovery Data API.\n",
    "It reads configuration from `../config.yaml`, evaluates the relevant TWQL queries,\n",
    "derives overlapping schedules and gaps, then writes `ip_analysis.csv` to the standard output directory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "requirements",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "\n",
    "We use `requests`, `pandas`, and `PyYAML`. Uncomment below to install if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -q requests pandas pyyaml\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "from urllib.parse import urljoin\n",
    "import ipaddress\n",
    "from typing import List, Dict, Any, Tuple\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "select-appliance",
   "metadata": {},
   "source": [
    "## Select Appliance (optional)\n",
    "If `config.yaml` defines multiple `appliances:`, set `APPLIANCE_NAME` or `APPLIANCE_INDEX`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "appliance-vars",
   "metadata": {},
   "outputs": [],
   "source": [
    "APPLIANCE_NAME = None   # e.g., 'prod' or 'dev'\n",
    "APPLIANCE_INDEX = 0     # integer index if not using name selection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "config-load",
   "metadata": {},
   "source": [
    "## Configuration (from config.yaml)\n",
    "Locates `../config.yaml`, reads connection details, and prepares the output folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _find_repo_root(start: Path) -> Path:\n",
    "    for p in [start] + list(start.parents):\n",
    "        if (p / 'config.yaml').exists():\n",
    "            return p\n",
    "    return start.parent\n",
    "\n",
    "repo_root = _find_repo_root(Path.cwd())\n",
    "config_path = repo_root / 'config.yaml'\n",
    "with open(config_path, 'r') as fh:\n",
    "    cfg = yaml.safe_load(fh) or {}\n",
    "\n",
    "apps = cfg.get('appliances') or []\n",
    "selected = None\n",
    "if isinstance(apps, list) and apps:\n",
    "    if APPLIANCE_NAME:\n",
    "        selected = next((a for a in apps if a.get('name') == APPLIANCE_NAME), None)\n",
    "        if selected is None:\n",
    "            raise ValueError(f\"No appliance named '{APPLIANCE_NAME}' in config.yaml\")\n",
    "    else:\n",
    "        try:\n",
    "            selected = apps[int(APPLIANCE_INDEX)]\n",
    "        except Exception:\n",
    "            selected = apps[0]\n",
    "\n",
    "target = ((selected or {}).get('target') or cfg.get('target') or '').strip()\n",
    "if not target:\n",
    "    raise ValueError('config.yaml missing \"target\"')\n",
    "BASE_URL = target if ('://' in target) else f'https://{target}'\n",
    "\n",
    "token = (((selected or {}).get('token') or cfg.get('token') or '').strip())\n",
    "token_file = (selected or {}).get('token_file') or cfg.get('token_file') or cfg.get('f_token')\n",
    "if not token and token_file:\n",
    "    tf_path = Path(token_file)\n",
    "    if not tf_path.is_absolute():\n",
    "        tf_path = repo_root / tf_path\n",
    "    with open(tf_path, 'r') as tf:\n",
    "        token = tf.read().strip()\n",
    "if not token:\n",
    "    raise ValueError('API token not found in config.yaml (token or token_file)')\n",
    "\n",
    "API_VERSION = str((selected or {}).get('api_version') or cfg.get('api_version') or 'v1.14')\n",
    "VERIFY_SSL = bool((selected or {}).get('verify_ssl', cfg.get('verify_ssl', True)))\n",
    "\n",
    "sanitized = target.replace('.', '_').replace(':', '_').replace('/', '_')\n",
    "output_dir = repo_root / f'output_{sanitized}'\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print('Base URL      :', BASE_URL)\n",
    "print('API Version   :', API_VERSION)\n",
    "print('Verify SSL    :', VERIFY_SSL)\n",
    "print('Output folder :', output_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "session-helpers",
   "metadata": {},
   "source": [
    "## Session and helpers\n",
    "`post_search` aggregates the 'object' table format across pages and returns {'headings': [...], 'results': [...]}.\n",
    "`table_to_dicts` zips headings into dict rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "session-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "session = requests.Session()\n",
    "auth_value = token if token.lower().startswith('bearer ') else f'Bearer {token}'\n",
    "session.headers.update({'Authorization': auth_value, 'Accept': 'application/json'})\n",
    "session.verify = VERIFY_SSL\n",
    "\n",
    "def api_url(path: str) -> str:\n",
    "    base = BASE_URL.rstrip('/') + f'/api/{API_VERSION}/'\n",
    "    return urljoin(base, path.lstrip('/'))\n",
    "\n",
    "def post_search(query: str, *, limit: int | None = None, page_size: int = 500) -> Dict[str, Any]:\n",
    "    url = api_url('data/search')\n",
    "    headings = None\n",
    "    results: List[List[Any]] = []\n",
    "    offset = 0\n",
    "    fetch_all = (limit == 0)\n",
    "    while True:\n",
    "        payload = {'query': query, 'format': 'object'}\n",
    "        if fetch_all:\n",
    "            payload['limit'] = page_size\n",
    "            if offset:\n",
    "                payload['offset'] = offset\n",
    "        elif limit is not None:\n",
    "            payload['limit'] = limit\n",
    "        r = session.post(url, json=payload)\n",
    "        if r.status_code >= 400:\n",
    "            print(f'Error {r.status_code} POST {url}: {r.text[:200]}')\n",
    "            return {'headings': [], 'results': []}\n",
    "        try:\n",
    "            data = r.json()\n",
    "        except Exception:\n",
    "            data = []\n",
    "        table = None\n",
    "        if isinstance(data, list):\n",
    "            for x in data:\n",
    "                if isinstance(x, dict) and 'headings' in x and 'results' in x:\n",
    "                    table = x\n",
    "                    break\n",
    "        elif isinstance(data, dict) and 'headings' in data and 'results' in data:\n",
    "            table = data\n",
    "        if not table:\n",
    "            return {'headings': [], 'results': []}\n",
    "        if headings is None:\n",
    "            headings = table.get('headings', [])\n",
    "        page_rows = table.get('results') or []\n",
    "        results.extend(page_rows)\n",
    "        if not fetch_all or len(page_rows) < page_size:\n",
    "            break\n",
    "        offset += page_size\n",
    "    return {'headings': (headings or []), 'results': results}\n",
    "\n",
    "def table_to_dicts(table: Dict[str, Any]) -> List[Dict[str, Any]]:\n",
    "    heads = table.get('headings') or []\n",
    "    rows = table.get('results') or []\n",
    "    return [dict(zip(heads, r)) for r in rows]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "queries",
   "metadata": {},
   "source": [
    "## TWQL Queries\n",
    "These mirror `core/queries.py` entries used by the CLI implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "queries-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "qry_scanrange = '''\n",
    "search ScanRange where scan_type = 'Scheduled'\n",
    "show\n",
    "    range_id as 'ID',\n",
    "    label as 'Label',\n",
    "    (range_strings or provider) as 'Scan_Range',\n",
    "    scan_level as 'Level',\n",
    "    recurrenceDescription(schedule) as 'Date_Rules'\n",
    "'''\n",
    "\n",
    "qry_excludes = '''\n",
    "search in '_System' ExcludeRange\n",
    "show\n",
    "    exrange_id as 'ID',\n",
    "    name as 'Label',\n",
    "    range_strings as 'Scan_Range',\n",
    "    recurrenceDescription(schedule) as 'Date_Rules'\n",
    "'''\n",
    "\n",
    "qry_ip_schedules = '''\n",
    "search DiscoveryAccess\n",
    "show endpoint,\n",
    "     nodecount(traverse Member:List:List:DiscoveryRun where scan_type = 'Scheduled') as 'schedules'\n",
    "process with unique()\n",
    "'''\n",
    "\n",
    "qry_connections_unscanned = '''\n",
    "search Host\n",
    "traverse InferredElement:Inference:Associate:DiscoveryAccess\n",
    "traverse DiscoveryAccess:DiscoveryAccessResult:DiscoveryResult:NetworkConnectionList\n",
    "traverse List:List:Member:DiscoveredNetworkConnection\n",
    "order by remote_ip_addr\n",
    "show remote_ip_addr as 'Unscanned Host IP Address'\n",
    "processwith connectionsToUnseen\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "logic",
   "metadata": {},
   "source": [
    "## Build report data\n",
    "- Collect scan ranges and excludes; prepare matchers.\n",
    "- For each endpoint, identify schedule labels; record overlaps and missing schedules.\n",
    "- Add 'seen but unscanned' IPs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "build-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _parse_scan_tokens(tokens: List[str]):\n",
    "    items = []\n",
    "    wildcard = False\n",
    "    for t in tokens:\n",
    "        t = (t or '').strip()\n",
    "        if not t:\n",
    "            continue\n",
    "        if t in ('0.0.0.0/0', '::/0'):\n",
    "            wildcard = True\n",
    "            continue\n",
    "        if '-' in t:\n",
    "            try:\n",
    "                start, end = [ipaddress.ip_address(x.strip()) for x in t.split('-', 1)]\n",
    "                items.append(('range', (int(start), int(end), start.version)))\n",
    "                continue\n",
    "            except Exception:\n",
    "                pass\n",
    "        if '/' in t:\n",
    "            try:\n",
    "                net = ipaddress.ip_network(t, strict=False)\n",
    "                items.append(('network', (net, net.version)))\n",
    "                continue\n",
    "            except Exception:\n",
    "                pass\n",
    "        # Single IP\n",
    "        try:\n",
    "            ip = ipaddress.ip_address(t)\n",
    "            items.append(('single', (int(ip), ip.version)))\n",
    "        except Exception:\n",
    "            # Unknown token; skip\n",
    "            pass\n",
    "    return wildcard, items\n",
    "\n",
    "def _endpoint_in_items(ep: str, wildcard: bool, items) -> bool:\n",
    "    if wildcard:\n",
    "        return True\n",
    "    try:\n",
    "        ip = ipaddress.ip_address(ep)\n",
    "        ival = int(ip)\n",
    "        ver = ip.version\n",
    "    except Exception:\n",
    "        return False\n",
    "    for kind, data in items:\n",
    "        if kind == 'network':\n",
    "            net, nver = data\n",
    "            if ver == nver and ip in net:\n",
    "                return True\n",
    "        elif kind == 'range':\n",
    "            start, end, rver = data\n",
    "            if ver == rver and start <= ival <= end:\n",
    "                return True\n",
    "        elif kind == 'single':\n",
    "            sval, sver = data\n",
    "            if ver == sver and ival == sval:\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "# Fetch tables\n",
    "t_scan = post_search(qry_scanrange, limit=0)\n",
    "t_exc = post_search(qry_excludes, limit=0)\n",
    "rows_scan = table_to_dicts(t_scan)\n",
    "rows_exc = table_to_dicts(t_exc)\n",
    "\n",
    "# Build label -> matchers\n",
    "label_matchers: List[Tuple[str, bool, list]] = []\n",
    "for row in rows_scan + rows_exc:\n",
    "    label = row.get('Label') or ''\n",
    "    ranges = row.get('Scan_Range')\n",
    "    if isinstance(ranges, str):\n",
    "        tokens = [x.strip() for x in ranges.split(',') if x.strip()]\n",
    "    elif isinstance(ranges, list):\n",
    "        # Flatten embedded comma lists\n",
    "        tokens = []\n",
    "        for r in ranges:\n",
    "            if isinstance(r, str):\n",
    "                tokens.extend([x.strip() for x in r.split(',') if x.strip()])\n",
    "    else:\n",
    "        tokens = []\n",
    "    wildcard, items = _parse_scan_tokens(tokens)\n",
    "    if items or wildcard:\n",
    "        label_matchers.append((label, wildcard, items))\n",
    "\n",
    "# Endpoints and their schedule count\n",
    "t_sched = post_search(qry_ip_schedules, limit=0)\n",
    "rows_sched = table_to_dicts(t_sched)\n",
    "\n",
    "data_rows = []  # [ip, schedules-or-message]\n",
    "# Overlaps and missing\n",
    "for r in rows_sched:\n",
    "    ep = r.get('endpoint')\n",
    "    sched = r.get('schedules')\n",
    "    try:\n",
    "        sched_n = int(sched) if sched is not None else 0\n",
    "    except Exception:\n",
    "        sched_n = 0\n",
    "    if not ep:\n",
    "        continue\n",
    "    if sched_n == 0:\n",
    "        data_rows.append([ep, 'Endpoint has previous DiscoveryAccess, but not currently scheduled.'])\n",
    "        continue\n",
    "    # Determine which labels include this endpoint\n",
    "    labels = []\n",
    "    for label, wildcard, items in label_matchers:\n",
    "        if _endpoint_in_items(ep, wildcard, items):\n",
    "            labels.append(label)\n",
    "    if len(labels) > 1:\n",
    "        data_rows.append([ep, sorted(labels)])\n",
    "\n",
    "# Seen but unscanned\n",
    "t_unseen = post_search(qry_connections_unscanned, limit=0)\n",
    "rows_unseen = table_to_dicts(t_unseen)\n",
    "existing_ips = {row[0] for row in data_rows}\n",
    "for r in rows_unseen:\n",
    "    ip = r.get('Unscanned Host IP Address')\n",
    "    if ip and ip not in existing_ips:\n",
    "        data_rows.append([ip, 'Seen but unscanned.'])\n",
    "        existing_ips.add(ip)\n",
    "\n",
    "# Build DataFrame and save\n",
    "df = pd.DataFrame(data_rows, columns=['IP Address', 'Scan Schedules'])\n",
    "df.insert(0, 'Discovery Instance', target)\n",
    "OUTPUT_CSV = str(output_dir / 'ip_analysis.csv')\n",
    "df.to_csv(OUTPUT_CSV, index=False)\n",
    "print(f'Saved to {OUTPUT_CSV} (rows: {len(df)})')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "footer",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
