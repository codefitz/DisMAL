{"cells":[{"cell_type":"markdown","id":"intro-title","metadata":{},"source":["# IP Analysis (BMC Discovery)\n","\n","This notebook reproduces the DisMAL `ip_analysis` report logic using the Discovery Data API.\n","It reads configuration from `../config.yaml`, evaluates the relevant TWQL queries,\n","derives overlapping schedules and gaps, then writes `ip_analysis.csv` to the standard output directory."]},{"cell_type":"markdown","id":"requirements","metadata":{},"source":["## Requirements\n","\n","We use `requests`, `pandas`, and `PyYAML`. Uncomment below to install if needed."]},{"cell_type":"code","execution_count":null,"id":"imports","metadata":{},"outputs":[],"source":["# %pip install -q requests pandas pyyaml\n","\n","import pandas as pd\n","import requests\n","import yaml\n","from pathlib import Path\n","from urllib.parse import urljoin\n","import ipaddress\n","import json, os\n","import math\n","import tideway\n","import re"]},{"cell_type":"markdown","id":"f680ade0-9e07-42b8-8e17-fd659293fbe4","metadata":{},"source":["## Configuration (from config.yaml)\n","\n","Reads settings from `../config.yaml` including target, token/token_file,\n","API version, and SSL verification preference.\n","Saves the CSV to `../output_<target>/credential_success.csv`."]},{"cell_type":"code","execution_count":null,"id":"756da44a-3dd9-4742-82a8-dbf5c7e83038","metadata":{},"outputs":[],"source":["def load_config_params(\n","    start: Path,\n","    appliance_name: str = None,\n","    appliance_index: int = 0,\n",") -> dict:\n","    def _find_repo_root(start: Path) -> Path:\n","        for p in [start] + list(start.parents):\n","            if (p / 'config.yaml').exists():\n","                return p\n","        return start.parent\n","\n","    repo_root = _find_repo_root(start)\n","    config_path = repo_root / 'config.yaml'\n","\n","    with open(config_path, 'r') as fh:\n","        cfg = yaml.safe_load(fh) or {}\n","\n","    apps = cfg.get('appliances') or []\n","    selected = None\n","    if isinstance(apps, list) and apps:\n","        if appliance_name:\n","            selected = next((a for a in apps if a.get('name') == appliance_name), None)\n","            if selected is None:\n","                raise ValueError(f\"No appliance named '{appliance_name}' in config.yaml\")\n","        else:\n","            try:\n","                selected = apps[int(appliance_index)]\n","            except Exception:\n","                selected = apps[0]\n","\n","    target = ((selected or {}).get('target') or cfg.get('target') or '').strip()\n","    if not target:\n","        raise ValueError('config.yaml missing \"target\"')\n","\n","    token = (((selected or {}).get('token') or cfg.get('token') or '').strip())\n","    token_file = (selected or {}).get('token_file') or cfg.get('token_file') or cfg.get('f_token')\n","    if not token and token_file:\n","        tf_path = Path(token_file)\n","        if not tf_path.is_absolute():\n","            tf_path = repo_root / tf_path\n","        with open(tf_path, 'r') as tf:\n","            token = tf.read().strip()\n","    if not token:\n","        raise ValueError('API token not found in config.yaml (token or token_file)')\n","\n","    api_version = str((selected or {}).get('api_version') or cfg.get('api_version') or 'v1.14')\n","    verify_ssl = bool((selected or {}).get('verify_ssl', cfg.get('verify_ssl', True)))\n","\n","    sanitized = target.replace('.', '_').replace(':', '_').replace('/', '_')\n","    output_dir = repo_root / f'output_{sanitized}'\n","    output_dir.mkdir(parents=True, exist_ok=True)\n","\n","    return {\n","        \"repo_root\": repo_root,\n","        \"config_path\": config_path,\n","        \"cfg\": cfg,\n","        \"selected\": selected,\n","        \"target\": target,\n","        \"token\": token,\n","        \"api_version\": api_version,\n","        \"verify_ssl\": verify_ssl,\n","        \"output_dir\": output_dir,\n","    }"]},{"cell_type":"code","execution_count":null,"id":"8cbb854e-5fbb-41f5-a4d2-ef87e3f19ada","metadata":{},"outputs":[],"source":["def init_appliance(appliance_name: str = \"prod\"):\n","    params = load_config_params(Path.cwd(), appliance_name=appliance_name)\n","\n","    target = params[\"target\"]\n","    api_version = params[\"api_version\"]\n","    verify_ssl = params[\"verify_ssl\"]\n","    output_dir = params[\"output_dir\"]\n","\n","    print('Base Host     :', target)\n","    print('API Version   :', api_version)\n","    print('Verify SSL    :', verify_ssl)\n","    print('Output folder :', output_dir)\n","\n","    api_number = api_version.lstrip('v')\n","    app = tideway.appliance(target, params[\"token\"], api_version=api_number, ssl_verify=verify_ssl)\n","\n","    try:\n","        about = app.api_about\n","        print('Appliance reachable:', about.status_code)\n","    except Exception as e:\n","        print('Warning: failed to contact appliance /api/about:', e)\n","\n","    return {\n","        \"params\": params,\n","        \"target\": target,\n","        \"app\": app,\n","        \"api_version\":api_number,\n","        \"output_dir\":output_dir,\n","    }"]},{"cell_type":"markdown","id":"19713aa2-18e7-4e37-82ce-ac1d46baa6f7","metadata":{},"source":["# Initialise Instances"]},{"cell_type":"code","execution_count":null,"id":"541f0e97-d0f0-4508-aa89-26c98a965c27","metadata":{},"outputs":[],"source":["print(\"Initialise Prod:\")\n","twprod = init_appliance(\"prod\")\n","\n","print(\"Initialise Dev:\")\n","twdev = init_appliance(\"dev\")"]},{"cell_type":"code","execution_count":null,"id":"780271ca-8686-413d-9ec6-010ff1eda717","metadata":{},"outputs":[],"source":["# bootstrapping variables\n","\n","token = twprod[\"params\"][\"token\"]\n","tokendev = twdev[\"params\"][\"token\"]\n","VERIFY_SSL = twprod[\"params\"][\"verify_ssl\"]\n","verify_ssl_dev = twdev[\"params\"][\"verify_ssl\"]\n","target = twprod[\"target\"]\n","target_dev = twdev[\"target\"]\n","api_version = twprod[\"api_version\"]\n","api_version_dev = twdev[\"api_version\"]\n","output_dir = twprod[\"output_dir\"]\n","output_dir_dev = twdev[\"output_dir\"]\n","\n","BASE_URL = target if ('://' in target) else f'https://{target}'\n","API_VERSION = api_version"]},{"cell_type":"markdown","id":"session-helpers","metadata":{},"source":["## Session and helpers\n","`post_search` aggregates the 'object' table format across pages and returns {'headings': [...], 'results': [...]}.\n","`table_to_dicts` zips headings into dict rows."]},{"cell_type":"code","execution_count":null,"id":"session-code","metadata":{},"outputs":[],"source":["from typing import List, Dict, Any, Tuple\n","\n","session = requests.Session()\n","auth_value = token if token.lower().startswith('bearer ') else f'Bearer {token}'\n","session.headers.update({'Authorization': auth_value, 'Accept': 'application/json'})\n","session.verify = VERIFY_SSL\n","\n","def api_url(path: str) -> str:\n","    base = BASE_URL.rstrip('/') + f'/api/{API_VERSION}/'\n","    return urljoin(base, path.lstrip('/'))\n","\n","def post_search(query: str, *, limit: int | None = None, page_size: int = 500) -> Dict[str, Any]:\n","    url = api_url('data/search')\n","    headings = None\n","    results: List[List[Any]] = []\n","    offset = 0\n","    fetch_all = (limit == 0)\n","    while True:\n","        payload = {'query': query, 'format': 'object'}\n","        if fetch_all:\n","            payload['limit'] = page_size\n","            if offset:\n","                payload['offset'] = offset\n","        elif limit is not None:\n","            payload['limit'] = limit\n","        r = session.post(url, json=payload)\n","        if r.status_code >= 400:\n","            print(f'Error {r.status_code} POST {url}: {r.text[:200]}')\n","            return {'headings': [], 'results': []}\n","        try:\n","            data = r.json()\n","        except Exception:\n","            data = []\n","        table = None\n","        if isinstance(data, list):\n","            for x in data:\n","                if isinstance(x, dict) and 'headings' in x and 'results' in x:\n","                    table = x\n","                    break\n","        elif isinstance(data, dict) and 'headings' in data and 'results' in data:\n","            table = data\n","        if not table:\n","            return {'headings': [], 'results': []}\n","        if headings is None:\n","            headings = table.get('headings', [])\n","        page_rows = table.get('results') or []\n","        results.extend(page_rows)\n","        if not fetch_all or len(page_rows) < page_size:\n","            break\n","        offset += page_size\n","    return {'headings': (headings or []), 'results': results}\n","\n","def table_to_dicts(table: Dict[str, Any]) -> List[Dict[str, Any]]:\n","    heads = table.get('headings') or []\n","    rows = table.get('results') or []\n","    return [dict(zip(heads, r)) for r in rows]\n"]},{"cell_type":"markdown","id":"logic","metadata":{},"source":["## Build report data\n","- Collect scan ranges and excludes; prepare matchers.\n","- For each endpoint, identify schedule labels; record overlaps and missing schedules.\n","- Add 'seen but unscanned' IPs."]},{"cell_type":"code","execution_count":null,"id":"4e9448df-aaf8-4ac3-a5a8-d8abc3821ca4","metadata":{},"outputs":[],"source":["def _parse_scan_tokens(tokens: List[str]):\n","    items = []\n","    wildcard = False\n","    for t in tokens:\n","        t = (t or '').strip()\n","        if not t:\n","            continue\n","        if t in ('0.0.0.0/0', '::/0'):\n","            wildcard = True\n","            continue\n","        if '-' in t:\n","            try:\n","                start, end = [ipaddress.ip_address(x.strip()) for x in t.split('-', 1)]\n","                items.append(('range', (int(start), int(end), start.version)))\n","                continue\n","            except Exception:\n","                pass\n","        if '/' in t:\n","            try:\n","                net = ipaddress.ip_network(t, strict=False)\n","                items.append(('network', (net, net.version)))\n","                continue\n","            except Exception:\n","                pass\n","        # Single IP\n","        try:\n","            ip = ipaddress.ip_address(t)\n","            items.append(('single', (int(ip), ip.version)))\n","        except Exception:\n","            # Unknown token; skip\n","            pass\n","    return wildcard, items\n","\n","def _endpoint_in_items(ep: str, wildcard: bool, items) -> bool:\n","    if wildcard:\n","        return True\n","    try:\n","        ip = ipaddress.ip_address(ep)\n","        ival = int(ip)\n","        ver = ip.version\n","    except Exception:\n","        return False\n","    for kind, data in items:\n","        if kind == 'network':\n","            net, nver = data\n","            if ver == nver and ip in net:\n","                return True\n","        elif kind == 'range':\n","            start, end, rver = data\n","            if ver == rver and start <= ival <= end:\n","                return True\n","        elif kind == 'single':\n","            sval, sver = data\n","            if ver == sver and ival == sval:\n","                return True\n","    return False"]},{"cell_type":"code","execution_count":null,"id":"57f45d5f-2ba7-41af-88af-8f693e9f54c3","metadata":{},"outputs":[],"source":["def get_results(instance, qry, columns=['No Results']):\n","    results = instance['app'].data().search({'query': qry}, format='object', limit=500)\n","    df = pd.DataFrame(results) if results else pd.DataFrame()\n","    if df.empty:\n","        # Provide headers if no rows returned\n","        df = pd.DataFrame(columns=columns)\n","    if 'Discovery Instance' not in df.columns:\n","        df.insert(0, 'Discovery Instance', instance['target'])\n","    else:\n","        df['Discovery Instance'] = instance['target']\n","    return df\n","\n","def run_and_display(instance, query, columns, head: int = 5):\n","    \"\"\"\n","    Run get_results for an instance, print the target, and display the head of the DataFrame.\n","    \"\"\"\n","    df = get_results(instance, query, columns)\n","    print(instance['target'])\n","    display(df.head(head))\n","    return df\n","\n","# Fetch tables\n","qry_scanrange = '''\n","                search ScanRange where scan_type = 'Scheduled'\n","                show\n","                range_id as 'ID',\n","                label as 'Label',\n","                (range_strings or provider) as 'Scan_Range',\n","                scan_level as 'Level',\n","                recurrenceDescription(schedule) as 'Date_Rules'\n","'''\n","\n","cols= [\"ID\",\"Label\",\"Scan_Range\",\"Level\",\"Date_Rules\",]\n","\n","#t_scan = post_search(qry_scanrange, limit=0)\n","#rows_scan = table_to_dicts(t_scan)\n","#s_df = pd.DataFrame(rows_scan)\n","#display(s_df.head())\n","\n","s_prod = run_and_display(twprod, qry_scanrange, cols)\n","s_dev = run_and_display(twdev, qry_scanrange, cols)\n","\n","qry_excludes = '''\n","                search in '_System' ExcludeRange\n","                show\n","                exrange_id as 'ID',\n","                name as 'Label',\n","                range_strings as 'Scan_Range',\n","                recurrenceDescription(schedule) as 'Date_Rules'\n","'''\n","#t_exc = post_search(qry_excludes, limit=0)\n","#rows_exc = table_to_dicts(t_exc)\n","#e_df = pd.DataFrame(rows_exc)\n","#display(e_df.head())\n","\n","e_prod = run_and_display(twprod, qry_excludes, cols)\n","e_dev = run_and_display(twdev, qry_excludes, cols)\n","\n","# Combine two DataFrames with the same headers\n","s_prod = pd.concat([s_prod, e_prod], ignore_index=True)\n","s_dev = pd.concat([s_dev, e_dev], ignore_index=True)"]},{"cell_type":"code","execution_count":null,"id":"e2f7ab0d-adba-4639-a3ee-c63f4c9447cd","metadata":{},"outputs":[],"source":["# Build label -> matchers\n","\n","# Endpoints and their schedule count\n","qry_ip_schedules = '''\n","                    search DiscoveryAccess\n","                    show endpoint,\n","                    nodecount(traverse Member:List:List:DiscoveryRun where scan_type = 'Scheduled') as 'schedules'\n","                    process with unique()\n","'''\n","#t_sched = post_search(qry_ip_schedules, limit=0)\n","#rows_sched = table_to_dicts(t_sched)\n","#i_df = pd.DataFrame(rows_sched)\n","#display(i_df.head())\n","\n","i_prod = run_and_display(twprod, qry_ip_schedules, cols)\n","i_dev = run_and_display(twdev, qry_ip_schedules, cols)"]},{"cell_type":"code","execution_count":null,"id":"e69afe68-535c-4f42-94c9-194ea49df122","metadata":{},"outputs":[],"source":["def attach_schedules(i, s, range_col):\n","    # Expand schedules so each Scan_Range entry gets its own row\n","    s_expanded = s.explode(range_col)\n","\n","    # Join endpoints against s_expanded ranges\n","    merged = i.merge(\n","        s_expanded,\n","        left_on='endpoint',\n","        right_on='Scan_Range',\n","        how='left'\n","    )\n","\n","    # Group so that each endpoint collects all matching labels\n","    grouped = (\n","        merged.groupby('endpoint')['Label']\n","        .apply(lambda x: ', '.join(sorted(set(x.dropna()))))\n","        .reset_index()\n","    )\n","\n","    # Replace blanks with default message\n","    grouped['Label'] = grouped['Label'].replace(\n","        '', \"Endpoint has previous DiscoveryAccess, but not currently scheduled.\"\n","    )\n","\n","    # Rename for clarity\n","    grouped = grouped.rename(columns={'Label': 'scan_schedules'})\n","\n","    return grouped\n","\n","prod = attach_schedules(i_prod, s_prod, 'Scan_Range')\n","dev = attach_schedules(i_prod, s_prod, 'Scan_Range')\n","\n","display(prod.head())"]},{"cell_type":"code","execution_count":null,"id":"63789a7e-4210-498a-a6bb-50f5af58093d","metadata":{},"outputs":[],"source":["# Seen but unscanned\n","qry_connections_unscanned = '''\n","                            search Host\n","                            traverse InferredElement:Inference:Associate:DiscoveryAccess\n","                            traverse DiscoveryAccess:DiscoveryAccessResult:DiscoveryResult:NetworkConnectionList\n","                            traverse List:List:Member:DiscoveredNetworkConnection\n","                            order by remote_ip_addr\n","                            show remote_ip_addr as 'endpoint'\n","                            processwith connectionsToUnseen\n","'''\n","\n","u_prod = run_and_display(twprod, qry_connections_unscanned, cols)\n","u_dev = run_and_display(twdev, qry_connections_unscanned, cols)"]},{"cell_type":"code","execution_count":null,"id":"c38b0f79-4d45-4a8c-808e-7c73f6b16bf5","metadata":{},"outputs":[],"source":["# Extract unscanned endpoints (prod)\n","eu_prod = u_prod[['endpoint']].copy()\n","eu_prod['scan_schedules'] = \"Seen but unscanned\"\n","\n","# Extract unscanned endpoints (dev)\n","eu_dev = u_dev[['endpoint']].copy()\n","eu_dev['scan_schedules'] = \"Seen but unscanned\"\n","\n","# Append to main results\n","prod = pd.concat([prod, eu_prod], ignore_index=True)\n","dev = pd.concat([dev, eu_dev], ignore_index=True)"]},{"cell_type":"code","execution_count":null,"id":"build-code","metadata":{},"outputs":[],"source":["def save(df: pd.DataFrame, output_dir: Path, filename: str):\n","    \"\"\"\n","    Save a discovery run DataFrame to CSV in the specified output directory.\n","    \"\"\"\n","    output_csv = str(output_dir / f\"{filename}.csv\")\n","    df.to_csv(output_csv, index=False)\n","    print(f\"Saved to {output_csv}\")\n","\n","save(prod, output_dir, \"ip_analysis\")\n","save(dev, output_dir, \"ip_analysis\")\n"]},{"cell_type":"code","execution_count":null,"id":"footer","metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.3"}},"nbformat":4,"nbformat_minor":5}