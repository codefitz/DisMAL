{"cells":[{"cell_type":"markdown","id":"9ba67b35","metadata":{},"source":["# Discovery Schedules (BMC Discovery)\n","\n","This notebook reproduces the DisMAL `schedules` report using the raw CSV exports generated by the CLI.\n","It loads `raw_exports/<appliance>/scheduled_scan_ranges.csv`, `exclude_ranges.csv`, and `discoveryaccess_schedule_counts.csv`\n","to summarise scheduled and excluded ranges along with the number of discovery endpoints that fall within each range.\n"]},{"cell_type":"markdown","id":"7a28dd99","metadata":{},"source":["## Requirements\n","\n","We rely on `pandas`, `PyYAML`, and the standard library. Uncomment the next cell to install requirements if needed.\n"]},{"cell_type":"code","execution_count":null,"id":"6f1ce37c","metadata":{},"outputs":[],"source":["# %pip install -q pandas pyyaml\n","\n","import ast\n","import ipaddress\n","from pathlib import Path\n","from typing import Iterable, List, Tuple\n","\n","import pandas as pd\n","import yaml\n"]},{"cell_type":"markdown","id":"d963bb72","metadata":{},"source":["## Configuration (from config.yaml)\n","\n","Locates the repository root, reads configuration, and determines the raw export and output directories for each appliance.\n"]},{"cell_type":"code","execution_count":null,"id":"35e297e4","metadata":{},"outputs":[],"source":["def load_config_params(\n","    start: Path,\n","    appliance_name: str = None,\n","    appliance_index: int = 0,\n",") -> dict:\n","    def _find_repo_root(path: Path) -> Path:\n","        for candidate in [path] + list(path.parents):\n","            if (candidate / 'config.yaml').exists():\n","                return candidate\n","        return path.parent\n","\n","    def _slugify(value: str) -> str:\n","        return ''.join(ch if ch.isalnum() else '_' for ch in value).strip('_').lower() or 'default'\n","\n","    repo_root = _find_repo_root(start)\n","    config_path = repo_root / 'config.yaml'\n","\n","    with open(config_path, 'r') as fh:\n","        cfg = yaml.safe_load(fh) or {}\n","\n","    appliances = cfg.get('appliances') or []\n","    selected = None\n","    if isinstance(appliances, list) and appliances:\n","        if appliance_name:\n","            selected = next((a for a in appliances if a.get('name') == appliance_name), None)\n","            if selected is None:\n","                raise ValueError(f\"No appliance named '{appliance_name}' in config.yaml\")\n","        else:\n","            try:\n","                selected = appliances[int(appliance_index)]\n","            except Exception:\n","                selected = appliances[0]\n","\n","    target = ((selected or {}).get('target') or cfg.get('target') or '').strip()\n","    if not target:\n","        raise ValueError('config.yaml missing \"target\"')\n","\n","    sanitized = target.replace('.', '_').replace(':', '_').replace('/', '_')\n","    output_dir = repo_root / f'output_{sanitized}'\n","    output_dir.mkdir(parents=True, exist_ok=True)\n","\n","    export_name = ((selected or {}).get('name') or appliance_name or sanitized)\n","    raw_export_dir = repo_root / 'raw_exports' / _slugify(export_name)\n","\n","    return {\n","        'repo_root': repo_root,\n","        'config_path': config_path,\n","        'cfg': cfg,\n","        'selected': selected,\n","        'target': target,\n","        'output_dir': output_dir,\n","        'raw_export_dir': raw_export_dir,\n","    }\n"]},{"cell_type":"markdown","id":"b117251b","metadata":{},"source":["## Initialise Instances\n"]},{"cell_type":"code","execution_count":null,"id":"f94204ca","metadata":{},"outputs":[],"source":["try:\n","    twprod = load_config_params(Path.cwd(), appliance_name='prod')\n","except ValueError:\n","    twprod = load_config_params(Path.cwd(), appliance_index=0)\n","print('Prod Target  :', twprod['target'])\n","print('Prod Exports :', twprod['raw_export_dir'])\n","print('Prod Output  :', twprod['output_dir'])\n","\n","try:\n","    twdev = load_config_params(Path.cwd(), appliance_name='dev')\n","except ValueError:\n","    twdev = load_config_params(Path.cwd(), appliance_index=1)\n","print('Dev Target   :', twdev['target'])\n","print('Dev Exports  :', twdev['raw_export_dir'])\n","print('Dev Output   :', twdev['output_dir'])\n"]},{"cell_type":"markdown","id":"4696dd36","metadata":{},"source":["## Helper Functions\n"]},{"cell_type":"code","execution_count":null,"id":"873786b5","metadata":{},"outputs":[],"source":["BASE_EXPORT_COLUMNS = ['Appliance Target', 'Appliance Name', 'Query Title']\n","SCHEDULE_FILE = 'scheduled_scan_ranges.csv'\n","EXCLUDE_FILE = 'exclude_ranges.csv'\n","SCHEDULE_COUNTS_FILE = 'discoveryaccess_schedule_counts.csv'\n","EXPECTED_SCHEDULE_COLUMNS = ['ID', 'Label', 'Scan_Range', 'Level', 'Date_Rules']\n","EXPECTED_EXCLUDE_COLUMNS = ['ID', 'Label', 'Scan_Range', 'Date_Rules']\n","\n","\n","def _parse_range_list(value) -> List[str]:\n","    if value is None or (isinstance(value, float) and pd.isna(value)):\n","        return []\n","    if isinstance(value, list):\n","        return [str(v).strip() for v in value if str(v).strip()]\n","    text = str(value).strip()\n","    if not text:\n","        return []\n","    if text.startswith('[') and text.endswith(']'):\n","        try:\n","            evaluated = ast.literal_eval(text)\n","        except Exception:\n","            evaluated = [token.strip() for token in text.strip('[]').split(',')]\n","        if isinstance(evaluated, (list, tuple)):\n","            return [str(v).strip() for v in evaluated if str(v).strip()]\n","    return [token.strip() for token in text.split(',') if token.strip()]\n","\n","\n","def _parse_range_token(token: str):\n","    token = token.strip()\n","    if not token:\n","        return None\n","    try:\n","        if '/' in token:\n","            net = ipaddress.ip_network(token, strict=False)\n","            return ('network', net)\n","        if '-' in token:\n","            start_txt, end_txt = token.split('-', 1)\n","            start_ip = ipaddress.ip_address(start_txt.strip())\n","            end_ip = ipaddress.ip_address(end_txt.strip())\n","            if start_ip.version != end_ip.version:\n","                return None\n","            start_val = int(start_ip)\n","            end_val = int(end_ip)\n","            if end_val < start_val:\n","                start_val, end_val = end_val, start_val\n","            return ('range', (start_val, end_val, start_ip.version))\n","        single_ip = ipaddress.ip_address(token)\n","        return ('single', (int(single_ip), single_ip.version))\n","    except Exception:\n","        return None\n","\n","\n","def _build_range_specs(tokens: Iterable[str]):\n","    specs = []\n","    for token in tokens:\n","        parsed = _parse_range_token(token)\n","        if parsed:\n","            specs.append(parsed)\n","    return specs\n","\n","\n","def load_ranges(instance: dict, filename: str, expected: List[str], range_type: str) -> pd.DataFrame:\n","    csv_path = instance['raw_export_dir'] / filename\n","    if csv_path.exists():\n","        df = pd.read_csv(csv_path)\n","    else:\n","        df = pd.DataFrame(columns=expected)\n","\n","    if df.empty and not set(expected).intersection(df.columns):\n","        for col in expected:\n","            df[col] = pd.NA\n","\n","    df = df.drop(columns=[c for c in BASE_EXPORT_COLUMNS if c in df.columns], errors='ignore')\n","    for col in expected:\n","        if col not in df.columns:\n","            df[col] = pd.NA\n","    df['Type'] = range_type\n","    return df[expected + ['Type']]\n","\n","\n","def load_schedule_counts(instance: dict) -> List[Tuple[ipaddress._BaseAddress, int]]:\n","    csv_path = instance['raw_export_dir'] / SCHEDULE_COUNTS_FILE\n","    if not csv_path.exists():\n","        return []\n","    df = pd.read_csv(csv_path)\n","    df = df.drop(columns=[c for c in BASE_EXPORT_COLUMNS if c in df.columns], errors='ignore')\n","    if 'endpoint' not in df.columns:\n","        return []\n","    df['schedules'] = pd.to_numeric(df.get('schedules', 1), errors='coerce').fillna(0).astype(int)\n","\n","    parsed: List[Tuple[ipaddress._BaseAddress, int]] = []\n","    for endpoint, count in zip(df['endpoint'], df['schedules']):\n","        try:\n","            ip_obj = ipaddress.ip_address(str(endpoint))\n","        except Exception:\n","            continue\n","        parsed.append((ip_obj, count if count > 0 else 1))\n","    return parsed\n","\n","\n","def endpoint_matches(ip_obj: ipaddress._BaseAddress, specs: List[Tuple[str, object]]) -> bool:\n","    ip_val = int(ip_obj)\n","    version = ip_obj.version\n","    for kind, data in specs:\n","        if kind == 'network':\n","            network: ipaddress._BaseNetwork = data\n","            if network.version == version and ip_obj in network:\n","                return True\n","        elif kind == 'range':\n","            start, end, ver = data\n","            if ver == version and start <= ip_val <= end:\n","                return True\n","        elif kind == 'single':\n","            value, ver = data\n","            if ver == version and value == ip_val:\n","                return True\n","    return False\n","\n","\n","def count_matched_endpoints(range_tokens: List[str], endpoints: List[Tuple[ipaddress._BaseAddress, int]]) -> int:\n","    specs = _build_range_specs(range_tokens)\n","    if not specs:\n","        return 0\n","    total = 0\n","    for ip_obj, sched in endpoints:\n","        if endpoint_matches(ip_obj, specs):\n","            total += sched\n","    return total\n"]},{"cell_type":"markdown","id":"4d66f3fb","metadata":{},"source":["## Load and Preview\n"]},{"cell_type":"code","execution_count":null,"id":"cb4afece","metadata":{},"outputs":[],"source":["def build_schedule_dataframe(instance: dict) -> pd.DataFrame:\n","    sched_df = load_ranges(instance, SCHEDULE_FILE, EXPECTED_SCHEDULE_COLUMNS, 'Scan Range')\n","    excl_df = load_ranges(instance, EXCLUDE_FILE, EXPECTED_EXCLUDE_COLUMNS, 'Exclude Range')\n","\n","    if 'Level' not in excl_df.columns:\n","        excl_df['Level'] = pd.NA\n","\n","    combined = pd.concat([sched_df, excl_df], ignore_index=True, sort=False)\n","    if combined.empty:\n","        return pd.DataFrame(columns=['Discovery Instance', 'Name', 'Type', 'Range ID', 'Ranges', 'Scan Level', 'When', 'Scheduled Endpoints'])\n","\n","    endpoints = load_schedule_counts(instance)\n","\n","    records = []\n","    for _, row in combined.iterrows():\n","        tokens = _parse_range_list(row.get('Scan_Range'))\n","        range_count = len(tokens)\n","        matched = count_matched_endpoints(tokens, endpoints)\n","        records.append({\n","            'Discovery Instance': instance['target'],\n","            'Name': row.get('Label'),\n","            'Type': row.get('Type'),\n","            'Range ID': row.get('ID'),\n","            'Ranges': range_count,\n","            'Scan Level': row.get('Level'),\n","            'When': row.get('Date_Rules'),\n","            'Scheduled Endpoints': matched,\n","        })\n","\n","    result = pd.DataFrame(records)\n","    result = result.sort_values(['Type', 'Range ID'], na_position='last').reset_index(drop=True)\n","    return result\n","\n","prod_df = build_schedule_dataframe(twprod)\n","print(twprod['target'])\n","display(prod_df.head(10))\n","\n","dev_df = build_schedule_dataframe(twdev)\n","print(twdev['target'])\n","display(dev_df.head(10))\n"]},{"cell_type":"markdown","id":"3fe416b8","metadata":{},"source":["## Save CSV\n"]},{"cell_type":"code","execution_count":null,"id":"f26f7f8d","metadata":{},"outputs":[],"source":["OUTPUT_FILE = 'schedules.csv'\n","\n","prod_path = twprod['output_dir'] / OUTPUT_FILE\n","prod_df.to_csv(prod_path, index=False)\n","print(f'Saved prod schedules to {prod_path} (rows: {len(prod_df)})')\n","\n","dev_path = twdev['output_dir'] / OUTPUT_FILE\n","dev_df.to_csv(dev_path, index=False)\n","print(f'Saved dev schedules to {dev_path} (rows: {len(dev_df)})')\n"]},{"cell_type":"markdown","id":"05270a0f","metadata":{},"source":["---\n","### Notes\n","- Scheduled endpoint counts are derived from `discoveryaccess_schedule_counts.csv` by matching endpoints to each range definition.\n","- Update the export mappings above if additional schedule-related queries are added to the CLI workflow.\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.13.7"}},"nbformat":4,"nbformat_minor":5}